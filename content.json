{"pages":[],"posts":[{"title":"为Linux添加一个系统调用","text":"在之前的博客中，我使用 C 和 Python 分别实现了一个简单的 Shell，这是一个很有意思的小程序，可以让你了解你每天都在使用的工具。而在简单的 Shell 之下则是一系列的系统调用，例如：read, fork, exec, wait, write等等。现在让我们继续这段旅程，开始学习这些系统调用是如何在 Linux 中实现的。 什么是系统调用在我们实现系统调用之前，我们最好弄清楚它们是什么。一个菜鸟程序员——像几年前的我——可能会将系统调用定义为 C 标准库提供的任何函数。但这并不完全准确。即使许多 C 标准库中的函数与系统调用对应的很好（例如 chdir），但大部分都会比简单地使用系统调用做更多的事（例如 fork, fprintf）。或者纯粹通过编程实现不使用系统调用（qsort, strtok）。 事实上，一个系统调用有着非常明确的定义。它是一种按照你的需求调用操作系统内核资源的方式。对字符串进行分词这样的操作不需要和内核交互，但是任何涉及到设备，文件或者进程的操作需要。 系统调用在底层的行为也与普通函数不同。不同于简单地让指令指针跳转到你的程序或函数库的代码段，而是让你的程序请求 CPU 切换到内核模式，然后访问预先在内核中定义的地址来执行系统调用。这只能通过几种方式实现，例如：处理器中断或者特定的指令（syscall,sysenter） 。 难得的是，实现系统调用的最复杂的部分已经在内核中实现了。不管一个系统调用是如何实现的，都得通过系统调用号查表来找到并调用正确的内核函数，这对于实现我们自己的系统调用来说是非常简单的。让我们开始吧。 设置虚拟机实现系统调用并不是通过修改某一个内核模块来实现。而是你必须获得一份 Linux 源码的复制，修改它，编译它，启动它。这些都是可以直接在你的主机中完成的事（如果主机是 Linux系统），但最好在虚拟机上进行这类尝试。例如在 VirtualBox 上。 虽然你可以手动地配置一台虚拟机，但更节省时间的方式是下载一台已经配置好的。你可以通过这个链接 下载一台已经配置好的虚拟机。在这篇文章中，我会使用201608CLI 版本的 VirtualBox 虚拟机。下载并解压它。在 VirtualBox 中创建一台新的虚拟机，选择下载解压好的 vdi 文件作为硬盘文件。创建并允许你的虚拟机，你就能看到你的命令行登录界面了。根用户的密码可以在下载页面查看到（我的是：osboxes.org）。 注意：如果你有一台多核的机器，编辑你的虚拟机设置让它使用多核将会是一个很好的主意。这将会显著地提升编译效率，只要你再接下来的命令中使用 make -jN 替换 make，其中 N 是你机器的 CPU 核数。 需要做的第一步是安装 bc，一个没有包含进虚拟机的编译期 Linux 依赖。不幸的是，这需要你首先更新虚拟机。注意我在这篇博客中给出的每一条命令都需要在 root 下运行。 123$ pacman -Syu$ pacman -S bc$ reboot 你需要重启虚拟机，因为内核将被更新。我们必须确保我们运行的内核是更新过的以开始后续的操作。 获取源码在你配置好虚拟机后，下一步是下载内核源码。尽管大部分开发者本能地想到用 Git 来获取他们需要的代码，但这一次可能并不需要。Linux 的 Git 仓库太大了，所以复制它是很不值得的。然而可以你可以下载内核版本相关的源码压缩文件。你可以通过 uname -r 来查看内核版本。然后在 kernel.org 上下载最接近的版本。在你的虚拟机中，使用 curl 命令来下载源码： 12# -O -J will set the output filename based on the URL$ curl -O -J https://www.kernel.org/pub/linux/kernel/v4.x/linux-VERSION.tar.xz 之后解压 tarball 文件： 12$ tar xvf linux-VERSION.tar.xz$ cd linux-VERSION 配置你的内核Linux 内核是非常方便配置的。你可以启用或废弃它的大部分功能。如果你要手动地配置每一个选项，你将会做一整天。幸运地使，你可以通过复制你的内核已有的配置文件来跳过这一步，它保存在/proc/config.gz 。通过这条命令来将配置应用在你的新内核中： 1$ zcat /proc/config.gz &gt; .config 为了确保配置文件中的所有变量都被赋值，运行 make oldconfig。如果没有问题，命令行将不会询问你任何配置问题。 你唯一需要修改的配置项是你的内核名，确保它不会和你当前已经安装的发生冲突。在 Arch Linux 中，内核在构建时就带有后缀 -ARCH。你应该改变这个后缀，通过文本编辑器打开.confit，然后直接修改它的行。你将会发现它就在”General setup”头下面，距文件底不远。 1CONFIG_LOCALVERSION=\"-ARCH\" 添加你的系统调用现在内核已经配置好了，你可以马上开始编译它了。然而创建一个系统调用需要编辑系统调用表，它记录了每一条系统调用的大致信息。因为编译需要花费很长的时间，你现在编译的话会浪费很多的时间。不如让我们做一些有价值的事情吧：开始写你自己的系统调用！ Linux 中的一些代码是架构特定的，例如一些初始化处理中断和系统调用的代码。因此系统调用表所在的目录取决于你的处理器架构。我们将在 x86_64 架构的机器上操作。 系统调用表在 x86_64 的机器上包含系统调用表的文件在路径arch/x86/entry/syscalls/syscall_64.tbl 。这张表通过脚本读入并生成一些模板代码，这将使我们的事情变得非常简单。来到文件底部（在4.7.1版本中系统调用号结束于328），添加下面这行： 1329 common yifeng sys_yifeng 注意每一列之间是一个 tab(不是空格)。第一列是系统调用号。在这张表中我选择了下一个可用的数字——329.在不同版本的系统中这个数字可能不同！第二列表明这个系统调用号是在32位和64位 CPU 中共用的。第三列是系统调用名，第四列是实现函数名。转换系统调用名到实现函数名是简单的，在系统调用名前加上前缀sys_ 即可。我使用了 yifeng 作为我的系统调用名，你也可以使用任何你喜欢的。 系统调用函数最后一步是为系统调用实现调用函数。我们其实并不知道系统调用应该做些什么，但我们想让它做一些简单的我们可以察觉的事。比如使用printk() 打印一些内核日志。所以，我们的系统调用将会需要传入一个参数（一个字符串），然后将它写入内核日志。 你可以在任何地方实现系统调用，但是杂项的系统调用最好写在kernel/sys.c 文件中。 123456789SYSCALL_DEFINE1(yifeng, char *, msg){ char buf[256]; long copied = strncpy_from_user(buf, msg, sizeof(buf)); if (copied &lt; 0 || copied == sizeof(buf)) return -EFAULT; printk(KERN_INFO \"yifeng syscall called with \\\"%s\\\"\\n\", buf); return 0;} SYSCALL_DEFINEN 是一系列的宏定义族，它使得定义 N 个变量的系统调用变得简单。宏的第一个参数是系统调用的名字（没有 sys_ 前缀）。接下来的参数是系统调用的参数类型和参数名对。因为我们的系统调用只有一个参数，所以我们使用 SYSCALL_DEFINE1 ，我们的唯一参数是char * 和参数名msg 。 我们马上会遇到一个有趣的问题，那就是我们无法直接使用提供给我们的 msg 的指针。有以下几点可能的原因： 进程可能会尝试愚弄我们来打印内核空间的数据。通过传递一个映射到内核空间的指针，这个系统调用将会打印内核空间的数据。这将不能被允许。 进程可能会尝试度其他进程的内存，通过传递一个映射到其他内存地址空间的指针。 我们需要考虑内存的 读/写/执行 的权限。 为了处理这个问题，我们使用了strncpy_from_user() 函数，它的行为和strncpy 类似，但是会首先检查传入的地址是否是来自用户空间的地址。如果传入的字符串太长或者在复制的时候出现了问题，我们都会返回 EFAULT (即使返回EINVAL 对于字符串太长的情况更合适)。 最终我们使用printk 来打印。KERN_INFO 会展开成一个文本字符串。编译器将会把它和格式化字符串串联起来，打印的效果和printf 类似。 编译和启动内核注意：这些步骤会有一些复杂。看这部分的时候，你不需要一步一步输入命令，在最后我会给你一个完成这些工作的脚本。 我们的第一步是编译内核和它的模块。内核的主镜像通过make 编译。你可以在下面的文件中看到编译结果：arch/x86_64/boot/bzImage 。在运行make modules_install 后， 这个版本的内核模块将被编译并复制到 /lib/modules/KERNEL_VERSION 。例如，按照我所给出的配置，这些模块将被编译并放置在/lib/modules/linux-4.7.1-yifeng/ 。 在你已经编译好内核及其模块后，你将需要做一些事情来启动它。首先，你将要复制你已经编译好的内核镜像到你的/boot 目录： 1$ cp arch/x86_64/boot/bzImage /boot/vmlinuz-linux-yifeng 接下来需要创建一个“initramfs”。 通过两步来实现它。首先基于你之前的文件创建一个 preset: 1$ sed s/linux/linux-yifeng/g &lt;/etc/mknitcpio.d/linux.preset &gt;/etc/mkinitcpio.d/linux-yifeng.preset 之后生成一个镜像： 1$ mkinitcpio -p linux-yifeng 最后你需要指导你的 bootloader (这里是指我们的虚拟机，GRUB)来启动我们的新内核。因为 GRUB 可以自动找到 /boot 目录下的内核镜像，所有我们需要做的是再生成 GRUB 配置： 1$ grub-mkconfig -o /boot/grub/grub.cfg 所以上面的操作可以总在一个脚本中： 1234567891011121314151617181920212223242526#!/usr/bin/bash# Compile and \"deploy\" a new custom kernel from source on Arch Linux# Change this if you'd like. It has no relation# to the suffix set in the kernel config.SUFFIX=\"-yifeng\"# This causes the script to exit if an error occursset -e# Compile the kernelmake# Compile and install modulesmake modules_install# Install kernel imagecp arch/x86_64/boot/bzImage /boot/vmlinuz-linux$SUFFIX# Create preset and build initramfssed s/linux/linux$SUFFIX/g \\ &lt;/etc/mkinitcpio.d/linux.preset \\ &gt;/etc/mkinitcpio.d/linux$SUFFIX.presetmkinitcpio -p linux$SUFFIX# Update bootloader entries with new kernels.grub-mkconfig -o /boot/grub/grub.cfg 将 deploy.sh 保存在你内核源码的主目录下，执行chmod u+x deploy.sh 来设置执行权限。现在你就可以通过运行这个脚本来构建和部署内核了。编译将会需要一些时间。 一旦脚本执行完成，执行reboot 。当 GRUB 弹出，选择“Advanced Options for Arch Linux”。这将会显示可用内核的菜单列表，选择你自定义的内核并启动它。 如果一切顺利，就能看到带有自定义内核版本的登录页面了。 测试你的系统调用至此，我们已经编译并启动了我们自定义并修改的内核了。接下来就是最激动人心的时刻了——运行我们的系统调用。 C 标准库为我们封装了大部分系统调用，但我们没有考虑过如何触发一个中断。对于无法直接调用的系统调用，GNC C标准库提供了一个 syscall() ，它可以根据系统调用号来调用系统调用。 下面是一个小程序，使用系统调用号来调用我们自定义的系统调用： 12345678910111213141516171819202122232425/* * Test the yifeng syscall (#329) */#define _GNU_SOURCE#include &lt;unistd.h&gt;#include &lt;sys/syscall.h&gt;#include &lt;stdio.h&gt;/* * Put your syscall number here. */#define SYS_yifeng 329int main(int argc, char **argv){ if (argc &lt;= 1) { printf(\"Must provide a string to give to system call.\\n\"); return -1; } char *arg = argv[1]; printf(\"Making system call with \\\"%s\\\".\\n\", arg); long res = syscall(SYS_yifeng, arg); printf(\"System call returned %ld.\\n\", res); return res;} 将这个文件命名为test.c ,编译它gcc -o test test.c 。接下来就可以试着调用系统调用了，尝试打印“Hello World”。 12$ ./test 'Hello World!'# use single quotes if you have an exclamation point :) 可以通过dmesg 指令查看生成的日志。因为dmesg会在你的终端弹出超多的信息，所以使用dmesg|tail 来查看日志的最后几行就可以了。你就可与在日志里看到系统调用生成的文本了。 本博客参考了该链接","link":"/2018/12/19/为Linux添加一个系统调用/"},{"title":"Kaggle之房价预测","text":"Kaggle入门赛之房价预测 前言：最近实验室有一个数据挖掘的任务需要完成，是一个和国家电网相关的回归任务。因此我在 Kaggle 上找一些回归任务的比赛来练练手，房价预测这个比赛的难度比较适合我这种新手，因此选择了这个题目。本篇博客的大部分内容来自该比赛的一个高分 kernel, 感兴趣的同学可以直接跳转查看原博。 1.比赛介绍这是 Kaggle 上的一个为数据科学初学者准备的比赛，需要参赛者掌握一定的 R 或 Python 以及机器学习的基础知识。它给出了一个训练集和一个测试集，最终需要对测试集中的结果进行预测并提交。训练集中包含了79个特征以预测房价（SalePrice），包括：房屋的大小、地段、用途、基础设施等等。具体的描述可查看该比赛的比赛页面,比赛中所用到数据也均可通过比赛页面下载。 2.准备工作数据挖掘类的比赛最为重要的工作就是：特征工程。本篇博客将会用到的特征工程方法还是非常朴素的，具体如下： 通过已有的数据拟合曲线来填充缺失值。 变换某些看起来更像分类变量的数值型变量。 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。 对于一些倾斜特征采用 Box Cox 变换（替代对数变换）：这会得到一个更好的结果。 得到分类特征的假值。 之后我们将会选择一些基本模型（例如基于 sklearn 的 XGBoost），在 stacking/ensembling 之前在数据集上进行交叉验证。它的关键在于让（线性）模型对异常值鲁棒。这将同时提高积分板和交叉验证的分数。 接下来真正开始工作吧！首先导入相关的包和比赛所需的数据。 123456789101112131415161718192021222324#import some necessary librairiesimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)%matplotlib inlineimport matplotlib.pyplot as plt # Matlab-style plottingimport seaborn as snscolor = sns.color_palette()sns.set_style('darkgrid')import warningsdef ignore_warn(*args, **kwargs): passwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)from scipy import statsfrom scipy.stats import norm, skew #for some statisticspd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal pointsfrom subprocess import check_outputprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory sample_submission.csv test.csv train.csv 123456#Now let's import and put the train and test datasets in pandas dataframetrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv')##display the first five rows of the train dataset.train.head(5) 12##display the first five rows of the test dataset.test.head(5) 123456789101112131415#check the numbers of samples and featuresprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))print(\"The test data size before dropping Id feature is : {} \".format(test.shape))#Save the 'Id' columntrain_ID = train['Id']test_ID = test['Id']#Now drop the 'Id' colum since it's unnecessary for the prediction process.train.drop(\"Id\", axis = 1, inplace = True)test.drop(\"Id\", axis = 1, inplace = True)#check again the data size after dropping the 'Id' variableprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) print(\"The test data size after dropping Id feature is : {} \".format(test.shape)) 123456&gt;The train data size before dropping Id feature is : (1460, 81) &gt;The test data size before dropping Id feature is : (1459, 80) &gt;&gt;The train data size after dropping Id feature is : (1460, 80) &gt;The test data size after dropping Id feature is : (1459, 79) &gt; 3.数据处理异常值首先对数据集中的重要特征的异常值进行处理。 12345fig, ax = plt.subplots()ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 可从散点图中看到有两个非常明显的异常值，有着极大的房屋面积房价却很低。这两个值是非常典型的异常值，因此可以安全地删除它们。 123456789#Deleting outlierstrain = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)#Check the graphic againfig, ax = plt.subplots()ax.scatter(train['GrLivArea'], train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 注意：删除异常值并不总是安全的。我们决定删除这两个值是因为它俩太奇怪了（极大的空间有着极低的价格）。 数据集中仍然可能存在其它的异常值。然而删除所有的异常值可能对模型产生不好的影响，特别是当它们可能出现在测试集的时候。因此在处理异常值得时候最好不要全部删除掉，以提高模型的鲁棒性。 目标变量房价是我们需要预测的目标变量，因此首先对它进行一些分析。 1df_train['SalePrice'].describe() 12345678910&gt;count 1460.000000&gt;mean 180921.195890&gt;std 79442.502883&gt;min 34900.000000&gt;25% 129975.000000&gt;50% 163000.000000&gt;75% 214000.000000&gt;max 755000.000000&gt;Name: SalePrice, dtype: float64&gt; 12345678910111213141516sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 180932.92 and sigma = 79467.79&gt; 目标变量是接近正态分布的。线性模型在正态分布的数据上拟合效果更好，所以我们可以对数据做一些变换使其更接近正态分布。 目标变量的对数变换1234567891011121314151617181920#We use the numpy fuction log1p which applies log(1+x) to all elements of the columntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])#Check the new distribution sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 12.02 and sigma = 0.40&gt; 在经过对数变换后，房价数据接近于正态分布。 4.特征工程首先将训练数据和测试数据合并到一个数据帧。 123456ntrain = train.shape[0]ntest = test.shape[0]y_train = train.SalePrice.valuesall_data = pd.concat((train, test)).reset_index(drop=True)all_data.drop(['SalePrice'], axis=1, inplace=True)print(\"all_data size is : {}\".format(all_data.shape)) 12&gt;all_data size is : (2917, 79)&gt; 缺失数据1234all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head(20) 123456f, ax = plt.subplots(figsize=(15, 12))plt.xticks(rotation='90')sns.barplot(x=all_data_na.index, y=all_data_na)plt.xlabel('Features', fontsize=15)plt.ylabel('Percent of missing values', fontsize=15)plt.title('Percent missing data by feature', fontsize=15) 12&gt;Text(0.5,1,&apos;Percent missing data by feature&apos;) &gt; 数据关联1234#Correlation map to see how features are correlated with SalePricecorrmat = train.corr()plt.subplots(figsize=(12,9))sns.heatmap(corrmat, vmax=0.9, square=True) 输入缺失值根据不同特征的不同情况对缺失值进行填充。 对于离散值且数据说明里有 NA 选项，则将缺失值补为None。 PoolQC : 数据描述说 NA 表示 “没有泳池”。这是有意义的，因为有大量的缺失数据，这表明大多数的房子都没有游泳池。 1all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\") MiscFeature : 数据描述说 NA 表示“没有杂项特征”。 1all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\") Alley : 数据描述说 NA 表示“没有小巷联通”。 1all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") Fence : 数据描述说 NA 表示“没有棚栏”。 1all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\") 对于和其他特征有关联的值，可以先按其它特征分组，然后取中位数 。 LotFrontage : 因为房子附近的街道到房子的距离和到该房子临近的房子的距离是类似的，因此去临近房子的中位数来填充该距离。 123#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhoodall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform( lambda x: x.fillna(x.median())) 对于连续值，将缺失的值补为0。 BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath :缺失的值为0意味着没有基础设施。 12for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'): all_data[col] = all_data[col].fillna(0) 对于离散值且在数据说明里没有 NA 选项，则将缺失值填充为出现最多的值 。 MSZoning(大致区域分类)：‘RL’是最常见的值。所以我们使用‘RL’填充缺失值。 1all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0]) 对于所有值几乎都一样，参考意义不大的特征，直接删除 。 Utilities:该特征所有的值几乎都为‘AllPub’,除了一个‘NoSeWa’和2个NA。因为唯一的’NoSewa’出现在训练集，这个特征将不会对预测有帮助，因此可以安全地移除它。 1all_data = all_data.drop(['Utilities'], axis=1) 在缺失值处理完后，可以简单地查看是否还有未处理地缺失值： 12345#Check remaining missing values if any all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head() 更多特征工程将某些看起来是连续值的特征转换为离散值1234567891011#MSSubClass=The building classall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)#Changing OverallCond into a categorical variableall_data['OverallCond'] = all_data['OverallCond'].astype(str)#Year and month sold are transformed into categorical features.all_data['YrSold'] = all_data['YrSold'].astype(str)all_data['MoSold'] = all_data['MoSold'].astype(str) 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。1234567891011121314from sklearn.preprocessing import LabelEncodercols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')# process columns, apply LabelEncoder to categorical featuresfor c in cols: lbl = LabelEncoder() lbl.fit(list(all_data[c].values)) all_data[c] = lbl.transform(list(all_data[c].values))# shape print('Shape all_data: {}'.format(all_data.shape)) 增加一个更为重要的特征因为面积相关的特征对房价的影响是非常大的。所以我们增加了一个特征来描述基础设施的总面积，第一层和第二层的面积。 12# Adding total sqfootage feature all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] 斜率特征1234567numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index# Check the skew of all numerical featuresskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)print(\"\\nSkew in numerical features: \\n\")skewness = pd.DataFrame({'Skew' :skewed_feats})skewness.head(10) Box Cox变换高斜率特征1234567891011skewness = skewness[abs(skewness) &gt; 0.75]print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))from scipy.special import boxcox1pskewed_features = skewness.indexlam = 0.15for feat in skewed_features: #all_data[feat] += 1 all_data[feat] = boxcox1p(all_data[feat], lam) #all_data[skewed_features] = np.log1p(all_data[skewed_features]) 获取分类特征的假值12all_data = pd.get_dummies(all_data)print(all_data.shape) 获取新的训练集和测试集 12train = all_data[:ntrain]test = all_data[ntrain:] 5.模型导入相关库 12345678910from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsICfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.kernel_ridge import KernelRidgefrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clonefrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.metrics import mean_squared_errorimport xgboost as xgbimport lightgbm as lgb 定义交叉策略我们使用 Sklearn 中的 cross_cal_score 函数。然而这个函数没有扰乱属性，我们添加了一行代码，使得在交叉验证前对数据集进行扰乱。 1234567#Validation functionn_folds = 5def rmsle_cv(model): kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) return(rmse) 基础模型 LASSO 回归： 这个模型会对异常值特别敏感。所以我们需要让它变得更加鲁棒。在 pipeline 中使用 sklearn 中的 Robustscaler() 方法。 1lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) Elastic Net 回归： 1ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)) Kernel Ridge 回归： 1KRR = KernelRidge(alpha=0.6, kernel=&apos;polynomial&apos;, degree=2, coef0=2.5) Gradient Boosting 回归： 使用 huber loss 使其对异常值鲁棒 1234GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5) XGBoost: 123456model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state =7, nthread = -1) LightGBM: 123456model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11) 基本模型分数模型初始化好之后就开始测试一下交叉验证的损失分数吧。 12score = rmsle_cv(lasso)print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Lasso score: 0.1115 (0.0074)&gt; 12score = rmsle_cv(ENet)print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;ElasticNet score: 0.1116 (0.0074)&gt; 12score = rmsle_cv(KRR)print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Kernel Ridge score: 0.1153 (0.0075)&gt; 12score = rmsle_cv(GBoost)print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Gradient Boosting score: 0.1177 (0.0080)&gt; 12score = rmsle_cv(model_xgb)print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Xgboost score: 0.1161 (0.0079)&gt; 12score = rmsle_cv(model_lgb)print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std())) 12&gt;LGBM score: 0.1157 (0.0067)&gt; 堆叠模型（stacking model)最简单的堆叠方法：平均化基础模型我们首先用最简单的方法实验，构建一个新类来拓展 scikit-learn 中 model类，作为我们自己的堆叠模型。 平均化的基础模型类1234567891011121314151617181920class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, models): self.models = models # we define clones of the original models to fit the data in def fit(self, X, y): self.models_ = [clone(x) for x in self.models] # Train cloned base models for model in self.models_: model.fit(X, y) return self #Now we do the predictions for cloned models and average them def predict(self, X): predictions = np.column_stack([ model.predict(X) for model in self.models_ ]) return np.mean(predictions, axis=1) 平均化的基础模型分数1234averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))score = rmsle_cv(averaged_models)print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt; Averaged base models score: 0.1091 (0.0075)&gt; 从结果看来，简单的堆叠模型确实提高了分数。这将激励我们探索更为高效的堆叠模型。 不那么简单的堆叠模型：增加一个元模型在这个方法中，我们将会在简单堆叠模型的基础上增加一个元模型并使用基本模型的预测结果来训练元模型。 元模型的训练过程如下： 划分数据集为两个分离的部分（train 和 holdout） 使用第一部分训练几个基本的模型（train） 在这些训练过的模型上使用第二部分进行测试（holdout） 使用3）中的预测结果作为输入，正确地响应作为输出来训练元模型 前三步是可以迭代完成的。举例来说，我们有一个包含5个基本模型的堆叠模型，我们首先会把训练数据分为5份 。之后我们会做五次迭代。在每一次迭代中，我们在4份数据上训练基本模型然后在剩下的那份数据上预测。 可以确信的是，在五次迭代后，我们可以得到完整的5份由基本模型预测得到的训练数据，这在之后将会作为训练数据来训练我们的原模型。 在预测部分，我们会平均所有基础模型在测试数据上的预测结果，并将它们作为元特征，最终的预测结果将由元模型得出。 #####改进的堆叠模型类 12345678910111213141516171819202122232425262728293031323334class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, base_models, meta_model, n_folds=5): self.base_models = base_models self.meta_model = meta_model self.n_folds = n_folds # We again fit the data on clones of the original models def fit(self, X, y): self.base_models_ = [list() for x in self.base_models] self.meta_model_ = clone(self.meta_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # Train cloned base models then create out-of-fold predictions # that are needed to train the cloned meta-model out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models))) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_of_fold_predictions[holdout_index, i] = y_pred # Now train the cloned meta-model using the out-of-fold predictions as new feature self.meta_model_.fit(out_of_fold_predictions, y) return self #Do the predictions of all base models on the test data and use the averaged predictions as #meta-features for the final prediction which is done by the meta-model def predict(self, X): meta_features = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ]) return self.meta_model_.predict(meta_features) 改进的堆叠模型的分数为了使得两种方法有可比性（使用了同样数量的基础模型），我们使用了 Enet、KRR、Gboost作为基础模型，使用 lasso 作为元模型。 12345stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), meta_model = lasso)score = rmsle_cv(stacked_averaged_models)print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())) 12&gt;Stacking Averaged models score: 0.1085 (0.0074)&gt; 通过加入元模型确实得到了更好的结果。 融合模型我们增加了 XGBoost、LightGBM 到之前定义的堆叠模型中。 我们首先定义一个评估函数 rmsle 12def rmsle(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred)) 最终的训练和预测StackRegressor: 1234stacked_averaged_models.fit(train.values, y_train)stacked_train_pred = stacked_averaged_models.predict(train.values)stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))print(rmsle(y_train, stacked_train_pred)) 12&gt;0.0781571937916&gt; XGBoost: 1234model_xgb.fit(train, y_train)xgb_train_pred = model_xgb.predict(train)xgb_pred = np.expm1(model_xgb.predict(test))print(rmsle(y_train, xgb_train_pred)) 12&gt;0.0785165142425&gt; LightGBM: 1234model_lgb.fit(train, y_train)lgb_train_pred = model_lgb.predict(train)lgb_pred = np.expm1(model_lgb.predict(test.values))print(rmsle(y_train, lgb_train_pred)) 12&gt;0.0716757468834&gt; 12345'''RMSE on the entire Train data when averaging'''print('RMSLE score on train data:')print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 )) 123&gt;RMSLE score on train data:&gt;0.0752190464543&gt; 融合预测： 1ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15 提交： 1234sub = pd.DataFrame()sub['Id'] = test_IDsub['SalePrice'] = ensemblesub.to_csv('submission.csv',index=False)","link":"/2018/12/07/Kaggle之房价预测/"},{"title":"从源码编译和安装Linux内核","text":"如何编译和安装Linux内核可以按如下的步骤编译和安装Linux内核 1.从 kernel.org 抓取最新版本的内核 2.验证内核 3.解压内核压缩文件 4.复制已存的 Linux 内核配置文件 5.编译和构建内核 6.安装内核和模块 7.更新 Grub 配置 8.重启系统 1.获取最新版本的源代码访问Linux官方网站下载最新的源代码。点击标有“Latest Stable Kernel”的黄色大按钮。 也可以使用 wget 命令下载 Linux 内核源代码： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.xz 2.获取 tar.xz 文件使用 unzx 命令和 xz 命令解压压缩文件获取源代码： 1unzx -v linux-4.19.1.tar.xz 或者 1xz -d -v linux-4.19.1.tar.xz 使用pgp验证 linux 内核 tarball首先获取linux-4.19.1.tar 的 PGP 签名： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.sign 尝试进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 从 PGP 密钥服务器获取公钥来验证签名。例如：RSA key ID 79BE3E4300411886（从上面的输出结果中得到） 1gpg --recv-keys 79BE3E4300411886 示例输出： 1234567gpg: key 79BE3E4300411886: 7 duplicate signatures removedgpg: key 79BE3E4300411886: 172 signatures not checked due to missing keysgpg: /home/vivek/.gnupg/trustdb.gpg: trustdb createdgpg: key 79BE3E4300411886: public key &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; importedgpg: no ultimately trusted keys foundgpg: Total number processed: 1gpg: imported: 1 现在再一次使用 gpg 命令进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 12345678gpg: assuming signed data in &apos;linux-4.19.1.tar&apos;gpg: Signature made Sun 12 Aug 2018 04:00:28 PM CDTgpg: using RSA key 79BE3E4300411886gpg: Good signature from &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; [unknown]gpg: aka &quot;Linus Torvalds &lt;torvalds@linux-foundation.org&gt;&quot; [unknown]gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: ABAF 11C6 5A29 70B1 30AB E3C4 79BE 3E43 0041 1886 如果通过了验证，那么就使用 tar 进行解压： 1tar xvf linux-4.19.1.tar 3.配置 Linux 内核特征和模型在开始构建内核之前，你必须配置内核特征。你也必须明确你的系统所需要的内核模块（驱动）。这个对于新手来说是很有压力的。我建议你使用 cp 命令复制已有的配置文件： 12cd linux-4.19.1cp -v /boot/config-$(uname -r) .config 示例输出： 1&apos;/boot/config-4.15.0-30-generic&apos; -&gt; &apos;.config&apos; 4.安装编译器和其他必须的工具你必须安装有类似 GCC 或者相关联的工具来编译 Linux 内核。 使用下列的 apt 命令或 apt-get 命令来安装： 1sudo apt-get install build-essential libncurses-dev bison flex libssl-dev libelf-dev 尝试 yum 命令： 1sudo yum group install &quot;Development Tools&quot; 或 1sudo yum groupinstall &quot;Development Tools&quot; 额外的包： 1sudo yum install ncurses-devel bison flex elfutils-libelf-devel openssl-devel 如何在 Fedora Linux 上安装GCC 和其它开发工具运行下列 dnf 命令： 12sudo dnf group install &quot;Development Tools&quot;sudo dnf ncurses-devel bison flex elfutils-libelf-devel openssl-devel 5.编译内核开始编译并创建一个压缩好的内核镜像，输入： 1make 为了加速编译时间，传递 -j 参数： 1234## use 4 core/thread ##$ make -j 4## get thread or cpu core count using nproc command ##$ make -j $(nproc) 编译和构建 linux 内核将会话费大量时间，这取决于你的系统资源（例如：CPU）。 安装 Linux 内核模块1sudo make modules_install 安装 Linux 内核到目前为止我们已经编译了 Linux 内核并且安装了内核模块。是时候安装内核本身了： 1sudo make install 这将安装三个文件在 /boot 目录并修改你的内核 grub 配置文件： initramfs-4.19.1.img System.map-4.19.1 vmlinuz-4.19.1 更新 grub 配置你需要修改 Grub 2 bootloader 配置。 输入下列 shell 命令 CentOS/RHEL/Oracle/Scientific and Fedora Linux12sudo grub2-mkconfig -o /boot/grub2/grub.cfgsudo grubby --set-default /boot/vmlinuz-4.19.1 Debian/Ubuntu Linux12sudo update-initramfs -c -k 4.19.1sudo update-grub 重启系统： 1reboot 在重启后验证 Linux 内核是否为最新版本 1uname -mrs","link":"/2018/12/05/从源码编译和安装Linux内核/"},{"title":"自己动手写Shell(一）——C实现","text":"本文参考自Write a Shell in C Shell的基本生命周期 让我们自顶向下的思考一下 Shell。一个 Shell 在它的生命周期里主要做了三件事。 初始化：Shell 会在初始化时读入和执行配置文件。这会改变 Shell 接下来各方面的行为。 解释：Shell 在解释阶段（也就是等待用户输入的阶段）读入标准输入的命令并解释执行。 终结：在用户输入 shutdown 命令后，Shell 会释放掉占用的内存并终结自己。 这些步骤是很通用的，它们可以应用在任何程序中，我们将在我们的 Shell 中利用它们作为基础。我们的 Shell会足够简单，以至于没有任何配置文件，也不会有任何 shutdown 命令。因此，我们仅仅调用循环函数然后结束它。但需要注意的是，在程序的生命周期中循环只是一个基础的组成部分，真正的架构往往会复杂的多。 1234567891011int main(int argc, char **argv){ // Load config files, if any. // Run command loop. lsh_loop(); // Perform any shutdown/cleanup. return EXIT_SUCCESS;} 可以在上面的代码中看到，我只使用了一个函数，lsh_loop()。它将会循环执行并解释命令。我们将在接下来的部分看到它的具体实现。 Shell 的基本循环我们已经思考过 Shell 程序是如何启动的。现在，考虑基本的程序逻辑：Shell 在执行循环的时候做了什么？ 答案是以下三点： 读入：从标准输入中读入命令 解析：将执行的命令和其参数解析出来输入程序执行 执行：根据命令和参数执行程序 将以上三点表述成代码放入 lsh_loop(): 12345678910111213141516void lsh_loop(void){ char *line; char **args; int status; do { printf(\"&gt; \"); line = lsh_read_line(); args = lsh_split_line(line); status = lsh_execute(args); free(line); free(args); } while (status);} 让我们看看这段代码。开头是几行声明语句。后面的 do-while 循环在检查变量的状态方面更加地方便，因为在检查之前就已经执行了一次了。在循环里，我们打印了一个提示符，调用了一个函数来读入一行，再调用了一个函数来划分读入行的参数，然后执行这些参数。最后我们释放了 line 和 arguments 变量。注意我们使用了一个状态变量 status （由 lsh_execute()返回）来决定何时终止循环。 读入一行从标准输入中读入一行听起来简单，但用 C 实现起来还是有点麻烦的。难受的事情是，你并不知道在这一段时间里用户会往 shell 中输入多少文本。你不能简单地就分配一个块并期望输入不会溢出。而是要在启动的时候分配一个块，然后在溢出的时候重新分配更多的空间。这在 C 里是一个普遍的策略，我们将会在 lsh_read_line()中实现。 12345678910111213141516171819202122232425262728293031323334353637#define LSH_RL_BUFSIZE 1024char *lsh_read_line(void){ int bufsize = LSH_RL_BUFSIZE; int position = 0; char *buffer = malloc(sizeof(char) * bufsize); int c; if (!buffer) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } while (1) { // Read a character c = getchar(); // If we hit EOF, replace it with a null character and return. if (c == EOF || c == '\\n') { buffer[position] = '\\0'; return buffer; } else { buffer[position] = c; } position++; // If we have exceeded the buffer, reallocate. if (position &gt;= bufsize) { bufsize += LSH_RL_BUFSIZE; buffer = realloc(buffer, bufsize); if (!buffer) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } } }} 第一部分有很多的声明。函数的主要内容在while(1)循环中（显然是无限循环）。在循环中，我们读入一个字符并把它存储为 int 而不是 char ，这很重要！EOF是一个 Integer，而不是一个 Character。如果你想在条件语句中检查它，就得把它声明为 int。这是一个 C 语言初学者普遍会犯的错误。如果读入的是一个新行或者 EOF，我们会终止当前的读入并返回。否则我们将读到的字符添加到缓存字符串中。 接下来，我们会判断新的字符是否会超过当前的缓存大小。如实是的话，我们在继续读入前重新分配我们的缓存大小（同时检查分配错误）。这些都是值得做的。 对较新版本 C 函数库比较熟悉的人可能会察觉到在 stdio.h 中的 getline() 可以完成大部分我们实现的功能。但是尝试着自己实现一下 C 标准库的函数也没什么不好。使用 getline 的代码如下： 1234567char *lsh_read_line(void){ char *line = NULL; ssize_t bufsize = 0; // have getline allocate a buffer for us getline(&amp;line, &amp;bufsize, stdin); return line;} 解析一行现在我们再看最初的循环，我们已经实现了 lsh_read_line()，所以可以获取读入的行了。之后我们需要把读入的行解析为一系列参数。这里我将做一些简化，不允许在命令行参数中出现引号和反斜杠，而是简单地通过空格来分隔每一个参数。因此命令echo &quot;this message&quot; 将不会通过一个参数调用echo，而是通过两个参数&quot;this&quot;和message&quot;调用。 在这些简化后，我们需要做的就是使用空格作为分隔符来对输入字符串做词法分析。这意味着我们可以调用标准库函数 strtok 来为我们做一些麻烦的事。 1234567891011121314151617181920212223242526272829303132#define LSH_TOK_BUFSIZE 64#define LSH_TOK_DELIM \" \\t\\r\\n\\a\"char **lsh_split_line(char *line){ int bufsize = LSH_TOK_BUFSIZE, position = 0; char **tokens = malloc(bufsize * sizeof(char*)); char *token; if (!tokens) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } token = strtok(line, LSH_TOK_DELIM); while (token != NULL) { tokens[position] = token; position++; if (position &gt;= bufsize) { bufsize += LSH_TOK_BUFSIZE; tokens = realloc(tokens, bufsize * sizeof(char*)); if (!tokens) { fprintf(stderr, \"lsh: allocation error\\n\"); exit(EXIT_FAILURE); } } token = strtok(NULL, LSH_TOK_DELIM); } tokens[position] = NULL; return tokens;} 这段代码看起来和之前的 lsh_read_line() 很类似，我们使用了同样的缓存和动态拓展策略。但这一次我们使用了 null 终结的指针数组而不是 null 终结的字符数组。 在一开始，我们调用了strtok来划分词元。它返回第一个词元的指针。strtok() 事实上做的是返回你给的字符串内部的指针，并将每一个词元的末尾（分割符所在地址）置\\0 （C语言中表示字符串结束的标志）。我们将每一个字符指针保存在 tokens 中。 最后，我们在需要的时候重新分配内存。这个过程将会重复进行直到所有的词元都被strtok返回，然后将 null 放在 tokens 末尾。 现在我们有了一组词元了，可以准备开始执行了。 Shell 启动进程现在我们来到了编写 Shell 的关键。启动线程是 Shell 的主要功能。所以编写一个 shell 意味着你需要明确地知道线程里发生了什么以及它们如何启动。因此在正式开写之前谈一谈 Unix 中的线程是很有必要的。 在 Unix 中仅有两种启动线程的方式。第一种是 Init 进程。当 unix 电脑启动时，它的内核会被加载。一旦内核被加载并初始化后，内核将会启动唯一的一个进程，Init 进程。Init 进程会在电脑启动后一直运行，并管理加载剩下的你所需要的进程。 因为大部分程序都不是 Init 进程，那么就只有一种常用的方式来启动进程了：fork() 系统调用。当这个函数被调用后，操作系统会构建一个该进程的副本并启动它。原进程称为父进程，新进程称为子进程。fork() 对子进程返回0，对父进程返回子进程的进程ID数（PID）。这意味着启动新进程的唯一方式是通过一个已存在的进程复制它自己并启动。 这存在一个问题。就是当你想要运行一个新程序的时候，你并不想要当前进程的复制——你就是想要运行一个不同的程序。这就是 exec() 系统调用做的事情。它用一个全新的程序替代了当前进程。这意味着当你调用exec() 时，操作系统将会终止当前进程，加载新的程序，然后在当前位置启动新的程序。一个程序不会从exec()中获得返回值（除非它报错）。 有了这两个系统调用，我们就有了在 Unix 系统中启动新进程的砖头。首先，一个已存的进程 fork 它本身得到两个一样的进程。然后子进程执行 exec() 来用新程序替换掉它自己。父进程可以继续做其它事情，或者使用wait() 系统调用来等待子进程执行完。 通过上述的背景知识的补充，下面的用于启动一个新程序的代码将会变得很好理解： 123456789101112131415161718192021222324int lsh_launch(char **args){ pid_t pid, wpid; int status; pid = fork(); if (pid == 0) { // Child process if (execvp(args[0], args) == -1) { perror(\"lsh\"); } exit(EXIT_FAILURE); } else if (pid &lt; 0) { // Error forking perror(\"lsh\"); } else { // Parent process do { wpid = waitpid(pid, &amp;status, WUNTRACED); } while (!WIFEXITED(status) &amp;&amp; !WIFSIGNALED(status)); } return 1;} 这个函数将我们之前得到的参数数组作为参数。之后调用fork() ，保存它的返回值。当fork() 返回时，我们实际上有两个正在运行的进程。子进程将会执行第一个 if 条件语句（where pid == 0）。 在子进程中，我们想要运行用户输入的命令。所以，我们使用了exec()的变种之一execvp()。execvp() 略微有一些不同。它将希望运行的程序名和一个字符串数组（也被成为’vector’,这就是其中’v’的由来）作为参数。其中的’p’表示不需要提供运行程序的完整路径，只需要给出它的名字，操作系统将会在系统路径中自动搜索它。 如果 exec 命令返回-1，我们就知道产生了一个 error。所以我们使用perror来打印系统的错误信息，和我们的 Shell 名一样，这样我们就知道这个 error 来自哪里了。之后，我们退出函数，使得 Shell 能继续运行。 第二个条件（pid &lt; 0）检查了是否 fork() 出现了error。如果是，我们打印 error 并继续运行 Shell——这里并没有提示用户出现了 error 并让他们决定是否需要退出。 第三个条件意味着fork() 成功执行了。父进程将会暂时挂起，等待子进程执行完毕。我们使用waitpid() 来等待进程状态的改变。不幸的是，waitpid() 有很多选项（像exec() ）。进程可以通过很多方式改变状态，并不是所有状态的改变都意味着进程结束。一个进程可以是退出（通常会留下错误码），也可以被信号杀死。我们使用waitpid() 提供的宏来等待进程退出或者被杀死。之后该函数将会返回1，作为调用函数的信号来提示应该再次进行输入了。 Shell 内置方法你也许已经察觉到了lsh_loop() 函数调用的是lsh_execute() 函数，但是在上一部分我们的函数名为lsh_launch() 。这是故意为之！大部分 Shell 执行的命令会启动一个进程，但并不是所有的命令都需要。它们中的一些就内置在 Shell 中。 比如你想要改变当前目录，你需要使用函数chdir() 。问题在于，目录是当前进程的一项属性。也就是说，如果你写了一个程序调用了cd 改变了目录，它将改变调用程序自己的当前目录。它的父进程的当前目录并没有改变。所以是 Shell 程序自身需要执行chdir() ，来更新它自己的当前目录。这样才能在启动子进程后，子进程们才会继承这个更新后的目录。 类似的，如果程序名为exit ，它将不会退出调用它的 Shell，而是退出直接调用的子进程。这个命令也需要内置在 Shell 里。大部分 Shell 通过运行配置脚本（~/.bashrc）来配置。这些脚本使用能改变 Shell 本身行为的命令。这些命令都是内置在 Shell 里的。 所以我们添加一些命令在 Shell 里是很有意义的。我添加在我的 Shell 里的命令是 cd，exit 和 help。它们的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* Function Declarations for builtin shell commands: */int lsh_cd(char **args);int lsh_help(char **args);int lsh_exit(char **args);/* List of builtin commands, followed by their corresponding functions. */char *builtin_str[] = { \"cd\", \"help\", \"exit\"};int (*builtin_func[]) (char **) = { &amp;lsh_cd, &amp;lsh_help, &amp;lsh_exit};int lsh_num_builtins() { return sizeof(builtin_str) / sizeof(char *);}/* Builtin function implementations.*/int lsh_cd(char **args){ if (args[1] == NULL) { fprintf(stderr, \"lsh: expected argument to \\\"cd\\\"\\n\"); } else { if (chdir(args[1]) != 0) { perror(\"lsh\"); } } return 1;}int lsh_help(char **args){ int i; printf(\"Stephen Brennan's LSH\\n\"); printf(\"Type program names and arguments, and hit enter.\\n\"); printf(\"The following are built in:\\n\"); for (i = 0; i &lt; lsh_num_builtins(); i++) { printf(\" %s\\n\", builtin_str[i]); } printf(\"Use the man command for information on other programs.\\n\"); return 1;}int lsh_exit(char **args){ return 0;} 这段代码包含三个部分。第一个部分是内置函数的前向声明。一个前向声明意味着你声明了一个函数（并没有定义），因此你能在定义它之前使用它的函数名。这样做的原因是lsh_help() 使用了内置函数名数组，数组中包含了lsh_help() 。打破这个依赖循环的最清晰的方式就是前向声明。 下一个部分是一个包含了内建命令名的数组以及一个包含命令名对应函数的数组。也就是说，在将来想要添加内建命令的时候只要修改这些数组就好了，不需要在代码的某处地方编辑一个非常复杂的 switch 语句。如果你对builtin_func 的声明感到困惑，那就对了！这是一个包含函数指针的数组，它将字符串数组作为输入并返回一个整型数。在 C 语言里，任何涉及函数指针的声明都是很复杂的，这样做的好处是，当你想要调用一系列函数时可以直接通过数组名加索引调用，而不是用硬编码的方式直接调用函数本身。 执行命令最后要做的就是实现 lsh_execute() ，这个函数将会启动新线程或者调用内置方法。 1234567891011121314151617int lsh_execute(char **args){ int i; if (args[0] == NULL) { // An empty command was entered. return 1; } for (i = 0; i &lt; lsh_num_builtins(); i++) { if (strcmp(args[0], builtin_str[i]) == 0) { return (*builtin_func[i])(args); } } return lsh_launch(args);} 这段代码做的就是检查输入命令是否是内置命令，如果是的话就运行它。如果不是则调用lsh_launch() 启动一个新进程。 整合代码到这里一个简易 Shell 所需要的所有代码就都实现了。将上面所有的代码片段复制到 main.c 文件里。在将下列头文件包含在文件起始位置。 1#include &lt;sys/wait.h&gt; waitpid() and associated macros 1#include &lt;unistd.h&gt; chdir() fork() exec() pid_t 1#include &lt;stdlib.h&gt; malloc() realloc() free() exit() execvp() EXIT_SUCCESS, EXIT_FAILURE 1#include &lt;stdio.h&gt; fprintf() printf() stderr getchar() perror() 1#include &lt;string.h&gt; strcmp() strtok() 保存退出后，执行 gcc -o main main.c 进行编译，然后执行./main 运行，lsh就启动了！","link":"/2018/12/10/自己动手写Shell——C实现/"},{"title":"基于深度学习的图像语义分割综述（译）","text":"一、语义分割是什么？语义分割，指在图像上找到具有同一语义的像素，并将其归为一类，是计算机视觉中的关键问题之一，属于高级的视觉任务，是完全场景理解的基础。许多新兴的应用需要精准和高效的语义分割：自动驾驶、室内导航、虚拟现实和增强现实。这些需求随着深度学习方法的崛起一同迸发，衍生了大量计算机视觉相关的应用。具体的应用如下图所示。 除了要识别摩托车和车上的人外，我们还需要描绘出每个物体的边界。所以，不同于分类任务，我们需要从我们的模型中得到密集的像素集的预测结果。 VOC2012和MSCOCO是语义分割任务中最成功的数据集。 ##二、两种方法的区别 在深度学习的方法流行之前，人们使用 TextonForest^1 和 Random Forest based classifiers^2 来进行语义分割。说到图片分类，卷积神经网络（CNN）在分割任务上也有大量成功的应用。 最早将深度学习成功应用于分割任务的方法是 patch classification^3,它将每一个像素根据周围的图片块分别分类。使用图片块的主要原因是分类网络通常有全连接层并且需要固定大小的图片。 在2014年，伯克利的 Lond 等人提出了 Fully Convolutional Nerworks(FCN)^4。推广了基于 CNN 的网络架构，在不使用全连接层的情况下进行密集预测。这使得用任意大小的图片来生成分割图成为可能，并且比块分类的速度更快。后来几乎所有的语义分割领域最优秀的成果都是对 FCN 进行的改进。FCN 可以称作是深度学习在语义分割领域的里程碑。 除了全连接层外，另一个使用 CNN 进行语义分割时会遇到的问题是池化层。池化层能够增加感知野并且能够在丢弃部分 ‘where’ 信息的同时聚合上下文信息。然而语义分割需要对像素的分类有明确的区分，需要将 ‘where’ 信息保留。有两种不同的网络架构可用来解决这个问题。 首先是编码-解码架构。编码通常是配合着池化层降低了空间维度，解码则是逐渐恢复对象细节和空间维度。网络中有很多直连编码器与解码器的链接，来复制解码器更好地恢复对象细节。U-Net^5 是这一架构的经典实现。 第二种架构是使用 dilated/atrous convolutions^6来替代传统的卷积操作。 Conditional Random Field(CRF) postprocessing^7通常用来提升语义分割的效果。CRFs 是一种图模型，可以使得语义分割的结果更加平滑，对于强度相似的像素更趋向于把它们标为一类。CRFs 可以提高 1~2% 的分数。 三、论文汇总接下来我将介绍一些自 FCN 以来的能代表语义分割领域进展的论文（所有的论文都以 VOC2012 作为基准） FCN SegNet Dilated Convolutions DeepLatb(v1 &amp; v2) DeepLab v3 对于每一篇论文，我列出了它们的主要贡献，并对主要工作进行了说明，并在最后列出了它们在 VOC2012 数据集上的得分，这样就能更直观地感受到语义分割领域的发展进程。 1.FCN^4 Fully Convolutional Networks for Semantic Segmentation Submitted on 14 Nov 2014 关键贡献： 推广了端到端的卷积神经网络在语义分割上的应用 将 ImageNet 上的预训练模型应用在了语义分割上 使用反卷积层上采样 引入了跳转链接来提高上采样时的稀疏性 概要： FCN的主要贡献是提出了利用分类网络中的全连接层来对输入图片的全部区域做卷积。这和传统的分类网络在输入图片块上做的操作是等价的，但是全连接层更加的高效，因为卷积核的共享权重机制。尽管这点贡献并不是这篇 Paper 独家提供的，它仍然将当时 VOC2012 数据集的分割效果做到了最好。 在对 ImageNet 上预训练的模型 VGG 进行全连接层的卷积操作后，特征图仍然需要上采样，因为 CNN 中的池化操作已经对特征图降了维。不同于使用简单的双线性插值，使用反卷积层可以自动学习插值的功能。这一层也被称为上卷积、全卷积、反置卷积等。 因为在池化过程中部分信息的丢失，反卷积操作只能产生稀疏的分割图。因此，用跳转链接将高分辨率的特征图引入就是非常有必要的了。 Benchmarks(VOC2012): 2.SegNet^8 SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation Submitted on 2 Nov 2015 关键贡献 使用Maxpooling indices来增强位置信息。 概要： FCN的upconvolution层+shortcut connections产生的分割图比较粗糙，因此SegNet增加了更多的shortcut connections。不过，SegNet并不是直接将encoder的特征进行直接复制，而是对maxpooling中的indices进行复制，这使得SegNet的效率更高。 Benchmarks(VOC2012): 3.Dilated convolution^9 Multi-Scale Context Aggregation by Dilated Convolutions Submitted on 23 Nov 2015 关键贡献： 使用空洞卷积用来进行稠密预测（dense prediction）。 提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。 概要： pooling操作可以增大感受野，对于图像分类任务来说这有很大好处，但由于pooling操作降低了分辨率，这对语义分割来说很不利。因此作者提出一种叫做dilated convolution的操作来解决这个问题。dilated卷积(在deeplab中称为atrous卷积)。可以很好地提升感受野的同时可以保持空间分辨率。 网络架构有两种，一种是前端网络，另外一种是前端网络+上下文模块，分别介绍如下： 将VGG网络的最后两个pooling层给拿掉了，之后的卷积层被dilated 卷积取代。并且在pool3和pool4之间空洞卷积的空洞率=2，pool4之后的空洞卷积的空洞率=4。作者将这种架构称为前端（front-end）。 除了前端网络之外，作者还设计了一种叫做上下文模块（context module）的架构，加在前端网络之后。上下文木块中级联了多种不同空洞率的空洞卷积，使得多尺度的上下文信息可以得到整合，从而改善前端网络预测的效果。需要注意的是前端网络和上下文木块是分开训练的，因为作者在实验中发现，如果是联合在一起进行端对端的训练并不能改善性能。 Benchmarks(VOC2012): 4.DeepLab(v1,v2)^10 v1: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs Submitted on 22 Dec 2014 v2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs Submitted on 2 Jun 2016 关键贡献： 使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率。 提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息。 使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。 概述： 空洞卷积可以在不增加参数的情况下增加感受野。 通过两种方式来进行多尺度的处理：A.将原始图像的多种尺度喂给网络进行训练。B.通过平行的不同空洞率的空洞卷积层来获得。 通过全连接条件随机场来进行后处理，以改善分割结果。 Benchmarks(VOC2012): 5.DeepLab(v3)^11 Rethinking Atrous Convolution for Semantic Image Segmentation Submitted on 17 Jun 2017 关键贡献： 提升了 atrous 空间金字塔池化（ASPP） 使用了层叠的 atrous 卷积的模型 概要： 使用 dilated/atrous 卷积优化了残差模型。提到了使用串联的图片级的特征来提升 ASPP，这种特征由不同比例的1x1卷积和3x3 atrous 卷积生成。在每一个并行的卷积层后也用到了批正态化。 层叠模型由多个残差块组成，这些残差块中的卷积层由不同比例的 atrous 卷积层构成。这个模型和 dilated 卷积中的模型很像，但它直接应用了中间的特征图，而不是 belief maps（belief maps 是CNN中最后的等同于数字分类的特征图）。 这两个提出的模型可以分别被独立验证，将两个模型混合并不能提升整体的效果。两个模型都能在验证集上取得不错的效果，ASPP 更好一些。而 CRF 并没有被应用在其中。 这两个模型的性能都优于 DeepLabv2。作者提到了性能的提升主要来自于批正态化和更好地对多维度特征图进行编码的方法。 Benchmarks(VOC):","link":"/2018/12/14/基于深度学习的图像语义分割综述（译）/"},{"title":"自己动手写Shell(二)——Python实现","text":"Shell的基本生命周期无论使用什么语言实现，Shell 的生命周期都是一样的，主要做三件事： 初始化：Shell 会在初始化时读入和执行配置文件。这会改变 Shell 接下来各方面的行为。 解释：Shell 在解释阶段（也就是等待用户输入的阶段）读入标准输入的命令并解释执行。 终结：在用户输入 shutdown 命令后，Shell 会释放掉占用的内存并终结自己。 使用Python实现的 Shell 主函数如下： 1234567def main(): # 在执行 shell_loop 函数进行循环监听之前，首先进行初始化 # 即建立命令与函数映射关系表 init() # 预处理命令的主程序 shell_loop() 和 C 中的主函数相比，多了一个init() 函数，该函数是用来注册函数关系映射表的，在 Shell 正式启动前将一些内置的命令先加载好，各个内置的命令写在主目录下的 func 文件夹下，每一个命令都对应一个.py文件。这个功能在 C 中的 Shell也有实现，只不过是通过字符串数组和函数指针数组实现的，且全部写在 main.c 文件里。在 Python 中这样实现的目的是为了降低代码的耦合度，使得增添命令变得更加灵活。如果想为我们的 Shell 增加或减少一个命令，只需要在 func 文件下增加 或减少一个.py文件即可，不需要频繁改动主函数。 init() 函数的实现如下（暂时只实现了这四个命令）： 12345678910111213141516def register_command(name, func): \"\"\" 注册命令，使命令与相应的处理函数建立映射关系 @Param name:命令名 @param func:函数名 \"\"\" built_in_cmds[name] = func def init(): \"\"\" 注册所有命令 \"\"\" register_command(\"cd\", cd) register_command(\"exit\", exit) register_command(\"getenv\", getenv) register_command(\"history\", history) Shell的基本循环Shell 的基本循环也是通用的以下三点： 读入：从标准输入中读入命令 解析：将执行的命令和其参数解析出来输入程序执行 执行：根据命令和参数执行程序 用 Python 的实现如下： 123456789101112131415161718192021222324252627282930def shell_loop(): status = SHELL_STATUS_RUN while status == SHELL_STATUS_RUN: #打印命令提示符，如'[&lt;user&gt;@&lt;hostname&gt; &lt;base_dir&gt;]$' display_cmd_prompt() #忽略 Ctrl-Z 或Ctrl-C 信号 ignore_signals() try: #读取命令 cmd = sys.stdin.readline() #解析命令 #将命令进行拆分，返回一个列表 cmd_tokens = tokenize(cmd) #预处理函数 #将命令中的环境变量用真实值进行替换 #比如讲$HOME这样的变量替换为实际值 cmd_tokens = preprocess(cmd_tokens) #执行命令，并返回shell的状态 status = execute(cmd_tokens) except: #sys.exc_info函数返回一个包含三个值的远足（type,value,traceback) #这三个值产生于最近一次被处理的异常 #而我们只需要获取中间的值 _, err, _ = sys.exc_info() print(err) 虽然没有 C 中实现的那么简洁清楚，但我增加了更多的功能。比如：打印命令提示符，在 C 中只是打印一个’&gt;’；忽略 Ctrl-z 和 Ctrl-c 终止信号；在循环内加入了异常检测语句。这样的 Shell 比起 C 里的 Shell 看起来就显得高大上多了。 显示命令提示符的代码如下： 12345678910111213141516171819202122232425#展示命令提示符，行如'[&lt;user&gt;@&lt;hostname&gt; &lt;base_dir&gt;]$'def display_cmd_prompt(): # getpass.getuser 用于获取当前用户名 user = getpass.getuser() # socket.gethostname() 返回当前运行 python 程序的机器的主机名 hostname = socket.gethostname() # 获取当前工作路径 cwd = os.getcwd() # 获取路径 cwd 的最低一级目录 base_dir = os.path.basename(cwd) # 如果用户当前位于用户的根目录之下，使用'~'代替目录名 home_dir = os.path.expanduser('~') if cwd == home_dir: base_dir = '~' # 输出命令提示符 if platform.system() != \"Windows\": sys.stdout.write(\"[\\033[1;33m%s\\033[0;0m@%s \\033[1;36m%s\\033[0;0m] $\"%(user, hostname, base_dir)) else: sys.stdout.write(\"[%s@%s %s]$ \" % (user, hostname, base_dir)) sys.stdout.flush() 分别通过 Python 标准库中的函数获取了当前用户名、主机名和工作路径。并判断如果是用户目录则用’~’替代。对于非 Windows 系统输出带颜色的代码。 忽略终止信号的代码如下： 12345678def ignore_signals(): if platform.system() != \"Window\": # 忽略 Ctrl-Z 信号 signal.signal(signal.SIGTSTP, signal.SIG_IGN) #忽略 Ctrl-C 信号 signal.signal(signal.SIGINT, signal.SIG_IGN) 在读入命令时直接使用了 Python 中的标准读入，所以就没有单独为读入写一个函数了。 解析一行首先将读到的行进行分词，我直接使用了 Python 库 shlex 中的 split() 方法，该方法按 Shell 的语法规则对字符串进行分割。 1234567def tokenize(string): # 将 string 按 shell 的语法规则进行分割 # 返回 string 的分割列表 # 其实就是按空格符将命令与参数分开 # 比如， 'ls -l /home/yifeng' 划分之后就是 # ['ls', '-l', '/home/yifeng'] return shlex.split(string) 相较于 C 中的 Shell 分词之后还增加了一个预处理函数，将输入的环境变量进行替换，这样就可以输入变量了。 预处理函数如下： 123456789101112def preprocess(tokens): # 用于存储处理之后的 token processed_token = [] for token in tokens: if token.startswith('$'): # os.getenv() 用于获取环境变量的值，比如 'HOME' # 变量不存在则返回空 processed_token.append(os.getenv(token[1:])) else: processed_token.append(token) return processed_token 解释执行在读取了预处理后的命令行参数后，便可以进行解释执行了，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738def execute(cmd_tokens): # 'a' 模式表示以添加的方式打开指定文件 # 这个模式下文件对象的 write 操作不会覆盖文件原有的信息，而是添加到文件原有信息之后 with open(HISTORY_PATH, 'a') as history_file: history_file.write(' '.join(cmd_tokens) + os.linesep) if cmd_tokens: # 获取命令 cmd_name = cmd_tokens[0] # 获取命令参数 cmd_args = cmd_tokens[1:] # 如果当前命令在命令表中 # 则传入参数，调用相应的函数进行执行 if cmd_name in built_in_cmds: return built_in_cmds[cmd_name](cmd_args) # 监听 Ctrl-C 信号 signal.signal(signal.SIGINT, handler_kill) # 如果当前系统不是 Windows # 则创建子进程 if platform.system() != \"Windows\": # Unix 平台 # 调用子进程执行命令 p = subprocess.Popen(cmd_tokens) #父进程从子进程读取数据，直到读取到EOF # 这里主要用来等待子进程终止运行 p.communicate() else: # Windows 平台 command = \"\" command = ' '.join(cmd_tokens) # 执行 command os.system(command) # 返回状态 return SHELL_STATUS_RUN 由于实现了 history 命令，所以在每一次执行命令时都会将该命令写入保存历史命令的文件 history_file 中，方便调用 history() 时读取。接下来判断输入的命令是否是内置实现的命令，如果是，则直接调用。不是的话则会判断当前操作系统类型，并根据操作系统类型调用不同的执行命令的函数。 总结至此，用 Python 实现的 Shell 就全部介绍完毕了。相较于 C 中的实现，我做了以下几点优化： 优化了命令提示符功能，看起来更专业 低耦合的设计，可以动态地添加删除命令 根据不同的操作系统类型进行了适配（C中的实现仅支持 Linux） 可以在输入中使用变量 总之，Shell 就是这么一个读取用户命令，并将其解释执行的程序。通过实现这么一个简单的 Shell 让我对平时编程最常打交道的程序有了更清楚的认识，十分有趣。强烈建议大家自己动手实现一遍。 最后是各个命令的具体实现代码： cd: 12345678from .constants import *def cd(args): if len(args) &gt; 0: os.chdir(args[0]) else: os.chdir(os.getenv('HOME')) return SHELL_STATUS_RUN exit: 1234from .constants import *def exit(args): return SHELL_STATUS_STOP getenv: 123456from .constants import *def getenv(args): if len(args) &gt; 0: print(os.getenv(args[0])) return SHELL_STATUS_RUN history: 1234567891011121314151617import sysfrom .constants import *def history(args): with open(HISTORY_PATH, 'r') as history_file: lines = history_file.readlines() limit = len(lines) if len(args) &gt; 0: limit = int(args[0]) start = len(lines) - limit for line_num, line in enumerate(lines): if line_num &gt;= start: sys.stdout.write('%d %s' %(line_num + 1, line)) sys.stdout.flush() return SHELL_STATUS_RUN","link":"/2018/12/19/自己动手写Shell——Python实现/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"image segmentation","slug":"image-segmentation","link":"/tags/image-segmentation/"}],"categories":[{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/数据挖掘/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"}]}