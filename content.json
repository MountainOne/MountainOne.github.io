{"pages":[],"posts":[{"title":"基于深度学习的图像语义分割综述（译）","text":"一、语义分割是什么？语义分割，指在图像上找到具有同一语义的像素，并将其归为一类，是计算机视觉中的关键问题之一，属于高级的视觉任务，是完全场景理解的基础。许多新兴的应用需要精准和高效的语义分割：自动驾驶、室内导航、虚拟现实和增强现实。这些需求随着深度学习方法的崛起一同迸发，衍生了大量计算机视觉相关的应用。具体的应用如下图所示。 除了要识别摩托车和车上的人外，我们还需要描绘出每个物体的边界。所以，不同于分类任务，我们需要从我们的模型中得到密集的像素集的预测结果。 VOC2012和MSCOCO是语义分割任务中最成功的数据集。 ##二、两种方法的区别 在深度学习的方法流行之前，人们使用 TextonForest^1 和 Random Forest based classifiers^2 来进行语义分割。说到图片分类，卷积神经网络（CNN）在分割任务上也有大量成功的应用。 最早将深度学习成功应用于分割任务的方法是 patch classification^3,它将每一个像素根据周围的图片块分别分类。使用图片块的主要原因是分类网络通常有全连接层并且需要固定大小的图片。 在2014年，伯克利的 Lond 等人提出了 Fully Convolutional Nerworks(FCN)^4。推广了基于 CNN 的网络架构，在不使用全连接层的情况下进行密集预测。这使得用任意大小的图片来生成分割图成为可能，并且比块分类的速度更快。后来几乎所有的语义分割领域最优秀的成果都是对 FCN 进行的改进。FCN 可以称作是深度学习在语义分割领域的里程碑。 除了全连接层外，另一个使用 CNN 进行语义分割时会遇到的问题是池化层。池化层能够增加感知野并且能够在丢弃部分 ‘where’ 信息的同时聚合上下文信息。然而语义分割需要对像素的分类有明确的区分，需要将 ‘where’ 信息保留。有两种不同的网络架构可用来解决这个问题。 首先是编码-解码架构。编码通常是配合着池化层降低了空间维度，解码则是逐渐恢复对象细节和空间维度。网络中有很多直连编码器与解码器的链接，来复制解码器更好地恢复对象细节。U-Net^5 是这一架构的经典实现。 第二种架构是使用 dilated/atrous convolutions^6来替代传统的卷积操作。 Conditional Random Field(CRF) postprocessing^7通常用来提升语义分割的效果。CRFs 是一种图模型，可以使得语义分割的结果更加平滑，对于强度相似的像素更趋向于把它们标为一类。CRFs 可以提高 1~2% 的分数。 三、论文汇总接下来我将介绍一些自 FCN 以来的能代表语义分割领域进展的论文（所有的论文都以 VOC2012 作为基准） FCN SegNet Dilated Convolutions DeepLatb(v1 &amp; v2) DeepLab v3 对于每一篇论文，我列出了它们的主要贡献，并对主要工作进行了说明，并在最后列出了它们在 VOC2012 数据集上的得分，这样就能更直观地感受到语义分割领域的发展进程。 1.FCN^4 Fully Convolutional Networks for Semantic Segmentation Submitted on 14 Nov 2014 关键贡献： 推广了端到端的卷积神经网络在语义分割上的应用 将 ImageNet 上的预训练模型应用在了语义分割上 使用反卷积层上采样 引入了跳转链接来提高上采样时的稀疏性 概要： FCN的主要贡献是提出了利用分类网络中的全连接层来对输入图片的全部区域做卷积。这和传统的分类网络在输入图片块上做的操作是等价的，但是全连接层更加的高效，因为卷积核的共享权重机制。尽管这点贡献并不是这篇 Paper 独家提供的，它仍然将当时 VOC2012 数据集的分割效果做到了最好。 在对 ImageNet 上预训练的模型 VGG 进行全连接层的卷积操作后，特征图仍然需要上采样，因为 CNN 中的池化操作已经对特征图降了维。不同于使用简单的双线性插值，使用反卷积层可以自动学习插值的功能。这一层也被称为上卷积、全卷积、反置卷积等。 因为在池化过程中部分信息的丢失，反卷积操作只能产生稀疏的分割图。因此，用跳转链接将高分辨率的特征图引入就是非常有必要的了。 Benchmarks(VOC2012): 2.SegNet^8 SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation Submitted on 2 Nov 2015 关键贡献 使用Maxpooling indices来增强位置信息。 概要： FCN的upconvolution层+shortcut connections产生的分割图比较粗糙，因此SegNet增加了更多的shortcut connections。不过，SegNet并不是直接将encoder的特征进行直接复制，而是对maxpooling中的indices进行复制，这使得SegNet的效率更高。 Benchmarks(VOC2012): 3.Dilated convolution^9 Multi-Scale Context Aggregation by Dilated Convolutions Submitted on 23 Nov 2015 关键贡献： 使用空洞卷积用来进行稠密预测（dense prediction）。 提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。 概要： pooling操作可以增大感受野，对于图像分类任务来说这有很大好处，但由于pooling操作降低了分辨率，这对语义分割来说很不利。因此作者提出一种叫做dilated convolution的操作来解决这个问题。dilated卷积(在deeplab中称为atrous卷积)。可以很好地提升感受野的同时可以保持空间分辨率。 网络架构有两种，一种是前端网络，另外一种是前端网络+上下文模块，分别介绍如下： 将VGG网络的最后两个pooling层给拿掉了，之后的卷积层被dilated 卷积取代。并且在pool3和pool4之间空洞卷积的空洞率=2，pool4之后的空洞卷积的空洞率=4。作者将这种架构称为前端（front-end）。 除了前端网络之外，作者还设计了一种叫做上下文模块（context module）的架构，加在前端网络之后。上下文木块中级联了多种不同空洞率的空洞卷积，使得多尺度的上下文信息可以得到整合，从而改善前端网络预测的效果。需要注意的是前端网络和上下文木块是分开训练的，因为作者在实验中发现，如果是联合在一起进行端对端的训练并不能改善性能。 Benchmarks(VOC2012): 4.DeepLab(v1,v2)^10 v1: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs Submitted on 22 Dec 2014 v2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs Submitted on 2 Jun 2016 关键贡献： 使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率。 提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息。 使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。 概述： 空洞卷积可以在不增加参数的情况下增加感受野。 通过两种方式来进行多尺度的处理：A.将原始图像的多种尺度喂给网络进行训练。B.通过平行的不同空洞率的空洞卷积层来获得。 通过全连接条件随机场来进行后处理，以改善分割结果。 Benchmarks(VOC2012): 5.DeepLab(v3)^11 Rethinking Atrous Convolution for Semantic Image Segmentation Submitted on 17 Jun 2017 关键贡献： 提升了 atrous 空间金字塔池化（ASPP） 使用了层叠的 atrous 卷积的模型 概要： 使用 dilated/atrous 卷积优化了残差模型。提到了使用串联的图片级的特征来提升 ASPP，这种特征由不同比例的1x1卷积和3x3 atrous 卷积生成。在每一个并行的卷积层后也用到了批正态化。 层叠模型由多个残差块组成，这些残差块中的卷积层由不同比例的 atrous 卷积层构成。这个模型和 dilated 卷积中的模型很像，但它直接应用了中间的特征图，而不是 belief maps（belief maps 是CNN中最后的等同于数字分类的特征图）。 这两个提出的模型可以分别被独立验证，将两个模型混合并不能提升整体的效果。两个模型都能在验证集上取得不错的效果，ASPP 更好一些。而 CRF 并没有被应用在其中。 这两个模型的性能都优于 DeepLabv2。作者提到了性能的提升主要来自于批正态化和更好地对多维度特征图进行编码的方法。 Benchmarks(VOC):","link":"/2018/12/14/基于深度学习的图像语义分割综述（译）/"},{"title":"从源码编译和安装Linux内核","text":"如何编译和安装Linux内核可以按如下的步骤编译和安装Linux内核 1.从 kernel.org 抓取最新版本的内核 2.验证内核 3.解压内核压缩文件 4.复制已存的 Linux 内核配置文件 5.编译和构建内核 6.安装内核和模块 7.更新 Grub 配置 8.重启系统 1.获取最新版本的源代码访问Linux官方网站下载最新的源代码。点击标有“Latest Stable Kernel”的黄色大按钮。 也可以使用 wget 命令下载 Linux 内核源代码： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.xz 2.获取 tar.xz 文件使用 unzx 命令和 xz 命令解压压缩文件获取源代码： 1unzx -v linux-4.19.1.tar.xz 或者 1xz -d -v linux-4.19.1.tar.xz 使用pgp验证 linux 内核 tarball首先获取linux-4.19.1.tar 的 PGP 签名： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.sign 尝试进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 从 PGP 密钥服务器获取公钥来验证签名。例如：RSA key ID 79BE3E4300411886（从上面的输出结果中得到） 1gpg --recv-keys 79BE3E4300411886 示例输出： 1234567gpg: key 79BE3E4300411886: 7 duplicate signatures removedgpg: key 79BE3E4300411886: 172 signatures not checked due to missing keysgpg: /home/vivek/.gnupg/trustdb.gpg: trustdb createdgpg: key 79BE3E4300411886: public key &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; importedgpg: no ultimately trusted keys foundgpg: Total number processed: 1gpg: imported: 1 现在再一次使用 gpg 命令进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 12345678gpg: assuming signed data in &apos;linux-4.19.1.tar&apos;gpg: Signature made Sun 12 Aug 2018 04:00:28 PM CDTgpg: using RSA key 79BE3E4300411886gpg: Good signature from &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; [unknown]gpg: aka &quot;Linus Torvalds &lt;torvalds@linux-foundation.org&gt;&quot; [unknown]gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: ABAF 11C6 5A29 70B1 30AB E3C4 79BE 3E43 0041 1886 如果通过了验证，那么就使用 tar 进行解压： 1tar xvf linux-4.19.1.tar 3.配置 Linux 内核特征和模型在开始构建内核之前，你必须配置内核特征。你也必须明确你的系统所需要的内核模块（驱动）。这个对于新手来说是很有压力的。我建议你使用 cp 命令复制已有的配置文件： 12cd linux-4.19.1cp -v /boot/config-$(uname -r) .config 示例输出： 1&apos;/boot/config-4.15.0-30-generic&apos; -&gt; &apos;.config&apos; 4.安装编译器和其他必须的工具你必须安装有类似 GCC 或者相关联的工具来编译 Linux 内核。 使用下列的 apt 命令或 apt-get 命令来安装： 1sudo apt-get install build-essential libncurses-dev bison flex libssl-dev libelf-dev 尝试 yum 命令： 1sudo yum group install &quot;Development Tools&quot; 或 1sudo yum groupinstall &quot;Development Tools&quot; 额外的包： 1sudo yum install ncurses-devel bison flex elfutils-libelf-devel openssl-devel 如何在 Fedora Linux 上安装GCC 和其它开发工具运行下列 dnf 命令： 12sudo dnf group install &quot;Development Tools&quot;sudo dnf ncurses-devel bison flex elfutils-libelf-devel openssl-devel 5.编译内核开始编译并创建一个压缩好的内核镜像，输入： 1make 为了加速编译时间，传递 -j 参数： 1234## use 4 core/thread ##$ make -j 4## get thread or cpu core count using nproc command ##$ make -j $(nproc) 编译和构建 linux 内核将会话费大量时间，这取决于你的系统资源（例如：CPU）。 安装 Linux 内核模块1sudo make modules_install 安装 Linux 内核到目前为止我们已经编译了 Linux 内核并且安装了内核模块。是时候安装内核本身了： 1sudo make install 这将安装三个文件在 /boot 目录并修改你的内核 grub 配置文件： initramfs-4.19.1.img System.map-4.19.1 vmlinuz-4.19.1 更新 grub 配置你需要修改 Grub 2 bootloader 配置。 输入下列 shell 命令 CentOS/RHEL/Oracle/Scientific and Fedora Linux12sudo grub2-mkconfig -o /boot/grub2/grub.cfgsudo grubby --set-default /boot/vmlinuz-4.19.1 Debian/Ubuntu Linux12sudo update-initramfs -c -k 4.19.1sudo update-grub 重启系统： 1reboot 在重启后验证 Linux 内核是否为最新版本 1uname -mrs","link":"/2018/12/05/从源码编译和安装Linux内核/"},{"title":"Kaggle之房价预测","text":"Kaggle入门赛之房价预测 前言：最近实验室有一个数据挖掘的任务需要完成，是一个和国家电网相关的回归任务。因此我在 Kaggle 上找一些回归任务的比赛来练练手，房价预测这个比赛的难度比较适合我这种新手，因此选择了这个题目。本篇博客的大部分内容来自该比赛的一个高分 kernel, 感兴趣的同学可以直接跳转查看原博。 1.比赛介绍这是 Kaggle 上的一个为数据科学初学者准备的比赛，需要参赛者掌握一定的 R 或 Python 以及机器学习的基础知识。它给出了一个训练集和一个测试集，最终需要对测试集中的结果进行预测并提交。训练集中包含了79个特征以预测房价（SalePrice），包括：房屋的大小、地段、用途、基础设施等等。具体的描述可查看该比赛的比赛页面,比赛中所用到数据也均可通过比赛页面下载。 2.准备工作数据挖掘类的比赛最为重要的工作就是：特征工程。本篇博客将会用到的特征工程方法还是非常朴素的，具体如下： 通过已有的数据拟合曲线来填充缺失值。 变换某些看起来更像分类变量的数值型变量。 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。 对于一些倾斜特征采用 Box Cox 变换（替代对数变换）：这会得到一个更好的结果。 得到分类特征的假值。 之后我们将会选择一些基本模型（例如基于 sklearn 的 XGBoost），在 stacking/ensembling 之前在数据集上进行交叉验证。它的关键在于让（线性）模型对异常值鲁棒。这将同时提高积分板和交叉验证的分数。 接下来真正开始工作吧！首先导入相关的包和比赛所需的数据。 123456789101112131415161718192021222324#import some necessary librairiesimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)%matplotlib inlineimport matplotlib.pyplot as plt # Matlab-style plottingimport seaborn as snscolor = sns.color_palette()sns.set_style('darkgrid')import warningsdef ignore_warn(*args, **kwargs): passwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)from scipy import statsfrom scipy.stats import norm, skew #for some statisticspd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal pointsfrom subprocess import check_outputprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory sample_submission.csv test.csv train.csv 123456#Now let's import and put the train and test datasets in pandas dataframetrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv')##display the first five rows of the train dataset.train.head(5) 12##display the first five rows of the test dataset.test.head(5) 123456789101112131415#check the numbers of samples and featuresprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))print(\"The test data size before dropping Id feature is : {} \".format(test.shape))#Save the 'Id' columntrain_ID = train['Id']test_ID = test['Id']#Now drop the 'Id' colum since it's unnecessary for the prediction process.train.drop(\"Id\", axis = 1, inplace = True)test.drop(\"Id\", axis = 1, inplace = True)#check again the data size after dropping the 'Id' variableprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) print(\"The test data size after dropping Id feature is : {} \".format(test.shape)) 123456&gt;The train data size before dropping Id feature is : (1460, 81) &gt;The test data size before dropping Id feature is : (1459, 80) &gt;&gt;The train data size after dropping Id feature is : (1460, 80) &gt;The test data size after dropping Id feature is : (1459, 79) &gt; 3.数据处理异常值首先对数据集中的重要特征的异常值进行处理。 12345fig, ax = plt.subplots()ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 可从散点图中看到有两个非常明显的异常值，有着极大的房屋面积房价却很低。这两个值是非常典型的异常值，因此可以安全地删除它们。 123456789#Deleting outlierstrain = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)#Check the graphic againfig, ax = plt.subplots()ax.scatter(train['GrLivArea'], train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 注意：删除异常值并不总是安全的。我们决定删除这两个值是因为它俩太奇怪了（极大的空间有着极低的价格）。 数据集中仍然可能存在其它的异常值。然而删除所有的异常值可能对模型产生不好的影响，特别是当它们可能出现在测试集的时候。因此在处理异常值得时候最好不要全部删除掉，以提高模型的鲁棒性。 目标变量房价是我们需要预测的目标变量，因此首先对它进行一些分析。 1df_train['SalePrice'].describe() 12345678910&gt;count 1460.000000&gt;mean 180921.195890&gt;std 79442.502883&gt;min 34900.000000&gt;25% 129975.000000&gt;50% 163000.000000&gt;75% 214000.000000&gt;max 755000.000000&gt;Name: SalePrice, dtype: float64&gt; 12345678910111213141516sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 180932.92 and sigma = 79467.79&gt; 目标变量是接近正态分布的。线性模型在正态分布的数据上拟合效果更好，所以我们可以对数据做一些变换使其更接近正态分布。 目标变量的对数变换1234567891011121314151617181920#We use the numpy fuction log1p which applies log(1+x) to all elements of the columntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])#Check the new distribution sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 12.02 and sigma = 0.40&gt; 在经过对数变换后，房价数据接近于正态分布。 4.特征工程首先将训练数据和测试数据合并到一个数据帧。 123456ntrain = train.shape[0]ntest = test.shape[0]y_train = train.SalePrice.valuesall_data = pd.concat((train, test)).reset_index(drop=True)all_data.drop(['SalePrice'], axis=1, inplace=True)print(\"all_data size is : {}\".format(all_data.shape)) 12&gt;all_data size is : (2917, 79)&gt; 缺失数据1234all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head(20) 123456f, ax = plt.subplots(figsize=(15, 12))plt.xticks(rotation='90')sns.barplot(x=all_data_na.index, y=all_data_na)plt.xlabel('Features', fontsize=15)plt.ylabel('Percent of missing values', fontsize=15)plt.title('Percent missing data by feature', fontsize=15) 12&gt;Text(0.5,1,&apos;Percent missing data by feature&apos;) &gt; 数据关联1234#Correlation map to see how features are correlated with SalePricecorrmat = train.corr()plt.subplots(figsize=(12,9))sns.heatmap(corrmat, vmax=0.9, square=True) 输入缺失值根据不同特征的不同情况对缺失值进行填充。 对于离散值且数据说明里有 NA 选项，则将缺失值补为None。 PoolQC : 数据描述说 NA 表示 “没有泳池”。这是有意义的，因为有大量的缺失数据，这表明大多数的房子都没有游泳池。 1all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\") MiscFeature : 数据描述说 NA 表示“没有杂项特征”。 1all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\") Alley : 数据描述说 NA 表示“没有小巷联通”。 1all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") Fence : 数据描述说 NA 表示“没有棚栏”。 1all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\") 对于和其他特征有关联的值，可以先按其它特征分组，然后取中位数 。 LotFrontage : 因为房子附近的街道到房子的距离和到该房子临近的房子的距离是类似的，因此去临近房子的中位数来填充该距离。 123#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhoodall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform( lambda x: x.fillna(x.median())) 对于连续值，将缺失的值补为0。 BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath :缺失的值为0意味着没有基础设施。 12for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'): all_data[col] = all_data[col].fillna(0) 对于离散值且在数据说明里没有 NA 选项，则将缺失值填充为出现最多的值 。 MSZoning(大致区域分类)：‘RL’是最常见的值。所以我们使用‘RL’填充缺失值。 1all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0]) 对于所有值几乎都一样，参考意义不大的特征，直接删除 。 Utilities:该特征所有的值几乎都为‘AllPub’,除了一个‘NoSeWa’和2个NA。因为唯一的’NoSewa’出现在训练集，这个特征将不会对预测有帮助，因此可以安全地移除它。 1all_data = all_data.drop(['Utilities'], axis=1) 在缺失值处理完后，可以简单地查看是否还有未处理地缺失值： 12345#Check remaining missing values if any all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head() 更多特征工程将某些看起来是连续值的特征转换为离散值1234567891011#MSSubClass=The building classall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)#Changing OverallCond into a categorical variableall_data['OverallCond'] = all_data['OverallCond'].astype(str)#Year and month sold are transformed into categorical features.all_data['YrSold'] = all_data['YrSold'].astype(str)all_data['MoSold'] = all_data['MoSold'].astype(str) 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。1234567891011121314from sklearn.preprocessing import LabelEncodercols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')# process columns, apply LabelEncoder to categorical featuresfor c in cols: lbl = LabelEncoder() lbl.fit(list(all_data[c].values)) all_data[c] = lbl.transform(list(all_data[c].values))# shape print('Shape all_data: {}'.format(all_data.shape)) 增加一个更为重要的特征因为面积相关的特征对房价的影响是非常大的。所以我们增加了一个特征来描述基础设施的总面积，第一层和第二层的面积。 12# Adding total sqfootage feature all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] 斜率特征1234567numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index# Check the skew of all numerical featuresskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)print(\"\\nSkew in numerical features: \\n\")skewness = pd.DataFrame({'Skew' :skewed_feats})skewness.head(10) Box Cox变换高斜率特征1234567891011skewness = skewness[abs(skewness) &gt; 0.75]print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))from scipy.special import boxcox1pskewed_features = skewness.indexlam = 0.15for feat in skewed_features: #all_data[feat] += 1 all_data[feat] = boxcox1p(all_data[feat], lam) #all_data[skewed_features] = np.log1p(all_data[skewed_features]) 获取分类特征的假值12all_data = pd.get_dummies(all_data)print(all_data.shape) 获取新的训练集和测试集 12train = all_data[:ntrain]test = all_data[ntrain:] 5.模型导入相关库 12345678910from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsICfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.kernel_ridge import KernelRidgefrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clonefrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.metrics import mean_squared_errorimport xgboost as xgbimport lightgbm as lgb 定义交叉策略我们使用 Sklearn 中的 cross_cal_score 函数。然而这个函数没有扰乱属性，我们添加了一行代码，使得在交叉验证前对数据集进行扰乱。 1234567#Validation functionn_folds = 5def rmsle_cv(model): kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) return(rmse) 基础模型 LASSO 回归： 这个模型会对异常值特别敏感。所以我们需要让它变得更加鲁棒。在 pipeline 中使用 sklearn 中的 Robustscaler() 方法。 1lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) Elastic Net 回归： 1ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)) Kernel Ridge 回归： 1KRR = KernelRidge(alpha=0.6, kernel=&apos;polynomial&apos;, degree=2, coef0=2.5) Gradient Boosting 回归： 使用 huber loss 使其对异常值鲁棒 1234GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5) XGBoost: 123456model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state =7, nthread = -1) LightGBM: 123456model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11) 基本模型分数模型初始化好之后就开始测试一下交叉验证的损失分数吧。 12score = rmsle_cv(lasso)print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Lasso score: 0.1115 (0.0074)&gt; 12score = rmsle_cv(ENet)print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;ElasticNet score: 0.1116 (0.0074)&gt; 12score = rmsle_cv(KRR)print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Kernel Ridge score: 0.1153 (0.0075)&gt; 12score = rmsle_cv(GBoost)print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Gradient Boosting score: 0.1177 (0.0080)&gt; 12score = rmsle_cv(model_xgb)print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Xgboost score: 0.1161 (0.0079)&gt; 12score = rmsle_cv(model_lgb)print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std())) 12&gt;LGBM score: 0.1157 (0.0067)&gt; 堆叠模型（stacking model)最简单的堆叠方法：平均化基础模型我们首先用最简单的方法实验，构建一个新类来拓展 scikit-learn 中 model类，作为我们自己的堆叠模型。 平均化的基础模型类1234567891011121314151617181920class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, models): self.models = models # we define clones of the original models to fit the data in def fit(self, X, y): self.models_ = [clone(x) for x in self.models] # Train cloned base models for model in self.models_: model.fit(X, y) return self #Now we do the predictions for cloned models and average them def predict(self, X): predictions = np.column_stack([ model.predict(X) for model in self.models_ ]) return np.mean(predictions, axis=1) 平均化的基础模型分数1234averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))score = rmsle_cv(averaged_models)print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt; Averaged base models score: 0.1091 (0.0075)&gt; 从结果看来，简单的堆叠模型确实提高了分数。这将激励我们探索更为高效的堆叠模型。 不那么简单的堆叠模型：增加一个元模型在这个方法中，我们将会在简单堆叠模型的基础上增加一个元模型并使用基本模型的预测结果来训练元模型。 元模型的训练过程如下： 划分数据集为两个分离的部分（train 和 holdout） 使用第一部分训练几个基本的模型（train） 在这些训练过的模型上使用第二部分进行测试（holdout） 使用3）中的预测结果作为输入，正确地响应作为输出来训练元模型 前三步是可以迭代完成的。举例来说，我们有一个包含5个基本模型的堆叠模型，我们首先会把训练数据分为5份 。之后我们会做五次迭代。在每一次迭代中，我们在4份数据上训练基本模型然后在剩下的那份数据上预测。 可以确信的是，在五次迭代后，我们可以得到完整的5份由基本模型预测得到的训练数据，这在之后将会作为训练数据来训练我们的原模型。 在预测部分，我们会平均所有基础模型在测试数据上的预测结果，并将它们作为元特征，最终的预测结果将由元模型得出。 #####改进的堆叠模型类 12345678910111213141516171819202122232425262728293031323334class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, base_models, meta_model, n_folds=5): self.base_models = base_models self.meta_model = meta_model self.n_folds = n_folds # We again fit the data on clones of the original models def fit(self, X, y): self.base_models_ = [list() for x in self.base_models] self.meta_model_ = clone(self.meta_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # Train cloned base models then create out-of-fold predictions # that are needed to train the cloned meta-model out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models))) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_of_fold_predictions[holdout_index, i] = y_pred # Now train the cloned meta-model using the out-of-fold predictions as new feature self.meta_model_.fit(out_of_fold_predictions, y) return self #Do the predictions of all base models on the test data and use the averaged predictions as #meta-features for the final prediction which is done by the meta-model def predict(self, X): meta_features = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ]) return self.meta_model_.predict(meta_features) 改进的堆叠模型的分数为了使得两种方法有可比性（使用了同样数量的基础模型），我们使用了 Enet、KRR、Gboost作为基础模型，使用 lasso 作为元模型。 12345stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), meta_model = lasso)score = rmsle_cv(stacked_averaged_models)print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())) 12&gt;Stacking Averaged models score: 0.1085 (0.0074)&gt; 通过加入元模型确实得到了更好的结果。 融合模型我们增加了 XGBoost、LightGBM 到之前定义的堆叠模型中。 我们首先定义一个评估函数 rmsle 12def rmsle(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred)) 最终的训练和预测StackRegressor: 1234stacked_averaged_models.fit(train.values, y_train)stacked_train_pred = stacked_averaged_models.predict(train.values)stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))print(rmsle(y_train, stacked_train_pred)) 12&gt;0.0781571937916&gt; XGBoost: 1234model_xgb.fit(train, y_train)xgb_train_pred = model_xgb.predict(train)xgb_pred = np.expm1(model_xgb.predict(test))print(rmsle(y_train, xgb_train_pred)) 12&gt;0.0785165142425&gt; LightGBM: 1234model_lgb.fit(train, y_train)lgb_train_pred = model_lgb.predict(train)lgb_pred = np.expm1(model_lgb.predict(test.values))print(rmsle(y_train, lgb_train_pred)) 12&gt;0.0716757468834&gt; 12345'''RMSE on the entire Train data when averaging'''print('RMSLE score on train data:')print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 )) 123&gt;RMSLE score on train data:&gt;0.0752190464543&gt; 融合预测： 1ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15 提交： 1234sub = pd.DataFrame()sub['Id'] = test_IDsub['SalePrice'] = ensemblesub.to_csv('submission.csv',index=False)","link":"/2018/12/07/Kaggle之房价预测/"},{"title":"自己动手写Shell——C实现","text":"Shell的基本生命周期","link":"/2018/12/10/自己动手写Shell——C实现/"}],"tags":[{"name":"image segmentation","slug":"image-segmentation","link":"/tags/image-segmentation/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"}],"categories":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/数据挖掘/"}]}