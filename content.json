{"pages":[],"posts":[{"title":"从源码编译和安装Linux内核","text":"如何编译和安装Linux内核可以按如下的步骤编译和安装Linux内核 1.从 kernel.org 抓取最新版本的内核 2.验证内核 3.解压内核压缩文件 4.复制已存的 Linux 内核配置文件 5.编译和构建内核 6.安装内核和模块 7.更新 Grub 配置 8.重启系统 1.获取最新版本的源代码访问Linux官方网站下载最新的源代码。点击标有“Latest Stable Kernel”的黄色大按钮。 也可以使用 wget 命令下载 Linux 内核源代码： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.xz 2.获取 tar.xz 文件使用 unzx 命令和 xz 命令解压压缩文件获取源代码： 1unzx -v linux-4.19.1.tar.xz 或者 1xz -d -v linux-4.19.1.tar.xz 使用pgp验证 linux 内核 tarball首先获取linux-4.19.1.tar 的 PGP 签名： 1wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.sign 尝试进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 从 PGP 密钥服务器获取公钥来验证签名。例如：RSA key ID 79BE3E4300411886（从上面的输出结果中得到） 1gpg --recv-keys 79BE3E4300411886 示例输出： 1234567gpg: key 79BE3E4300411886: 7 duplicate signatures removedgpg: key 79BE3E4300411886: 172 signatures not checked due to missing keysgpg: /home/vivek/.gnupg/trustdb.gpg: trustdb createdgpg: key 79BE3E4300411886: public key &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; importedgpg: no ultimately trusted keys foundgpg: Total number processed: 1gpg: imported: 1 现在再一次使用 gpg 命令进行验证： 1gpg --verify linux-4.19.1.tar.sign 示例输出： 12345678gpg: assuming signed data in &apos;linux-4.19.1.tar&apos;gpg: Signature made Sun 12 Aug 2018 04:00:28 PM CDTgpg: using RSA key 79BE3E4300411886gpg: Good signature from &quot;Linus Torvalds &lt;torvalds@kernel.org&gt;&quot; [unknown]gpg: aka &quot;Linus Torvalds &lt;torvalds@linux-foundation.org&gt;&quot; [unknown]gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: ABAF 11C6 5A29 70B1 30AB E3C4 79BE 3E43 0041 1886 如果通过了验证，那么就使用 tar 进行解压： 1tar xvf linux-4.19.1.tar 3.配置 Linux 内核特征和模型在开始构建内核之前，你必须配置内核特征。你也必须明确你的系统所需要的内核模块（驱动）。这个对于新手来说是很有压力的。我建议你使用 cp 命令复制已有的配置文件： 12cd linux-4.19.1cp -v /boot/config-$(uname -r) .config 示例输出： 1&apos;/boot/config-4.15.0-30-generic&apos; -&gt; &apos;.config&apos; 4.安装编译器和其他必须的工具你必须安装有类似 GCC 或者相关联的工具来编译 Linux 内核。 使用下列的 apt 命令或 apt-get 命令来安装： 1sudo apt-get install build-essential libncurses-dev bison flex libssl-dev libelf-dev 尝试 yum 命令： 1sudo yum group install &quot;Development Tools&quot; 或 1sudo yum groupinstall &quot;Development Tools&quot; 额外的包： 1sudo yum install ncurses-devel bison flex elfutils-libelf-devel openssl-devel 如何在 Fedora Linux 上安装GCC 和其它开发工具运行下列 dnf 命令： 12sudo dnf group install &quot;Development Tools&quot;sudo dnf ncurses-devel bison flex elfutils-libelf-devel openssl-devel 5.编译内核开始编译并创建一个压缩好的内核镜像，输入： 1make 为了加速编译时间，传递 -j 参数： 1234## use 4 core/thread ##$ make -j 4## get thread or cpu core count using nproc command ##$ make -j $(nproc) 编译和构建 linux 内核将会话费大量时间，这取决于你的系统资源（例如：CPU）。 安装 Linux 内核模块1sudo make modules_install 安装 Linux 内核到目前为止我们已经编译了 Linux 内核并且安装了内核模块。是时候安装内核本身了： 1sudo make install 这将安装三个文件在 /boot 目录并修改你的内核 grub 配置文件： initramfs-4.19.1.img System.map-4.19.1 vmlinuz-4.19.1 更新 grub 配置你需要修改 Grub 2 bootloader 配置。 输入下列 shell 命令 CentOS/RHEL/Oracle/Scientific and Fedora Linux12sudo grub2-mkconfig -o /boot/grub2/grub.cfgsudo grubby --set-default /boot/vmlinuz-4.19.1 Debian/Ubuntu Linux12sudo update-initramfs -c -k 4.19.1sudo update-grub 重启系统： 1reboot 在重启后验证 Linux 内核是否为最新版本 1uname -mrs","link":"/2018/12/05/从源码编译和安装Linux内核/"},{"title":"自己动手写Shell——C实现","text":"Shell的基本生命周期","link":"/2018/12/10/自己动手写Shell——C实现/"},{"title":"Kaggle之房价预测","text":"Kaggle入门赛之房价预测 前言：最近实验室有一个数据挖掘的任务需要完成，是一个和国家电网相关的回归任务。因此我在 Kaggle 上找一些回归任务的比赛来练练手，房价预测这个比赛的难度比较适合我这种新手，因此选择了这个题目。本篇博客的大部分内容来自该比赛的一个高分 kernel, 感兴趣的同学可以直接跳转查看原博。 1.比赛介绍这是 Kaggle 上的一个为数据科学初学者准备的比赛，需要参赛者掌握一定的 R 或 Python 以及机器学习的基础知识。它给出了一个训练集和一个测试集，最终需要对测试集中的结果进行预测并提交。训练集中包含了79个特征以预测房价（SalePrice），包括：房屋的大小、地段、用途、基础设施等等。具体的描述可查看该比赛的比赛页面,比赛中所用到数据也均可通过比赛页面下载。 2.准备工作数据挖掘类的比赛最为重要的工作就是：特征工程。本篇博客将会用到的特征工程方法还是非常朴素的，具体如下： 通过已有的数据拟合曲线来填充缺失值。 变换某些看起来更像分类变量的数值型变量。 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。 对于一些倾斜特征采用 Box Cox 变换（替代对数变换）：这会得到一个更好的结果。 得到分类特征的假值。 之后我们将会选择一些基本模型（例如基于 sklearn 的 XGBoost），在 stacking/ensembling 之前在数据集上进行交叉验证。它的关键在于让（线性）模型对异常值鲁棒。这将同时提高积分板和交叉验证的分数。 接下来真正开始工作吧！首先导入相关的包和比赛所需的数据。 123456789101112131415161718192021222324#import some necessary librairiesimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)%matplotlib inlineimport matplotlib.pyplot as plt # Matlab-style plottingimport seaborn as snscolor = sns.color_palette()sns.set_style('darkgrid')import warningsdef ignore_warn(*args, **kwargs): passwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)from scipy import statsfrom scipy.stats import norm, skew #for some statisticspd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal pointsfrom subprocess import check_outputprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory sample_submission.csv test.csv train.csv 123456#Now let's import and put the train and test datasets in pandas dataframetrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv')##display the first five rows of the train dataset.train.head(5) 12##display the first five rows of the test dataset.test.head(5) 123456789101112131415#check the numbers of samples and featuresprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))print(\"The test data size before dropping Id feature is : {} \".format(test.shape))#Save the 'Id' columntrain_ID = train['Id']test_ID = test['Id']#Now drop the 'Id' colum since it's unnecessary for the prediction process.train.drop(\"Id\", axis = 1, inplace = True)test.drop(\"Id\", axis = 1, inplace = True)#check again the data size after dropping the 'Id' variableprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) print(\"The test data size after dropping Id feature is : {} \".format(test.shape)) 123456&gt;The train data size before dropping Id feature is : (1460, 81) &gt;The test data size before dropping Id feature is : (1459, 80) &gt;&gt;The train data size after dropping Id feature is : (1460, 80) &gt;The test data size after dropping Id feature is : (1459, 79) &gt; 3.数据处理异常值首先对数据集中的重要特征的异常值进行处理。 12345fig, ax = plt.subplots()ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 可从散点图中看到有两个非常明显的异常值，有着极大的房屋面积房价却很低。这两个值是非常典型的异常值，因此可以安全地删除它们。 123456789#Deleting outlierstrain = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)#Check the graphic againfig, ax = plt.subplots()ax.scatter(train['GrLivArea'], train['SalePrice'])plt.ylabel('SalePrice', fontsize=13)plt.xlabel('GrLivArea', fontsize=13)plt.show() 注意：删除异常值并不总是安全的。我们决定删除这两个值是因为它俩太奇怪了（极大的空间有着极低的价格）。 数据集中仍然可能存在其它的异常值。然而删除所有的异常值可能对模型产生不好的影响，特别是当它们可能出现在测试集的时候。因此在处理异常值得时候最好不要全部删除掉，以提高模型的鲁棒性。 目标变量房价是我们需要预测的目标变量，因此首先对它进行一些分析。 1df_train['SalePrice'].describe() 12345678910&gt;count 1460.000000&gt;mean 180921.195890&gt;std 79442.502883&gt;min 34900.000000&gt;25% 129975.000000&gt;50% 163000.000000&gt;75% 214000.000000&gt;max 755000.000000&gt;Name: SalePrice, dtype: float64&gt; 12345678910111213141516sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 180932.92 and sigma = 79467.79&gt; 目标变量是接近正态分布的。线性模型在正态分布的数据上拟合效果更好，所以我们可以对数据做一些变换使其更接近正态分布。 目标变量的对数变换1234567891011121314151617181920#We use the numpy fuction log1p which applies log(1+x) to all elements of the columntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])#Check the new distribution sns.distplot(train['SalePrice'] , fit=norm);# Get the fitted parameters used by the function(mu, sigma) = norm.fit(train['SalePrice'])print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))#Now plot the distributionplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')plt.ylabel('Frequency')plt.title('SalePrice distribution')#Get also the QQ-plotfig = plt.figure()res = stats.probplot(train['SalePrice'], plot=plt)plt.show() 12&gt; mu = 12.02 and sigma = 0.40&gt; 在经过对数变换后，房价数据接近于正态分布。 4.特征工程首先将训练数据和测试数据合并到一个数据帧。 123456ntrain = train.shape[0]ntest = test.shape[0]y_train = train.SalePrice.valuesall_data = pd.concat((train, test)).reset_index(drop=True)all_data.drop(['SalePrice'], axis=1, inplace=True)print(\"all_data size is : {}\".format(all_data.shape)) 12&gt;all_data size is : (2917, 79)&gt; 缺失数据1234all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head(20) 123456f, ax = plt.subplots(figsize=(15, 12))plt.xticks(rotation='90')sns.barplot(x=all_data_na.index, y=all_data_na)plt.xlabel('Features', fontsize=15)plt.ylabel('Percent of missing values', fontsize=15)plt.title('Percent missing data by feature', fontsize=15) 12&gt;Text(0.5,1,&apos;Percent missing data by feature&apos;) &gt; 数据关联1234#Correlation map to see how features are correlated with SalePricecorrmat = train.corr()plt.subplots(figsize=(12,9))sns.heatmap(corrmat, vmax=0.9, square=True) 输入缺失值根据不同特征的不同情况对缺失值进行填充。 对于离散值且数据说明里有 NA 选项，则将缺失值补为None。 PoolQC : 数据描述说 NA 表示 “没有泳池”。这是有意义的，因为有大量的缺失数据，这表明大多数的房子都没有游泳池。 1all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\") MiscFeature : 数据描述说 NA 表示“没有杂项特征”。 1all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\") Alley : 数据描述说 NA 表示“没有小巷联通”。 1all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") Fence : 数据描述说 NA 表示“没有棚栏”。 1all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\") 对于和其他特征有关联的值，可以先按其它特征分组，然后取中位数 。 LotFrontage : 因为房子附近的街道到房子的距离和到该房子临近的房子的距离是类似的，因此去临近房子的中位数来填充该距离。 123#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhoodall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform( lambda x: x.fillna(x.median())) 对于连续值，将缺失的值补为0。 BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath :缺失的值为0意味着没有基础设施。 12for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'): all_data[col] = all_data[col].fillna(0) 对于离散值且在数据说明里没有 NA 选项，则将缺失值填充为出现最多的值 。 MSZoning(大致区域分类)：‘RL’是最常见的值。所以我们使用‘RL’填充缺失值。 1all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0]) 对于所有值几乎都一样，参考意义不大的特征，直接删除 。 Utilities:该特征所有的值几乎都为‘AllPub’,除了一个‘NoSeWa’和2个NA。因为唯一的’NoSewa’出现在训练集，这个特征将不会对预测有帮助，因此可以安全地移除它。 1all_data = all_data.drop(['Utilities'], axis=1) 在缺失值处理完后，可以简单地查看是否还有未处理地缺失值： 12345#Check remaining missing values if any all_data_na = (all_data.isnull().sum() / len(all_data)) * 100all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})missing_data.head() 更多特征工程将某些看起来是连续值的特征转换为离散值1234567891011#MSSubClass=The building classall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)#Changing OverallCond into a categorical variableall_data['OverallCond'] = all_data['OverallCond'].astype(str)#Year and month sold are transformed into categorical features.all_data['YrSold'] = all_data['YrSold'].astype(str)all_data['MoSold'] = all_data['MoSold'].astype(str) 标签编码一些带有排序信息的分类变量（例如，标签顺序按照出现次数大小排序）。1234567891011121314from sklearn.preprocessing import LabelEncodercols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold')# process columns, apply LabelEncoder to categorical featuresfor c in cols: lbl = LabelEncoder() lbl.fit(list(all_data[c].values)) all_data[c] = lbl.transform(list(all_data[c].values))# shape print('Shape all_data: {}'.format(all_data.shape)) 增加一个更为重要的特征因为面积相关的特征对房价的影响是非常大的。所以我们增加了一个特征来描述基础设施的总面积，第一层和第二层的面积。 12# Adding total sqfootage feature all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] 斜率特征1234567numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index# Check the skew of all numerical featuresskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)print(\"\\nSkew in numerical features: \\n\")skewness = pd.DataFrame({'Skew' :skewed_feats})skewness.head(10) Box Cox变换高斜率特征1234567891011skewness = skewness[abs(skewness) &gt; 0.75]print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))from scipy.special import boxcox1pskewed_features = skewness.indexlam = 0.15for feat in skewed_features: #all_data[feat] += 1 all_data[feat] = boxcox1p(all_data[feat], lam) #all_data[skewed_features] = np.log1p(all_data[skewed_features]) 获取分类特征的假值12all_data = pd.get_dummies(all_data)print(all_data.shape) 获取新的训练集和测试集 12train = all_data[:ntrain]test = all_data[ntrain:] 5.模型导入相关库 12345678910from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsICfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.kernel_ridge import KernelRidgefrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clonefrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.metrics import mean_squared_errorimport xgboost as xgbimport lightgbm as lgb 定义交叉策略我们使用 Sklearn 中的 cross_cal_score 函数。然而这个函数没有扰乱属性，我们添加了一行代码，使得在交叉验证前对数据集进行扰乱。 1234567#Validation functionn_folds = 5def rmsle_cv(model): kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) return(rmse) 基础模型 LASSO 回归： 这个模型会对异常值特别敏感。所以我们需要让它变得更加鲁棒。在 pipeline 中使用 sklearn 中的 Robustscaler() 方法。 1lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) Elastic Net 回归： 1ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)) Kernel Ridge 回归： 1KRR = KernelRidge(alpha=0.6, kernel=&apos;polynomial&apos;, degree=2, coef0=2.5) Gradient Boosting 回归： 使用 huber loss 使其对异常值鲁棒 1234GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5) XGBoost: 123456model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state =7, nthread = -1) LightGBM: 123456model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11) 基本模型分数模型初始化好之后就开始测试一下交叉验证的损失分数吧。 12score = rmsle_cv(lasso)print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Lasso score: 0.1115 (0.0074)&gt; 12score = rmsle_cv(ENet)print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;ElasticNet score: 0.1116 (0.0074)&gt; 12score = rmsle_cv(KRR)print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Kernel Ridge score: 0.1153 (0.0075)&gt; 12score = rmsle_cv(GBoost)print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Gradient Boosting score: 0.1177 (0.0080)&gt; 12score = rmsle_cv(model_xgb)print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt;Xgboost score: 0.1161 (0.0079)&gt; 12score = rmsle_cv(model_lgb)print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std())) 12&gt;LGBM score: 0.1157 (0.0067)&gt; 堆叠模型（stacking model)最简单的堆叠方法：平均化基础模型我们首先用最简单的方法实验，构建一个新类来拓展 scikit-learn 中 model类，作为我们自己的堆叠模型。 平均化的基础模型类1234567891011121314151617181920class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, models): self.models = models # we define clones of the original models to fit the data in def fit(self, X, y): self.models_ = [clone(x) for x in self.models] # Train cloned base models for model in self.models_: model.fit(X, y) return self #Now we do the predictions for cloned models and average them def predict(self, X): predictions = np.column_stack([ model.predict(X) for model in self.models_ ]) return np.mean(predictions, axis=1) 平均化的基础模型分数1234averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))score = rmsle_cv(averaged_models)print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) 12&gt; Averaged base models score: 0.1091 (0.0075)&gt; 从结果看来，简单的堆叠模型确实提高了分数。这将激励我们探索更为高效的堆叠模型。 不那么简单的堆叠模型：增加一个元模型在这个方法中，我们将会在简单堆叠模型的基础上增加一个元模型并使用基本模型的预测结果来训练元模型。 元模型的训练过程如下： 划分数据集为两个分离的部分（train 和 holdout） 使用第一部分训练几个基本的模型（train） 在这些训练过的模型上使用第二部分进行测试（holdout） 使用3）中的预测结果作为输入，正确地响应作为输出来训练元模型 前三步是可以迭代完成的。举例来说，我们有一个包含5个基本模型的堆叠模型，我们首先会把训练数据分为5份 。之后我们会做五次迭代。在每一次迭代中，我们在4份数据上训练基本模型然后在剩下的那份数据上预测。 可以确信的是，在五次迭代后，我们可以得到完整的5份由基本模型预测得到的训练数据，这在之后将会作为训练数据来训练我们的原模型。 在预测部分，我们会平均所有基础模型在测试数据上的预测结果，并将它们作为元特征，最终的预测结果将由元模型得出。 #####改进的堆叠模型类 12345678910111213141516171819202122232425262728293031323334class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, base_models, meta_model, n_folds=5): self.base_models = base_models self.meta_model = meta_model self.n_folds = n_folds # We again fit the data on clones of the original models def fit(self, X, y): self.base_models_ = [list() for x in self.base_models] self.meta_model_ = clone(self.meta_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # Train cloned base models then create out-of-fold predictions # that are needed to train the cloned meta-model out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models))) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_of_fold_predictions[holdout_index, i] = y_pred # Now train the cloned meta-model using the out-of-fold predictions as new feature self.meta_model_.fit(out_of_fold_predictions, y) return self #Do the predictions of all base models on the test data and use the averaged predictions as #meta-features for the final prediction which is done by the meta-model def predict(self, X): meta_features = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ]) return self.meta_model_.predict(meta_features) 改进的堆叠模型的分数为了使得两种方法有可比性（使用了同样数量的基础模型），我们使用了 Enet、KRR、Gboost作为基础模型，使用 lasso 作为元模型。 12345stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), meta_model = lasso)score = rmsle_cv(stacked_averaged_models)print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std())) 12&gt;Stacking Averaged models score: 0.1085 (0.0074)&gt; 通过加入元模型确实得到了更好的结果。 融合模型我们增加了 XGBoost、LightGBM 到之前定义的堆叠模型中。 我们首先定义一个评估函数 rmsle 12def rmsle(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred)) 最终的训练和预测StackRegressor: 1234stacked_averaged_models.fit(train.values, y_train)stacked_train_pred = stacked_averaged_models.predict(train.values)stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))print(rmsle(y_train, stacked_train_pred)) 12&gt;0.0781571937916&gt; XGBoost: 1234model_xgb.fit(train, y_train)xgb_train_pred = model_xgb.predict(train)xgb_pred = np.expm1(model_xgb.predict(test))print(rmsle(y_train, xgb_train_pred)) 12&gt;0.0785165142425&gt; LightGBM: 1234model_lgb.fit(train, y_train)lgb_train_pred = model_lgb.predict(train)lgb_pred = np.expm1(model_lgb.predict(test.values))print(rmsle(y_train, lgb_train_pred)) 12&gt;0.0716757468834&gt; 12345'''RMSE on the entire Train data when averaging'''print('RMSLE score on train data:')print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 )) 123&gt;RMSLE score on train data:&gt;0.0752190464543&gt; 融合预测： 1ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15 提交： 1234sub = pd.DataFrame()sub['Id'] = test_IDsub['SalePrice'] = ensemblesub.to_csv('submission.csv',index=False)","link":"/2018/12/07/Kaggle之房价预测/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"}],"categories":[{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/数据挖掘/"}]}